package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 20/09/2025
 * Demo on Using/leveraging Apache Kafka to implement Event Driven Ms's inside our ms's inside Docker environment
 * ----------------------------------------------------------------------------------------------------------------
 * Now, we tested everything inside the local system, as a next step we need to test the things inside the docker environment. I will generate new containers for Accounts and Message ms's i.e., V11 and V2 respectively. We will also be making some changes inside the docker-compose.yml file which we will later execute and finally validate the entire changes inside the docker environment.
 * Docker file changes
 * -------------------
 * 1. Update the message and accounts services to use the latest images.
 * 2. Introduce the Apache Kafka service details. 1 or 2 years ago, my instructor encountered a problem. He validated the Apache Kafka website to understand if there is any information to set up kafka with the help of docker, unfortunately he didn't find anything. At your free time you can try validate and see if you will find anything. Post that, after doing some research, he found some information on how to set up Apache Kafka with the help of docker and docker compose. The information you can find in the GitHub repo which is maintained by bitnami - https://github.com/bitnami. What is bitnami? It is a kind of marketplace where they are going to help you to set up any kind of application in any kind of environment like Cloud, Docker, K8S, etc. You can check more information about them here - https://bitnami.com/
 *   They are a very trusted company/community as they are also supported by the VMWare. That's why we can safely use/leverage the docker compose instructions provided by them.
 *   The url details you can also always find them in your instructors GitHub repo. For example the url to the kafka image definition inside a docker compose file - https://github.com/bitnami/containers/blob/main/bitnami/kafka/docker-compose.yml
 * The format of that yml file is very familiar to us, first they are trying to create a service with the name 'kafka', they have also defined/provide the image details, ports mapping, there are also some volumes configurations, etc. i.e.,
 * # Copyright Broadcom, Inc. All Rights Reserved.
 * # SPDX-License-Identifier: APACHE-2.0
 *
 * services:
 *   kafka:
 *     image: docker.io/bitnami/kafka:4.1
 *     ports:
 *       - "9092:9092"
 *     volumes:
 *       - "kafka_data:/bitnami"
 *     environment:
 *       # KRaft settings
 *       - KAFKA_CFG_NODE_ID=0
 *       - KAFKA_CFG_PROCESS_ROLES=controller,broker
 *       - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
 *       # Listeners
 *       - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
 *       - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
 *       - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
 *       - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
 *       - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
 * volumes:
 *   kafka_data:
 *     driver: local
 *
 * Regarding the volumes configurations, volumes:
 *                                         - "kafka_data:/bitnami",
 * at the end of that compose file, you can see, volumes:
 *                                               	kafka_data:
 *                                                  	driver: local
 * that, with the help of this volumes, they are trying to create a folder 'kafka_data' inside your local system
 * The same they are trying to map to the folder, '/bitnami', which is prent inside the docker container. That means that the kafka that we are going to set up with this docker compose file, it is going to store all the data inside your local system in the folder named as 'kafka_data'
 * After these volumes configurations, we have some environment details which we need to follow i.e.,
 *  environment:
 *       # KRaft settings
 *       - KAFKA_CFG_NODE_ID=0
 *       - KAFKA_CFG_PROCESS_ROLES=controller,broker
 *       - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
 *       # Listeners
 *       - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
 *       - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
 *       - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
 *       - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
 *       - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
 * So, by taking this information, I have update our docker-compose files for all the environments i.e., prod, qa,default. You can check that out.
 * In our docker compose files, previously we had rabbit related service, my instructor deleted that. But for me I am not going to delete due to the reason we discussed previously in details in the previous commit.
 * Once you have defined this Kafka related information, we need to make sure, we are tagging this kafka service to the same network where we are trying to start all the other services.
 * Now lets update the accounts service definition. We initially had a depends-on rabbit configuration. My instructor deleted that. I didn't.
 * After that, I then created a new environment variable i.e., SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: "kafka:9092". The same environment variable you also need to create for the message service definition. And don't forget to also update the image tags for the 2 services.
 * Yes, those are the only changes my instructor did inside the docker compose file. As a next step, I can try to start all my containers with the help of docker compose up command. Before that, I need o stop all the running instances and containers/service/applications inside my local system(IDE). This you already know. We should also stop the running local apache kafka server by Ctrl + C and then 'Y'. Also stop the running container of keycloak and any other containers in the docker desktop because behind the scenes my docker compose is going to start/spin new containers.
 * Once you have stopped all the running containers and running servers inside your local system, as a next step, run the docker compose up -d command. After a couple of mins, all your containers should start successfully! As a next step, we need to set up the client details inside the keycloak. This we have done many times. You can check with our previous session. Basically, Click on 'Clients' and the try to create a new client by clicking on the 'Create Client' button, mention the clientId as 'get_tt_right-bank-callcenter-cc'. Click on the 'Next' button then enable the toggle 'Client Authentication'. Next uncheck 'Standard Flow' and 'Direct Access grants'. After that, check/tick 'Service Account Roles'. Click on the 'Next' button and then 'Save' button. Post that, you can navigate to the credentials tab to copy the client secret. Mention it inside your postman under the authorization tab. Also, the client id.
 * Now my client is created but it does not have enough roles assigned. To assign the same, on the LHS nav, click on 'Realm Roles' >> Create Role and create a role with the name 'ACCOUNTS' >> Save it. Back to the LHS nav, click on 'Clients' >> Open the client that we just created. Then, click on 'service account roles' tab >> Assign Role >> Select filter by real roles and the assign the role that we just created i.e., 'ACCOUNTS'. Now since we have assigned a role to the client that we just created, as a next step we can try to test the API with the help of Postman.. So, I will try to click on the 'Get New Access Token', use it inside your request and then fire the POST request. After few seconds, I will get a successful response. To confirm whether the communication between message and accounts services happened or not with the help of Kafka and asynchronous communication, we need to validate the logs available inside the containers of accounts and message ms.
 *  First, I am going to open the message ms logs, you will see some logs related to 'Sending email with the details ...' and 'Sending sms with the details ...'. Alongside that, you will see other log details related to 'Consumer Coordinator', Offset Information. This confirms that right now my message ms is leveraging apache kafka. Now, if you open accounts ms container logs, you can see a log like 'Is the communication successfully triggered : true'. Post tha, you will also see another logger saying that, 'Updating communication status for the account number ...". This confirms that the communication between 2 ms's is happening with the help of event streaming platform which is Apache Kafka. You should now be clear with the demo as you have visualized everything firsthand, not just stories. Now, you have the stepping stone knowledge about building event driven ms's. We explored 2 products, RabbitMQ and Apache Kafka. With this information now you should be good to go.
 * If a person asks you about event driven ms's, you are now in a position to shine by talking about the capabilities provided by spring cloud functions and spring cloud stream.
 *
 * Container Orchestration
 * -------------------------
 * Check slides for the intro and some details.
 *
 * Introduction to Kubernetes
 * -------------------------------
 * Check slides for more details
 *
 *  Internal structure/components inside K8S
 *  ------------------------------------------
 *  Check slides for more details
 *
 *  Setup of local K8S cluster using Docker desktop
 *  -------------------------------------------------
 *  Now, we have a basic understanding about what is K8S. As a next step, we need to set up a K8S cluster and try to deploy all our ms's. For the same, first we are going to set up a K8s cluster inside the local system. Once we setup a local K8S cluster, we are going to explore all the concepts of K8S. And once we are clear with all the concepts, we are also going to visualize a demo of creating a K8S cluster and deploying our ms's inside a cloud environment as well. But first, lets try to clear all the concepts with local K8S cluster. The reason is, if you try to spend more time on cloud learning all these concepts, you need to make sure that your K8S cluster is running a lot inside the cloud and that is going to attract a good amount of bill. That's why to avoid that, lets try to explore local K8S cluster. If comfortable and if you want, you can also try the same inside the cloud environment as well.
 *  To set up a local K8S cluster, we have various options; We have an installation with the name Mini KUbe. With the help of Mini Kube installation, we can set up a small K8S cluster inside our local system. But there are some drawbacks with Mini Kube installation because few of the commands that we give for the mini Kube will be different from the actual commands that we give to the K8s cluster inside the production environment. That's why we will avoid this Mini kube installation option. Instead, we are going to deploy a local K8s cluster with the help of Docker Desktop. This is going to work very similar to production K8S cluster. Since we already have docker desktop installed inside our local system, creating a k8s cluster with the help of Docker desktop is not going to be that super challenging. It is going to be very easy. Like you can see, you can visit the page https://docs.docker.com/desktop/features/kubernetes/. Inside this page there are clear steps on how to turn on K8s inside the docker desktop.
 *  Check the section 'Install and Turn on K8S'. By this you will be able to create a local K8S cluster inside your local system. Nothing but 'Start a Kubernetes single-node cluster when starting Docker Desktop.'. Do not check the option labeled, 'Show system containers (advanced)'. When you select this option and whenever you run docker commands like; 'docker ps', it is also going to display to you also all the containers created by the K8S cluster as well. Since we don't want that, leave that checkbox unchecked. Post that, click on Apply and restart. This is going to take few minutes. Meanwhile, lets try to understand more good information available inside the page - https://docs.docker.com/desktop/features/kubernetes/.  You can see that they have clearly highlighted that, 'for Docker Desktop versions 4.38+, there os multi-node cluster support'. Mine is version 4.35.1, hence even while you were enabling K8S, it was very clear that a single node cluster is what will be spinned up. So, normalize updating yourself with these docs, lest you are passed with time, haha!.
 *  The single node cluster creation, my instructor was saying that it's because, inside the local system we will not have large memory or CPU capacity. And that's why he was saying, "Always local K8S clusters are going to install a single node cluster the same acting as a master node as well as worker node". But when we try to create the same cluster inside the cloud environment, I mean when we will be creating a production ready K8S cluster with at least 3 different worker nodes and one master node, that's when everything will be even more clear. For now for our local testing, whatever we have set up should be fine. If you scroll down just below the 'Install and turn on Kubernetes', under the label, "Additional settings", you will see why I was saying we not check/select the option for 'Show system containers (advanced)'. Most users don't need to select this option, but for some reason if you want to see the internal Kubernetes system containers then you can select/check this option. The same we already discussed, we didn't select this as we don't want to see the internal K8S system containers when running docker commands.
 *  You can also see information under the label, "Install and turn on Kubernetes" that, When Kubernetes is enabled, its status is displayed in the Docker Desktop Dashboard footer and the Docker menu. When you go back to the Docker desktop, you can verify this. When you click the 'X' to close the page in docker desktop, in the footer you will see, "Engine running" which is related to Docker and "Kubernetes running" which is related to K8S. This indicates that our K8S cluster is right noe set up. Similarly at the same footer, if you click the 3 vertical dots, you should be able to see an option "Kubernetes context", if you expand it you will see some options, which means that our local K8S cluster is created. Now, we have local K8S cluster available. As a next step we need to make sure that kubectl is set up inside our local system as well. Like we discussed in our slided, 'kubectl' is one of the approaches to interact with the K8S cluster. Another option is the Admin UI, which we will discuss later. To give instructions to our K8S cluster, we need to make sure we have kubectl CLI set up inside our local system. In the page, "https://docs.docker.com/desktop/features/kubernetes/" under the section/label "Using the kubectl command", there are some instructions saying that;
 *   By default, this kubectl is going to be set up by your docker desktop while it is trying to create the local K8S cluster at the location, 'C:\Program Files\Docker\Docker\resources\bin'. You can find this information in the page,https://docs.docker.com/desktop/features/kubernetes/, at the section, 'Using the kubectl command". Next, lets try running some Kubectl commands, if they are not working, then make sure that this path/location is set in the path variables. Same drill like we do for the Java Home and Maven Home. Once this kubectl is is set up and installed properly, you can try to run the command, 'kubectl config get-contexts'. You will get an output like below:
 *   CURRENT   NAME             CLUSTER          AUTHINFO         NAMESPACE
 *      *     docker-desktop   docker-desktop   docker-desktop
 *  While firing any K8S command through kubectl cli to give any instructions to the K8S cluster, you have to make sure, 'kubectl' + space is prefixed, Always! With the command that we have just fired, like can be seen in the output, we will get what is the list of contexts available inside my local system. What is a context? It is a kind of isolated environment using which my client application or my CLI in this case kubectl CLI can interact with the K8S cluster. As of now, by firing the command we just fired above, you can see that by default there is a context created with the name 'docker-desktop'. Very similarly, you can try to run one more command which is, 'kubectl config get-clusters'. You will get an output like below;
 *  C:\Users\pc>kubectl config get-clusters
 * NAME
 * docker-desktop
 * This will give what are the list of K8S clusters that are running inside your local system. As of now, you can see that I have only one K8S cluster inside my local system. Just to note: It is perfectly possible to create any number of clusters with various approaches. But right now, since we only have one K8S cluster, the same is visible in our output with the name, 'docker-desktop'. So far, you should be clear with the kubectl instructions that we have fired so far. For some reason if you have various contexts available inside your local system, I mean you might have installed MiniKube or any other approaches to set up K8S cluster, in such scenarios if you have various contexts, we need to make sure we are setting the docker-desktop as the default context that we want to use for our local testing by running the command, 'kubectl config use-context docker-desktop'. You will see an output like;
 * C:\Users\pc>kubectl config use-context docker-desktop
 * Switched to context "docker-desktop".
 * For us since it our first time setting up K8S in our local system, this command is not going to have any effect because as of now, we only have one context. That's why under the output of running the command, 'kubectl config get-contexts', the 'CURRENT' column field has an asterisk for the record where the context name is 'docker-desktop'. This asterisk symbol is an indication that the record context is what is right now activated in our local system. The command, 'kubectl config use-context docker-desktop' is only going to help you if you have multiple record contexts available in your local system. Now, back in our page, 'https://docs.docker.com/desktop/features/kubernetes/' if you can scroll down, you will see another command i.e., 'kubectl get nodes' under the section, 'Verify installation'. This will confirm you how many nodes are available inside your local system. On firing this command, I got an output i.e.,
 * C:\Users\pc>kubectl get nodes
 * NAME             STATUS   ROLES           AGE   VERSION
 * docker-desktop   Ready    control-plane   25h   v1.30.2
 * The output is only a one record node, reason: As of now, we have created a local K8S cluster with only a single node. When we will be creating a K8s cluster inside a cloud environment, at that time we will visualize that our cluster is going to have more than a single node. For now, we should be good with this single node for our local testing. With this we have successfully set a local K8s cluster. Right now you should have enough confidence so far. And remember everything we are learning is a stepping stone. For more details, always check with the docs. For some reason if you want to turn off/disable the K8S cluster inside the local system, you acn follow the steps mentioned under the section, 'Turn off and uninstall Kubernetes'
 * *  * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
