package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 15/06/2025
 * Introduction to Distributed Tracing in microservices
 * -----------------------------------------------------
 * - As of now, we discussed 2 pillars of observability and monitoring which are: Logs and metrics. Now, we will be focusing on the 3rd pillar of observability and monitoring which is Tracing. With the 2 pillars that we earlier on discussed which are related to event logs and metrics, we can only derive the internal condition/ overall health of an application. So the information that we get with the help of event logs, health probes and metrics -  they fail to help developers in debugging the issues especially in distributed environments like ms's or CN applications. Because inside the distributed applications like ms's a user request can travel multiple applications/ms's and with this kind of complexity we should have some mechanism for the developers to understand up-to which ms/application a request is travelled and how much time it is taking at each service so that the developer can pinpoint the exact location where the issue is happening inside distributed applications. So, to handle this challange/problem, we have a concept which is referred to as distributed tracing. This is a technique which is especially used in ms's or CN applications to understand and analyze the flow of requests as they propagate across multiple services and components inside the distributed environments. With this tracing information, the developers can diagnose any kind of issues inside any kind of complex and distributed systems. In rel projects, there will be 100's of ms's deployed and if a request has to travel more than 10 ms's to get a successful response, then the developer should have crisp clear information of how the request is travelling under each ms; like which method is invoked, how much time a request is taking at each method, .etc. So until unless the developer is having this distributed tracing information, he cannot debug issues inside the distributed systems.
 *   You may have a question like; I understand the concept of distributed tracing but how do we implement it? One of the best/possible solutions to implement this distributed tracing is we should generate a unique identifier known as correlationId which is going to be generated for each and every request at the entry point of your distributed system or ms's. Previously, we discussed a scenario inside gatewayserver where at the entrypoint we tried to generate a correlation ID and the same correlation id we sent to the accounts, loans and cards ms's and with this correlation id we are able to easily track the request that is travelling from Gatewayserver to Accounts to Cards to Loans - When fetching a customers details. The same correlation id can be used as perfect solution for distributed tracing. Because with the help of correlation id we can track our ms's requests where it is travelling and up to which point/ms it travelled, what are the exceptions it threw, .etc. All such information we should be able to understand with the help of this correlation ID implementation. But do you think generating the correlation id and attaching the same for all the logs inside our ms's network is a feasible option for the developers? Of course not, haha. Because, if we did this, the developer has to visit each log present inside each ms and he needs to make sure to be appending the correlation id generated by the gateway server to each log. This is going to be a complex and challenging task. That's why we are going to look for a better option compared to what we have discussed previously with the help of gatewayserver to generate this correlation id using which we can try to implement distributed tracing in our distributed systems/ms's. Before we try to understand what are the best practices that we have around this, 1st we will see some standards that we will be following while generating the distributed tracing details. Distributed tracing always recommends to follow 3 important components:
 *    1. Tags - Using these tags we can build metadata that offer details like username of the authenticated user, or the ms/service identifier, etc. So, If you attach this metadata information to the logs, you can easily identify from the log statements to which ms/service or to which metadata information a particular log belongs. For example if you attach the application name to all your logs as a metadata information, then it is going to be easy for you to understand to which ms application a particular distributed tracing log belongs. When we will try to implement this and see a demo then it is going to be crisp clear.
 *    2. TraceId - It has to be generated at the starting f your request. Maybe when a request is trying to enter into a ms network, at the edge server the traceId has to be generated. The same traceId has to be attached to all the logs that are associated to that request. Regardless of where the request is traveling inside your ms network, throughout all the ms's where a particular request is being processed, the same traceId has to be present inside the logs.
 *    3. SpanId - A span represents each individual stage of request processing. For example, your request may travel Accounts >> Loans >> and cards ms. Like this under each service, this request should have its own spanID. Reason is because inside accounts ms you may invoke multiple methods and similarly inside other services also you are going to implement lot many other methods. That's why we should assign an id value which is specific to each ms. Like this, for all the logs which are generated withing the Accounts ms, they are going to have the same spanId along with the traceId that is common for all the ms's.
 *  For what we have just discussed - Check the slide number 149 on Distributed Tracing in m's from our pdf - for a visualization.
 *
 *  Possible options that we have to implement distributed tracing inside our ms's
 *  -------------------------------------------------------------------------------
 * The very first option that we will see is Spring Cloud Sleuth. This is a project which is available inside Spring cloud that is going to help us implement distributed tracing. When we try to implement distributed tracing with the help of Spring cloud sleuth, all the logs that are going to be generated inside you ms's, they will automatically have all the 3 components like; metadata information, trace id as well as span id. Developer need not make any changes inside the log statements. We just need to add the dependencies around the Spring cloud sleuth. Once the logs are generated with the help of this sleuth, we can integrate Spring cloud sleuth with ZipKin. Both these components by working together they are going to help the developer to identify the tracing details of the particular request along with the performance bottlenecks. But, we are not going to follow this approach inside our sessions because, if you visit the https://spring.io/projects/spring-cloud >> Spring Cloud Sleuth. You can see in the overview tab i.e., "Spring Cloud Sleuthâ€™s last minor version is 3.1. and post that they are not going to make any changes inside this project because they are trying to move all the tracing related changes into a common project called 'Micrometer tracing'. Since this Spring cloud sleuth project is going to be outdated, we will not make use of this library. You may see some blogs/course explaining/teaching around Spring cloud sleuth and zipkin - note that those are outdated approaches. So, click on the micrometer tracing link i.e., https://micrometer.io/docs/tracing. This will take us to their page. We previously used this micrometer when we were trying to expose metrics to prometheus. Under the same micrometer project, we also have capabilities to implement the distributed tracing. There is a lot of documentation inside this page, like what dependencies you need to add, how the distributed tracing is going to work, what are the span id's, trace id's and how this micrometer tracing has integration OpenZipkin Brave and OpenTelemetry. Yea, there is a lot of information that you can always explore on your own later. For now, lets use our instructors experience - what he knows around this and what he understood in this doc.
 * Based on my instructors experience, this approach demands a lot of changes and properties configurations from the developers. That's why we are not going to use this approach as well. Instead, we are going to use one of the best approaches which is OpenTelemetry. You can check their page i.e., https://opentelemetry.io/ - It is going to do the same kind of job that micrometer can do in terms of distributed tracing. The only difference and which lazy devs like me are going to like is - it is going to be super easy to implement distributed tracing. And, on top of that, it is worth noting that micrometer library is specific to Java - means, you can only use this micrometer library inside your java projects. But, OpenTelemetry is going to be available for lot many languages. It is also an open-source project and right now it is being maintained under the CNCF- Cloud Native Computing Foundation. All this information I am telling you can notice it in a section of the page - https://opentelemetry.io. It has a good interaction ith many languages and frameworks and that's why we are going to make use of it. Now, we will try to understand what are the changes that we need to make whenever we try to implement distributed tracing with the help of OpenTelemetry. You can click on the Docs link in the top nav bar. Post that, you can click on the Getting started link. You can see there are various options, for developers there is an options as well as for Operations team there is also an option. Since we are developers, we can click on the Dev option. Under the page,'Getting started for Developers', if you scroll down a bit you can see a lot many languages are supported. Since there are many languages supported by this 'Open Telemetry', its a good choice to learn this so that in future if you have a ms which is going to be developed  with for example the help of Go or any other language, you will use the very same approach that we are going to discuss with regard to distributed tracing. Now, click on the Java option and you will be landed on a page, 'Language-specific implementation of OpenTelemetry in Java.' In this page on the LHS nav, under Java you can click on the child link, 'Automatic' option, there us also a manual option which will expect a lot of changes from the developer - These 2 options I didn't see inside my page. But Suppose due to updates and stuff, that may have been renamed to something else. You can always find this out.
 *  - Actually, I noticed the updated LHS nav link is - 'Instrumentation ecosystem'. In this page you will notice something like 'Zero-code'. You will also notice we have various agents around the same i.e., Java Agent, Spring Boot starter. All these are a form of zero-code automatic instrumentation. For the respective agent we are to use, we have to follow certain steps. My Instructor was tryna use OpenTelemetry Java Agent - DocsZero-code Instrumentation >> Java >> Agent >>Getting started. Here the steps you need to follow are:
 *    . You need to make sure the jar -  opentelemetry-javaagent.jar - is present inside your class path.
 *    . Post that we need to invoke this jar using a property like; -javaagent , and the path to your opentelementry java agent jar file i.e., :path/to/opentelemetry-javaagent.jar, by path/to/opentelemetry-javaagent.jar, implies the path where this is available inside your application. Or if you are trying to start your application with the Java command then use - java -javaagent:path/to/opentelemetry-javaagent.jar -Dotel.service.name=your-service-name -jar myapp.jar - Like this you are trying to pass this Java Agent property , -javaagent , along with the path to your opentelementry java agent jar file i.e., :path/to/opentelemetry-javaagent.jar , I mean, where it is present inside your application. Apart from these Java Agent details we should also pass one property which is - Dotel.service.name=your-service-name . This property is going to help the java agent to identify the service name that you are trying to run. This same service name is going to be used as the metadata information inside your distributed tracing. The other options that we have with the help of JAVA_TOOL_OPTIONS i.e.,
 *      export JAVA_TOOL_OPTIONS="-javaagent:path/to/opentelemetry-javaagent.jar"
 *      export OTEL_SERVICE_NAME="your-service-name"
 *      java -jar myapp.jar
 *    After mentioning the 'JAVA_TOOL_OPTIONS' we are just below defining an environment variable with the property name 'OTEL_SERVICE_NAME' and the value of this property will be 'your-service-name'.
 *  - These very steps we are going to follow inside our ms's as well because with these changes, we don't have to make much changes inside our applications/services. We just need to make sure the jar 'opentelemetry-javaagent.jar' is present inside our class path of our service/ms's.
 * Up to now, you should be clear with all the options that we have discussed and why I am choosing 'OpenTelemetry'. When we see all these in demo, you will like this 'OpenTelemetry' as well. I mean, with very minimum changes, we are going to implement distributed tracing inside our ms's.
 *
 * Implement OpenTelemetry Changes Inside Our Microservices
 * -----------------------------------------------------------
 * Inside the pom.xml file present inside accounts ms, add the dependency related to OpenTelemetry which is , opentelemetry-javaagent , When I add this dependency inside my pom.xml, the same jar will be available inside the class-path of my application. In other words, the same jar is going to be packaged and available inside the docker image so that we can use the same whenever we are trying to generate a docker container from the docker image. So I will mention the OpenTelemetry version that we are trying to use inside the pom.xml file properties parent tag. This is going to be in between the child tags like; <otelVersion>version</otelVersion>. During the time he was teaching the version was at 1.27.0.  He mentioned that in future if there are any updates to anything including this, he will update that in the GitHub repo every quarter. That's why always compared notes with what he has in the GitHub repo frequently so that you are always up-to-date. After mentioning the <otelVersion>version</otelVersion> tag, just after the actuator dependency definition, add the dependency related to open-telemetry Java agent i.e., artifactId opentelemetry-javaagent. with a groupId io.opentelemetry.javaagent scope is runtime because we are going to use this jar only at runtime. You can see the pom.xml file present inside the accounts ms.
 * Now, open the application.yml file of accounts service. We are going to introduce a new property just under the logging parent property, nothing but create/define a new child property i.e., 'pattern' and under pattern 'level' property and finally assign a logging pattern value to the level property i.e., "%5p [${spring.application.name},%X{trace_id},%X{span_id}]". By default, the open telemetry is going to follow some logging pattern, but since I want to have my own custom pattern, I am mentioning the same here i.e., "%5p [${spring.application.name},%X{trace_id},%X{span_id}]". So, '%5p' indicates that I want to assign some 5 length characters before my application name which is represented by '${spring.application.name}'. Inside this 5 length character, open telemetry will try to generate some log severity like whether the log is debug, info, warn, error, etc. After mentioning the '%5p' 5 length characters, you can see I have opened a square bracket and similarly at the end I  closed square bracket. Inside the 2 square brackets, I am going to add 3 parameters i.e., the first one is tag i.e., ${spring.application.name} inside which I am going to mention the name of the application, nothing but, I am trying to mention/define the metadata information which is related to the spring.application.name. After this tag information, I am trying to convey to my spring boot framework that, at runtime open telemetry library is going to generate a trace id with the context information 'trace_id'and that we are injecting as a second parameter inside the square brackets. After trace_id, we also want to inject the span_id at the runtime into all my logs. So, my Spring boot framework, whenever it is trying to generate a log statement, it will look for the pattern that we have just configured inside the application.yml file i.e., logging.pattern.level and for the dynamic information in the value of this pattern, it will look for the values of the dynamic information, and accordingly it is going to append the same inside all my log statements that are going to be generated by my application/service.
 *   - As a next step, make the 2 changes that we have just discussed inside all our ms's i.e., loans, gatewayserver, configserver, cards, eurekaserver.
 * As a next step, I will go to the customer controller class. Check this class's docstring for continuation of this discussion.
 * - The very same thing that I have done inside the customer controller class, I am going to do inside the Loans controller class. - Check that class's docstring for continuation of this discussion.
 * - The very same thing I have done inside Cards controller class /fetch API. With these, we are done with all the changes we needed to make with regards OpenTelemetry inside our ms's. With OpenTelemetry we are going to add the distributed tracing information to our logs. Do you think by just adding distributed tracing information is going to help the developers. Of course to some extent it is going to help the developers, but to make developers life easy, we need to provide some UI where they can try to see all the information related to a request by inputting a span id or the trace id. We need to set up such a solution inside our ms's. So, below, lets try to understand how to set up the same.
 *
 * Distributed Tracing with OpenTelemetry, Tempo and Grafana
 * -----------------------------------------------------------
 * - Like we said before, with the help of OpenTelemetry we are going to add the  traces and spans information automatically. OpenTelemetry also known as OTel, is a vendor neutral open-source observability framework for instrumenting, generating, collecting, and exporting telemetry data such as traces, metrics and logs. How this Otel is going to generate the traces information is, at runtime, it is going to attach some byte code to your ms application. Using the same byte code, it is going to attach all the tracing information, span information or any other metadata information. Once my Otel has generated all the logs with the required tracing information, we are going to use a component inside the grafana ecosystem which is Tempo. What is its purpose? Just like Loki for logs and Prometheus for metrics, similarly using tempo we are going to index all the tracing information. This Tempo again is an Open-Source product that is highly scalable and cost-effective solution. Using this Tempo, we are going to store and analyze all the trace information. So, Tempo is only capable of storing the tracing information. Therefore, we need some UI application which is going to connect to the tempo and display the same inside a UI page. And that's where Grafana will come into picture.
 *   So, as you can see the ecosystem of grafana is very beautiful and fascinating. They always have a common UI component which is Grafana. And based upon the DataSource/Connection details, it can connect with various components like Loki, Prometheus, and now Tempo. Based upon your scenario, from a central service which is grafana, we can search for logs information or tracing information or metrics information. That's why learning the foundational concepts of Grafana, and it's various components inside Grafana ecosystem is mandatory for any ms developer and even deeper for platforms/operations team member. So, using Tempo, grafana is going to show all the distributed tracing information in an easily understandable visual information. When we see the demo, it is going to be super crisp clear for you. As of now, we are done only with the changes related to OTel, As a next step, we need to generate the docker images for all our ms's and post that we need to update the docker compose file to implement the changes related Tempo and integrating 
 *
 *  * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
