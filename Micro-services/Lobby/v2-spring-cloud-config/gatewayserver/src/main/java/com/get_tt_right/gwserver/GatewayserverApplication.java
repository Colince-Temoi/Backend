package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 07/06/2025
 * Demo of prometheus
 * --------------------
 * For the same we can try to start all our containers with the help of docker-compose file. But do you think we can straight away start our containers? haha Of course we cannot start our containers directly because as of now the micrometer related code changes that we have done inside the application.yml files and pom.xml files of our services/ms's are present inside our local workspace only. To test these changes with docker-compose file, we need to make sure we are re-generating the docker images to adapt to the latest changes we have done. So for accounts, cards and loans ms's, I edited the pom.xml files image tag to V7. For configserver, eurekaserver to V2 and for gatewayserver to V3. Then finally generated the docker images. These changes you also have to adopt inside your docker-compose.yml file.
 * Run the docker compose up -d from your prod profile where your docker-compose file is present. Once you confirm all your containers started successfully without any issues. It is simple just check that prometheus is up and running - Since its what we want to visualize our demo with and the gatewayserver container started successfully - Since it will basically start last after all other containers which it depends on have started successfully. Now, we can go ahead and test the changes related to micrometer and prometheus.
 * Steps
 * ----------
 * - 1st I will try to randomly test one of the url's related to prometheus i.e., http://localhost:8080/actuator/prometheus - Like this I am trying to invoke the prometheus url which is available against the accounts service. - It should show the metrics related to the accounts service. - If you see the metrics related to accounts service then it means prometheus is working fine. This confirms to you that we are visually seeing that we are getting a successful response even in the docker environment.
 * - As a next step try open the page http://localhost:9090 - Which is the prometheus page. To the url append /targets - If you try to invoke this path, you should be able to see all the running containers/targets information that are currently registered with prometheus. - You should see the accounts, cards, loans targets in the list etc.
 *   Under the Filter by target health dropdown, you can filter the targets based upon the health status i.e., Up, Down, Unknown etc.
 *   Like this we are able to see all the containers that are being monitored by the prometheus. In our case, 6 containers are being monitored by the prometheus.i.e., accounts, cards, loans, configserver, eurekaserver and gatewayserver.
 * From this single UI page related to prometheus, we are able at a glance to see what is the overall status of all of our containers/applications. If you want to know more details you can expand into any one of the containers, and you will be able to see more details like what is the label(s) information, Job information that we configured behind the scenes with the help of prometheus configurations, Last scrape, State, etc.
 * Now, after validating these targets. We can also see other metrics like graphs - Click on the Prometheus homepage and click on graph tab. - Here you can search for some of the metric like; cpu_usage, memory_usage, http_requests etc. - Then on clicking execute you will be able to see the graph related to that metric. Similarly, if you click on the table tab you should be able to see all the table related information. i.e.,
 * system_cpu_usage{application="eurekaserver", instance="eurekaserver:8070", job="eurekaserver"}	0.08169934640522876    # Like inside eurekaserver service under the instance 8070  what is the CPU usage value. The same you can also visualize for other ms's.
 * system_cpu_usage{application="cards", instance="cards:9000", job="cards"}	0.08243277205327973
 * system_cpu_usage{application="loans", instance="loans:8090", job="loans"}	0.08148893360160966
 * system_cpu_usage{application="accounts", instance="accounts:8080", job="accounts"}	0.0805234021137393
 * system_cpu_usage{application="gatewayserver", instance="gatewayserver:8072", job="gatewayserver"}	0.08542713567839197
 *
 * - All this you can also see in a beautiful graph  when you click on the graph tab. You can visualize this for a particular timeframe as required. Beautiful stuff! Just below the graph, you can turn on or off the graph to a particular service as required. By default, all are turned on. Very similarly you can visualize the graph in  2 options; Unstacked and Stacked. You can activate any of this option as per your requirement. Beautiful stuff.
 *   As of now the graph is only showing the CPU usage for all the ms's. If you want to only see the graph for a particular service, you can also activate this, just below the graph and turn the other services graphs off.
 * This way we can execute and visualize any metric. As of now we are only visualizing the CPU usage metric. You can visualize other metrics like; process_uptime_seconds - This will show you what is the uptime of the application. This is very useful when you are trying to see how long your application has been running - in seconds.
 *  - You can play around with the other metrics as well.You can look for connection details, threads information related metrics etc. So based upon your requirements you can visualize any of the metrics for any or all of the ms's.
 * If you don't know what are the metrics available inside the prometheus and if you want to check what are all available metrics that you can visualize inside prometheus, then you can always click on the 3 dots just infront of the Execute button, and you can choose the option Explore metrics. Here you should be able to see the list of all the metrics that are being tracked by the prometheus with the help of micrometer. So based upon your requirements you can try to execute and visualize any of the metrics.
 * Also for some reason if you don't like the dark theme you can always switch to the light theme. With this you should be able to see all the graphs with different colors with more clarity. If also needed, while visualizing metric information you can reduce the timeframe and this will help you to understand your graphs even more clearly.
 *
 * Up-to now, you should be crisp clear about the capabilities of prometheus and micrometer. Soo powerful!!! How we are bale to see all our metrics in a single UI page. Now back to the 'target health' page where you can see a list of all containers with their respective status.
 * You can filter out the containers based in the target health status i.e., Up, Down, Unknown etc. You can kill one of the containers and visualize how the dashboard is going to be affected. Inside the docker desktop, I decided to stop the configserver service. Now, behind the scenes prometheus maybe looking for the metrics related to configserver or even lets say cards ms and if it doesn't receive the response then it will mark the status of this service as 'down'. You can refresh the page and see the changes. If you restart such services from the docker desktop and again refresh the page you should be able to see the changes.
 * For services marked as 'down' - unhealthy, if you try to expand them you should be able to see the respective error i.e., Error scraping target: Get "http://cards:9000/actuator/prometheus": dial tcp: lookup cards on 127.0.0.11:53: no such host Which implies that while prometheus is trying to send the request to the url http://cards:9000/actuator/prometheus, it is not getting any response from cards service with an error like, 'no such host'.
 * If you restart your services again from the docker desktop and again refresh the page you should be able to see the changes effecting to the prometheus dashboard after some time. This informs us that Prometheus is continuously monitoring the overall status/health of our services. With this limited functionality you should be able to monitor your ms's. For bigger organisations or the complex projects where they are trying to run their critical applications inside their prod environments, whatever prometheus provides may not be sufficient and that's why you will find organisations will try to integrate prometheus with grafana because with the help of grafana we can build dashboards complex in nature.
 *  As of now with the graph option that we have visualized in prometheus dashboard, we should only be able to track a few limited information. But not the many other metrics and that's why we need to integrate prometheus with grafana and we already did that with the help of datasource configurations inside the grafana. As a next step we will try to explore the integration between prometheus and grafana and how we can visualize the graphs in the dashboard.
 *
 *  Demo of Prometheus and Grafana integration
 *  ----------------------------------------------
 * - On the grafana page i.e., localhost:3000,on the LHS nav you should see a connections option.  Under which you have the option to either create/Add a new connection or check the existing connections by clicking in the Data Source link. As of now under the datasource you can be able to see we set up/created/added 2 connections - Loki and Prometheus. Matters regards Loki, we already saw the demo where we are able to access and search through logs from here inside the grafana. In a very similar fashion, using prometheus, we can also search and build metrics dashboards with the help of the UI provided by grafana. If you click on the Prometheus data source you should be able to see all the settings/configurations that you have given inside the datasource.yml file that we created/defined under the observability/grafana folder in our workspace. So,using the same contents, the connections are going to be created. You should be able to see the Prometheus server URL using which the connection between prometheus and grafana is going to be established/work.
 * - On the top RHS of the grafana page, click on the Grafana icon and then on the RHS nav bar,navigate to the explore link. Under this page you can select from the dropdown any datasource. With Loki we can access and search through the logs of respective services. With Prometheus we can access and search through the metrics for respective services. So select Prometheus - It is selectd by default. And now on the Metric label, I am going to select the metric related to system_cpu_usage. Post that for the label filter I will select 'application'. And if needed under the 'select value' dropdown, you can select which ms system_cpu_usage metric information you want to see.  As of now I want to visualize the system_cpu_usage metric information for all the ms's amd so on the to LHS, I will try to run this query. and with that you will be able to get a beautiful graph  with a time period of last 1hr. You can try to change this range to your preffered timeframe. i.e., last 15 mins, last 30 mins, etc. Like this the graph will be updated accordingly and you should be able to visualize the system_cpu_usage metric information for all the ms's. Each color represents a single ms. Just below the graph you also have the flexibility to visualize the metric information of only a single ms by clicking on it.
 *   Here also inside the grafana, you have various options like using lines(the default) you can see the graphs, using Bars, points, stacked line and even stacked bars. So it's up to you which style you want to leverage.
 * This way we can search and visualize any metric information for any or all of the ms's. You have the flexibility to visualize more than one query on the same graph by just clicking on the Add query button. This time I added the query with the name 'up'  and select label filter as 'job' . Post that run the query(s). Now if you check the graph you should be able to see the combined information of the system_cpu_usage and the up metric information for all the ms's.  The line which you can see at the top of the graph is the 'up' metric information. Whatever graph you see at the bottom of the graph is the system_cpu_usage metric information. So this way you can visualize the combined information of the system_cpu_usage and the up metric information for all the ms's. You can try to add as many metrics ad you want but it is always recommended to search one metric at a time so that it will be more clear for you.
 * Up to now, you should have crisp clarity about how the integration between prometheus and grafana works. So powerful!!! Grafana is a more matured project and can help you to visualize your metrics better than prometheus on it's own. So far we've seen demo for both but the story does not end here. There is more to grafana and prometheus which we will be discussing next.
 *
 * Demo of grafana inbuilt and custom dashboards
 * -------------------------------------------------
 * We will be seeing more capabilities of prometheus and grafana together. For the same if you visit the prometheus website - https://prometheus.io/ >> Get Started button >> On the LHS Nav go to the Visualization tab >> Click on the option 'grafana'. Inside this page - Grafana support for Prometheus, you will see information about how to install grafana, how to create a prometheus datasource inside grafana, How to create a prometheus graph with the help of grafana - All of which we have already explored in our discussions above. Now, as a next step, we will be exploring the 'prebuilt dashboards from garafana.com' . Under this section you will notice a link that is going to redirect you to the grafana collection of pre-built dashboards - https://grafana.com/grafana/dashboards/. So, grafana team and other open-source team members built a lot of dashboards which you can try to set up inside your own grafana instance based upon your own requirements. If you can see here, there are many graphs which based upon the component you are trying to use accordingly you can set up these graphs. Like you can visualize your Jira data, MongoDB data, Monitor K8S deployments, etc. Like this, there are a lot of in-built graphs but right now we are more interested with the graphs related to Java ecosystem. You can simply try to search for the string, 'JVM' and with that you will be able to see a lot of dashboards related to JVM.
 * So, try to get one which has more number of downloads. The very first dashboard i.e.,JVM(Micrometer). Okay, in the current you can not see the which has more number of downloads, seems they removed this. But let's assume they are sorted, such that the one with the highest number of downloads comes first. In our case, JVM(Micrometer). Now, if you click on it, you will be taken to a page with more details about that dashboard. What we can do is, take that dashboard by copying its url from the top of your browser. Now, in our grafana application i.e., localhost:3000. Now, since we want to build a dashboard that we want to see again and again we need to make sure we are logged into the grafana. So, click on the sign-in option on the top RHS and then enter the default credentials which are 'admin' and 'admin' for the username and pwd respectively. On trying to log in, it will try to recommend you to change the pwd but for now you can just skip that. As a next step on the LHS nav, if you click on the Dashboards link, as of now we have no dashboards. So, click on 'New' >> 'Import' >> and paste the URL that we just copied in the input box labeled 'Grafana.com dashboard URL or ID' >> Click on the 'Load' button and then as a next step in the input box labelled, 'Select a prometheus data source', we need to make sure we are providing a prometheus datasource details by selecting the correct option inside the dropdown.
 * Now, you can go ahead and click on the import button and yeeyy!! we have a dashboard. In this you should be able to see a lot of good information with the help of this imported pre-built dashboard. As of now, I am trying to visualize the metrics of accounts service with the instance details as '8080'. You should see Quick facts like what the Uptime is, Start time, Heap used, Non-Heap used . Under the I/O overview you should be able to see what is what is the duration, Utilization, Errors information, Rate. Under the JVM memory section, you should be able to see the JVM heap, JVM Non-Heap, JVM Total, JVM process Memory. Under the JVM Misc section, you should be able to see what is the JVM thread information, Load, CPU usage information, thread states, GC pressure, Log events information, File Descriptors. Under the JVM Memory Pool(Heap) details you should be able to see Eden Space, Survivor Space, Tenured Gen. Like this, there is a lot of information available inside this dashboard in more sections like, JVM Memory Pool (Non-Heap), Garbage Collection, Class Loading, Buffer Pools, etc. This dashboard I didn't build, haha, I am just trying to leverage one of the dashboards build by the open-source community and the grafana team. Yea! this is the beauty of grafana and prometheus. If you want to understand the details of other ms's, you can select that >> Select the corresponding instance and happily visualize the graphs.
 * Let's try to create/import one more pre-built dashboard so that you can seal your clarity. This time we can try looking for another useful dashboard inside the grafana website i.e., https://grafana.com/grafana/dashboards/?search. So I will try to search something like 'Spring Boot' and with that you should be able to see some dashboards here. I decided to choose 'Spring Boot Observability' and 'Spring Boot 2.1 System Monitor' pre-built dashboard. Same drill as we did with the first dashboard. Simple! Up to now, hope you have visualized the power of grafana and prometheus together. How helpful they are to monitor your ms's. Now, as a next step we will try to understand how to create our own dashboards. Sometimes you may want to create your own dashboards based upon your own requirements. Below are the steps my instructor took me through.
 * 1. Create a new dashboard in grafana. First try to Save this dashboard we are trying to create by giving it a name, 'Get_tt_right'
 *    Click on Edit >> From the Add dropdown, select 'Row' as we are trying to create a row element inside our dashboard. To this Row, I want to give some title i.e., Accounts ms because under this row I am going to have the metrics related to the accounts ms. Now click on update to update this.
 *    Next from the Add dropdown, select 'Visualization' to add a panel. On the LHS section, provide the name of the panel for example, 'Up time'. At the center bottom section,for this uptime panel select a datasource i.e, prometheus - it's selected by default anyway. Select a metric i.e., process-uptime_seconds. Select the Label filter as 'Application' and select the value as 'accounts' since we are trying to build a dashboard for accounts ms. If you click on the 'Run queries' with this you should be able to see the uptime details of our accounts ms. If you don't like what you are seeing in the graph, on the top LHS section - From the visualization dropdown, where you are seeing 'Time series' selected by default, you can change that to graph type you want. But for now I am fine with the default i.e., 'Time series'  for visualizing this 'process_uptime_seconds' metric. Now you can go ahead and save your custom create dashboard.
 *
 * Now if you click on the RHS tab i.e., Dashboard >> Get_tt_right You should be able to see a panel created with the name 'Up Time'. To bring this just created panel under the 'Accounts ms' row that we created >> Click on the Edit button on the top LHS >> then just drag the panel below to this row. Finally, you can save your dashboard changes. Very similarly we can create one more visualization. Click on the Edit >> Select Visualization from the Add dropdown. At the center bottom section -> Select the Data source as prometheus >> Metric as 'Up' >> Label filter 'job' >> Value 'accounts'. At the RHS section -> Panel option I am going to give the Title as 'Up'. This will indicate/tell if my ms is up and running, or it is in stopped status. If needed on this LHS section under Panel options, you can explore the many other options/configurations you can make. For now, I will go with the remaining things as defaults for the Panel options. Now, coming to the graph style under the 'Visualization' label on the LHS section, I will this time use something like 'gauge'. Like this we will be able to visualize our 'up' metric for accounts ms with the help of gauge style of graph. Now, on saving this dashbaord and then going back to our dashboard, you can be able to see we now have 2 panels that we have created by ourselves. Drag the new panel to the RHS of Up Time panel.
 * Like this, under the Row 'Accounts ms' we have 2 panels. So to add any rows and panels we will always be using the same drill i.e., Edit >> Add dropdown >> ...etc as we have discussed above. This way by adding any number of rows and panels we can create our own custom dashboard. Grafana itself is a very big topic and usually the platforms team or operations team they should learn everything about grafana. We as developers should be fine with these basics/foundational knowledge that we have just learnt above. If interested to learn more about grafana and other components inside grafana, please enroll into a course that covers grafana in detail. Now if you go back to the dashboards, we have 3 imported predefined dashboards and one custom dashboard that we have created by ourselves. So now we have 4 dashboards in total. With these dashboards, anytime, any team member or any operations team member they can quickly come here and try to visualize and understand what is the overall health of our ms's. Up to now you should have crisp clarity on how we are going to perform monitoring with the help of these dashboards.
 *
 * Create Alerts and send notifications using grafana - Approach 1 - 'Alerting' option in the LHS nav of grafana.
 * -----------------------------------------------------------------------------------------------------------------
 * We will learn how to trigger alerts and notifications from grafana whenever a condition is met. So, under the LHS nav, you should see an option of 'Alerting'. On expanding its dropdown you can go to 'Alert rules' . As of now, we don't have any rule configured. You can click on the create 'New Alert rule'. Rule name I am giving as 'accounts' because using this rule I want to monitor my accounts ms. Under the 'Define query and alert condition' If you toggle the Advanced options input, you will see Rule type label which asks you to select where the alert rule will be managed. Leave it as it is 'Grafana-Managed' reason - We want grafana to manage the alerts. On this same step2 we need to provide what is the criteria that grafana needs to consider to trigger an alert. For the same, under the datasource dropdown I am going to select prometheus because we want to trigger an alert based upon a metric available inside the prometheus datasource. This Prometheus datasource is selected by default anyway. Now under the select metric dropdown >> I am going to input a metric with the name 'up' - This metric is going to tell me whether my application is up and running or not. Select the label filter as 'job'  and the value as account ms because we want to monitor our accounts ms.
 * Now under the Expressions section by default you should be able to see one Expression option i.e., Threshold. Make sure under step 2 you have enabled 'Advanced options' in order to see what I am talking about. Now if you click under 'Add expression' dropdown you should see more other expressions. Select the 'Reduce' option and you should see a section pop up with this selection.  First lets understand how we want to trigger an alert. If you see under the reduce section, there are various input options for the respective labels. For the 'Function' label, you can see options like "Whether we want to send based upon minimum value or max or mean or median or sum or count or last".  For me I am selecting 'Last' - based upon the last value I want to generate an alert. You can see for this Function 'Last' we are providing an input data from the 'A' section - as you can see we have selected 'A' in the Input label. A is the query section that we just defined. For the Mode label in Section B, we left it by default to the selection 'Strict'. Now coming to the C section where we are doing definitions to 'Threshold'. So based upon 'B', in our case 'Reduce' function,  as our selection to the input label for C section I want to define a threshold like, " Whenever my value 'is below' 1 then I want to trigger an alert. You can also clearly see that for this section 'C' you can see a highlighted test on the top RHS i.e., "Alert condition' which mean that, based on this threshold value only, the alert is going to be triggered.
 * With our 3 sections definitions, i.e., Section A - Query definition, Section B - Alert condition definition and Section C as the Threshold condition definition, we have created all the criteria to generate an alert. And btw do you know why under the section C I have given 'IS BELOW 1'? It's simple, whenever my application is up and running based on our query definition, the metric 'UP' will give the value as 1 and when the service/ms is down, the value is going to be zero. With this, we have defined all the definitions/configurations for the alert.
 * Under step 3 - Add folder and labels, we need to assign our alert definition into a folder. You can select a folder if you already had created one or click on the 'New folder' button to create a folder. In our case we are creating a folder with the name 'Accounts' so that we can keep all the accounts service related alerts inside this folder.
 * Post that, under step 4 - Set evaluation behavior, under the label 'Evaluation group and interval' - We need to assign our alert definition into a group. For this click on the 'New Evaluation group' button to create a group i.e., 'Accounts'. Once we have mentioned this 'Evaluation group name' we need to define the 'evaluation interval' - Like how frequently we want the alert to be evaluated by grafana. By default, you can see the alert is going to be evaluated for every one minute. Since we want to test this alert very quickly we can modify this to '10s' - nothing but 10 seconds.  After that you can see a label 'for' and a default value like '5minutes' - this I din't see, it was in my instructors grafana page presentation. But what does this mean by the time he was doing his recording? With this 'For 5 minutes', we are telling to the grafana that instead of throwing the alert at the very first time, we want the grafana to monitor the alert for the next 5 minutes - beyond this, if the alert is not yet resolved then only we want to send the notification. Yea that what is being achieved here by this default value of 5 minutes. In my grafana page I would equate this to the 'pending oeriod label' definition as it's description suites what my instructor was trying to present. Though in my grafana page the default value is 1 minute. But we don't want to wait for this much time to receive a notification, I can make this as 20 seconds.
 * After this, if you can scroll down, we can provide what is summery, description, etc. of the alert. This I can be able to see under step 6 - Configure Notification message in my grafana page. If someone receives this alert they should know what this alert is all about. For the same under the summery I am mentioning something like - "Accounts service is down" and inside the description you can say something funny like "Please do something".
 * Next, under Section 5 - Configure notifications. This we can leave as blank as it is by default. Finally, on the top RHS you can happily click on 'Save rule and exit' button. Didn't manage to Save that, haha. I got an error under the step 5 - Configure notifications' section, and they are forcing us to configure/define a 'contact point'. For this I am selecting 'grafana-default-email' and then 'Save rule and exit'. Now, you should be able to see our alert rule is created, and it is in normal state. Reason - Right now my accounts ms is up and running. It is going to be evaluated every 10 seconds for a period of 30 seconds. Beyond 30 seconds, if the alert is not resolved then a notification is going to be triggered. Since we want this alert to be notified to someone else, on the LHS nav under 'Alerting' link, click the child link i.e., 'Contact points'. By default, there is a 'grafana-default-email' contact point - Whenever an alert is triggered, this contact point is going to be notified - Nothing but to the email(s) that you have configured the email/notification will be sent. So, to configure email(s) you can click on the 'edit' button of the 'grafana-default-email' contact point and there you can mention your sample email value(s). But this email option may not work for us for now because in order to send emails with the help of grafana we need to provide SMTP details. We don't want to follow that complex process. Instead, lets see other contact point options available. So, click on the 'Create contact point' button.
 *  - Under the Integration dropdown,you can see many other otpions i.e., 'Grafana IRM', 'AWS SNS', 'Alertmaneger', 'Cisco Webex Teams', 'DingDing', 'Discord', 'Opsgenie','PagerDuty','Slack','Webhook', 'VictorOps', 'Telegram', 'Threema Gateway', 'Kafka REST Proxy', 'Jira', 'Google Chat', 'LINE', 'MQTT', etc. For now, we are going to use 'Webhook' option. And I will give the Contact point name as 'Get_tt_right webhook'. Now we need to provide a webhook URL. As of now we don't have any webhook url set up. For the same go to the site - hookdeck.com - We used this previously also. Now to test things out, click on the Products dropdown and then click on the Console link. You will be redirected to a page console.hookdeck.com and here you should be able to see a sample url that you can easily copy and paste inside the webhook field inside the grafana. Post that, in our grafana page, you can click on the 'Test' button  which will send a sample test notification. If you go to the console.hookdeck.com page where you copied the webhook url, you can see the test notification sent has been received. In the body label in the console.hookdeck.com page for our received notification, if you click on that value, you should be able to see section pop up on the LHS of the page with 2 tabs labeled, 'Request' and 'response'. Under the request we have the headers section, body section. Under the body section you can see we received  k-v pair details like, 'status':'firing', 'alertname':"TestAlert", "instance":"Grafana", etc. This confirms that grafana is able to send test alerts to the webhook url that we have provided. Now, click on the 'Save contact point' button inside our grafana page where we were trying to create a contact point.
 * Post that on the LHS nav under Alerting >> Notification policies. As of now you can see by default all the notifications are going to be delivered to the grafana-default-email option. I want to change this to webhook, for the same you can click on the RHS 'More' dropdown then 'Edit' option. Here under the input labelled, 'Contact Point' you can select the 'Get_tt_right webhook' option. In this same pop-up we have a Timing options you can be able to see various default values that my grafana will follow whenever it is trying to send the alerts. Like there is a Group wait of 30s, Group interval of 5m, and a repeat interval of 4h. So, what us the purpose of these timing options. If my alert is triggered, I don't want it to be continuously triggered as that will send a lot of emails or notifications to my audience. That's why they have mentioned this default values like 4h. So, the alert will be triggered only once in 4 hours. So, every 4 hours the alert notification is going to be sent if the alert is not resolved. So let me change all the default timing options to 10s for our testing purpose so that we can see these alerts in action. Now click on the 'Update default policy' button. Now, If I go back to our Alert rules page, as of now we can see the alert is still in 'normal' status which means our accounts ms is working without any issues. As a next step, I will go to the docker desktop and I will try to stop my accounts service. Now, if you go back to grafana page, specifically on the alert rule we created, you will see its state changed to 'pending' which means my alert is waiting for the period of 30s. Okay, it's trying to evaluate the alert for every 10s but it's not going to send the alert immediately instead it is going to wait until 30 seconds are done. So, during this transition period, it is going to be in this 'Pending' status. After the 30s have elapsed it will go into the 'Firing' status. That means my alert is being send to the notifications that we have configured.
 *   - If you go to the Webhook page, console.hookdeck.com, you should be able to see the alerts. If not make sure to edit your Alert configuration under step 5 - Configure notifications and set the contact point to 'Get_tt_right webhook'. Then you can see the alerts in the console.hookdeck.com page when you retyr this test. Another thing I noticed is with the states transition, especially from 'firing' back to 'normal' state. It wasn't happening automatically as I would expect. That is, even though the service resumed working normally, the state still stagnated at 'firing' and this caused a lot of notifications. I had to intervene in a very creepy way - On the respective alert that we defined, I had to click the More dropdown and then forced to 'Pause evaluation' and then 'Resume the evaluation'. This is the only way I was able to see the state change from 'firing' to 'normal' and the notifications stopped firing. At some point I went even ahead clicked the view button on the alert and then tried to vie in explorer the configured query in section A and tried to refresh it and then back to pausing and resuming the evaluation. This is not the best way, but I was able to see the state change as I'd expected. I expected everything to be automatic just the same was my instructors presentation was. But anyway, he did the presentation like 2 years agao to the time I ma doing this docstring and mmaybe due to the grafana UI changes I might have missed something or might have not got something correct. But for sure I followed all his steps. In the future, you will need to find out how to seamlessly do this from the grafana UI. When the alert is resolved and moved to the normal status, if you go to the webhook page, you should be able to see the alert has been resolved.i.e., status:resolved. This indicates to your contact point(s) that a particular alert is resolved. Which means your accounts service started/resumed working fine. In the sext session we will discuss one more approach of defining the alerts with the help of dashboards in the grafana. Hope we don't run into the same problem again that we experienced in the 1st approach that we have just completed discussing . But you can always find out more about approach one in future to make things seamless as expected.
 *
 * Create Alerts and send notifications using grafana - Approach 2 - 'Dashboards' option
 * -------------------------------------------------------------------------------------
 * Previously we created alerts by defining alerts directly inside the 'Alerting' >> 'Alert rules'. As an alternative approach, we can also try to trigger the alerts from the dashboards themselves. So, go to the LHS nav >> Dashboards>>Create a new dashboard >>  General tab >> Give a title 'Alerts Demo' >> Save it. Post that, open this Dashboard that we have just created and the click on Edit >> Add >> Visualization. Make sure on the RHS section under the Visualization label, the 'Time Series' is selected. In the center bottom section under the Data source, select 'Prometheus'. This time I will try to create an alert for my cards ms. Under the A section in the same center bottom section, okay if needed you can also change the value i.e., 'A' to something else. But by default, prometheus will follow the sequence like A, B, C D, ...etc. So, under A I am going to mention the metric name which is 'Up'. For this metric the label filter I am going to select as 'Job' and value as 'cards'. After defining this query for this A section. In the same center bottom section nav, you can see a tab 'Alert'. Click this option >> Befoe we try to create a new Alert rule, on the RHS section, make sure we are giving a proper name/title for this panel i.e., 'CardsUp'. Now lets edit more what we have started , in the alerts tab click on the button 'New alert rule'. This will directly take you to the Alerting>>Alert rules page. Here we will use the same kind of conditions that we discussed in details in approach one.
 *  - Here I didn't define any Section B and section C related configurations as we did in approach1. For step 4 - Set evaluation behavior, I created and selected the evaluation group as 'Cards'  with an evaluation period of 10s and with a pending period of 20s.Once you're done with step4, if you can scroll down to step 5 - configure notifications, you can set your contact point to the 'Get_tt_right Webhook' that we configure initially from approach 1 discussion. Then under step 6 - Configure notification messages, you can set your notification message/summery to something like - "Cards service is down" and inside the description you can say something funny like "Please do something". If you want to add more details or more annotations about your alert, you can do so by clicking on the 'Add custom annotation' button. At last, save rule and exit. Like this, you can see that for our panel we have 1 alert set-up under the 'Alert' tab. You can now go back to the Alerts Demo dashboard in which we have created one panel named CardsUp and for this panel we have happily defined an alert for it. Right now the panel is showing last 6 hours data. You can change this to a smaller value if you want. I selected last 5 mins. You can visualize that in the last 5 mins the value is always 1 which means the cards service is running without any issues. For my instructor in his panel - of course for the older grafana, I mean by the time he was doing his presentation like 2 years ago, at the top LHS of his panel, we have a green heart symbol which is an indication that there are no issue with our alert rule/panel that we have configured for the cards service. It is indirectly conveying that there are no health issues with the cards service/for our configured alert on cards ms. Now, I will go to the docker desktop and will try to stop our cards service. Now back to our panel, if you try to refresh it, you can see the graph drops to zero. After few second if you refresh again you will see a yellow vertical line in the graph which indicates the point in time our configured alert got into pending status from the normal status. After sometime if you refresh the panel you will see a red vertical line which indicates our alert got triggered, nothing but the alert got into the firing status. You can confirm from or hookdeck page - console.hookdeck.com - that you are receiving the notifications. Now if you go back to the docker desktop and start your cards ms again, you will see the panel is back to the normal state after some time and the alert will stop firing and sending notifications to our contact point(s) in our case console.hookdeck.com.
 *   So in between the yellow and red vertical lines, evaluation will still be happening based upon our configurations and also between the red line and the point where our ms gets back to normal, nothing but when our configured alert us resolved. In the console.hookdeck.com page, the latest notifications will always come up. You can open any one of them and analyse what information it holds in the body. So, far you should have clarity with this approach as well. It was the smoothest actually! haha. And btw instead of refreshing manually you can always select to auto refresh it at a particular time interval. With this we completed all the discussions about micrometer, prometheus and grafana and with the help of these 3 components we are able to set up the metrics aggregation,monitoring and alerting for our services.
 * Check last slide of our pdf on - Metrics and monitoring with spring boot actuator , micrometer, prometheus and grafana for a quick summer on what we have discussed so far.
 *
 *
 *  * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
