package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 06/10/2025
 * Deploying the K8S Dashboard UI
 * ----------------------------------
 * As of now we have been trying to interact with our K8S cluster only with the help of kubectl cli commands from our local terminal. Like discussed before, we also have another approach which we can interact with the K8S cluster which is with the help of Admin UI. With this, we can see the overall status of the K8S cluster. So, having a UI for your K8S cluster is going to make your life super easy, that's why inside this session, we are going to see how to set up a K8S Admin UI dashboard. For the same, use the official website page i.e., https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/. This will guide us on how to set up a K8S dashboard inside a K8S cluster.
 * In the page you can see an image of how the K8S dashboard that we are just about to set up is going to look like. under the section labelled 'Deploying the Dashboard UI', you will see a note saying that, 'The Dashboard UI is not deployed by default' This is inside any K8S cluster. So, we need to follow certain steps which have been listed for you under that section. Based upon the instruction there, you can see that we can only install the K8S dashboard only with the help of helm. What is helm? It is a package manager for K8S, just like we have node for JS world, very similarly for K8S related installations, helm is a package manager using which we can install any kind of components inside a K8S cluster. In the coming sections, we have a dedicated discussion on helm which we will in details explore.
 * For now, no need to worry about helm. We are focusing on setting up the K8S dashboard with the help of this helm. You can see, that we have certain commands that we need to execute in order to set up the K8S dashboard i.e.,
  # Add kubernetes-dashboard repository
   helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  # Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart
   helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
 *
 * In order to execute the above commands, first we need to make sure we are installing the helm inside our local system. So, to install the helm, we need to visit a website which is https://helm.sh/.  Inside this website you can click on 'Get Started' >> On the LHS nav click on 'Installing Helm' that is under the 'Introduction' part. Here you will find details on how to install helm on various OS's. There are multiple approaches that they have provided but the easiest and recommended way by my instructor is Through Package Managers. Actually if you scroll a little bit down you will see this section. Helm is a package manager which we can install with the help of other package managers. For macOS you should already have Homebrew which is a package manager. Actually there is a command there that you can run to install helm inside your local system. For Windows OS users, first you need to install 'Chocolatey' package manager.
 * If you can click on the Chocolatey link, it will take you to the website page i.e., https://chocolatey.org/. Here, there is an option of Install. Follow the instructions on that page to install Chocolatey inside your OS. For step 1 '1. Choose How to Install Chocolatey:' you need to select the 'Individual' option and actually it is selected by default for you. For a detailed lecture on how to install helm, there is one in our next sessions on how to install helm in our local system. For now try by yourself by following the instructions in the official doc page i.e., https://chocolatey.org/install as it is very straight forward. Once you install Chocolatey inside your local OS, you need to run the command 'choco install kubernetes-helm'. This is how it is pretty easy to install helm. Once you install helm, you need to verify if the installation is complete or not.How? Easy, just execute the command helm version. You should be able to get a proper output on what is the current version of helm installed inside your system.
 * Now back to the page, 'https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/' under the section, 'Deploying the Dashboard UI', as a next step, we need to take the very first command i.e.,
  # Add kubernetes-dashboard repository
    helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
 * For now, don't worry much about these commands as we are going to discuss in details about helm in the coming sessions. On executing the command for the first time, you will get an output like, '"kubernetes-dashboard" has been added to your repositories'. This confirms that adding a helm repo inside your system is successful. Execute the second command i.e.,
 # Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart
    helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
 * This is going to install K8S dashboard inside my local K8S cluster. Once the installation is successful you are going to get a message output like below:
 PS C:\Windows\system32> helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard
 Release "kubernetes-dashboard" does not exist. Installing it now.
 NAME: kubernetes-dashboard
 LAST DEPLOYED: Tue Oct  7 08:05:24 2025
 NAMESPACE: kubernetes-dashboard
 STATUS: deployed
 REVISION: 1
 TEST SUITE: None
 NOTES:
 *************************************************************************************************
 *** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***
 *************************************************************************************************

 Congratulations! You have just installed Kubernetes Dashboard in your cluster.

 To access Dashboard run:
 kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443

 NOTE: In case port-forward command does not work, make sure that kong service name is correct.
 Check the services in Kubernetes Dashboard namespace using:
 kubectl -n kubernetes-dashboard get svc

 Dashboard will be available at:
 https://localhost:8443
 * So before running the command make sure your Docker desktop is up and running and your local K8S is running in the Docker Desktop. If you get any errors or warnings make sure that you have the latest version of docker desktop, on top of that you should have a local K8S cluster running inside your system and this is what we set up inside our previous sessions.
 * You can also see more instructions in the output, I mean, In order to access the dashboard you need to run the command i.e., kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443. As soon as I execute this command you should be able to see an output like below:
            PS C:\Windows\system32> kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
               Forwarding from 127.0.0.1:8443 -> 8443
               Forwarding from [::1]:8443 -> 8443
 * This means that right now, my K8S cluster is exposing the dashboard to the outside world at the port 8443. So, once you execute this command, don't try to disturb this terminal. Reason: If you try to close or even execute some other command on this terminal, then the exposing of the traffic to the outside world is going to be stopped and with that you will not be able to access K8S dashboard inside your system. Once the command is executed, you can see even the URL that you can use to access the K8S dashboard inside your system i.e.,
     Dashboard will be available at:
      https://localhost:8443
 * As soon as I press enter in the browser tab on pasting this url, since my connection is 'https with certificate' which is generated by my K8S cluster, my browser is not considering that as a valid certificate, but for our local testing this should be fine, so you can advance to accessing your K8S dashboard site. You at first should be able to see the login page for the K8S dashboard. Here, it is expecting a token as an input, once we enter that token inside this page, we will be able to see that dashboard UI. How to get this token? We will get to know in a few. But before that, one important note is that, as of npw you can visualize that we are able to access the dashboard at the port 8443, in some sessions you may see a port which is 8001, this should not worry you as the approach that my instructor was following when setting K8S dashboard very first time, they used to have a different lengthy approach and in that legacy or deprecated approach, the dashboard used to get deployed at the port 8001. But with this latest approach/the helm approach that we have just discussed so far, the K8S dashboard is going to spin up at the port 8443. That's why if you see the K8S dashboard in my instructors sessions running at 8001, let that be your last worry.
 * Back to our official site, https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/, under the section labelled, 'Accessing the Dashboard UI' you can see a hyperlink, which is going to help us on how to create a sample user. This will take you to a GitHub page i.e., https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md. Here you will find clear instructions on how to create a sample user. We will try to understand them and follow the same inside our local system as well. You can see that we first need to create a 'Creating a Service Account'. Inside k8S we have a concept like Service account which is nothing but a credential which you are trying to create for a particular user. How to create this service account, you need to write the yml configurations i.e.,
 	apiVersion: v1 # Represents what is the API version. Using this,we are telling to the K8S cluster that we are trying to use the version one of API to create to create a service account.
 	kind: ServiceAccount # Under the key 'kind' I have mentioned the value, 'ServiceAccount'. This, 'ServiceAccount' is a predefined object inside the K8S. So, whenever the K8S cluster sees the value 'ServiceAccount' to the key 'kind', it is going to create a service account for me.
 	metadata: # Under the 'metadata' configuration, we need to define what is the name of the service account.
 		name: admin-user # The name of the Service Account that I want to create is 'admin-user'. Think of this as your username.
 		namespace: kubernetes-dashboard # By default if I don't mention/define the namespace where this service account has to be created, my K8S cluster is going to create it inside the default namespace. But since we don't want the Service Account to be created inside the default namespace, we need to also define/mention/provide what is the namespace as part of the metadata configurations as well. The namespace that we are providing is 'kubernetes-dashboard'. What is a namespace inside the K8S cluster? It is a kind of boundary or an isolated area inside your cluster. Just like we have dev. qa and prod environments. Very similarly, inside the K8S also, with the help of names spaces we can create separate separate isolated areas under which we can create various objects like 'ServiceAccount
 *
 * This you will need to provide to the K8S cluster with the help of 'kubectl apply -f <filename.yml>' command. Now, create a file with the name 'dashboard-adminuser.yaml'. For the same inside my terminal, I am going to open a new tab/terminal, I will not disturb the existing one because it has opened a proxy port using which I can try to access my K8S dashboard. Now inside the new terminal, navigate to a folder which is 'C:\Users\pc\Desktop\K8s'. Next run the command 'touch dashboard-adminuser.yaml'. Open this file and paste the configurations that we have just discussed from the GitHub repo and paste them there. Those are the yml configurations that will help us to create a service account with the name admin-user. Now, as a next step I need to provide these configurations to the K8S cluster. For the same we need to run the command which is 'kubectl apply -f dashboard-adminuser.yaml' The -f flag indicates what is the file name, nothing fancy. On executing this command, you can see that behind the scenes a ServiceAccount with the name admin-user is created as can be seen from the output, 'serviceaccount/admin-user created'. My instructor got an output like, 'serviceaccount/admin-user unchanged'. This is because he had already executed this command behind the scenes. But if you are trying to run this command very first time you should get an output i.e., 'serviceaccount/admin-user created'.
 * Now, as a next step we need to create 'Creating a ClusterRoleBinding'. As of now we have a ServiceAccount, but we need to provide privileges to this account that we just created with the help of ClusterRoleBinding. For the same, we need to create a new file i.e., dashboard-rolebinding.yaml. The file name can be anything. Copy and paste the contents from the GitHub repo. The yaml configurations look like below:
 		apiVersion: rbac.authorization.k8s.io/v1 # Under the 'apiVersion' for this kind of 'ClusterRoleBinding' we have to mention the value 'rbac.authorization.k8s.io/v1'. For the ServiceAccount kind, we so that the apiVersion value is simply 'v1'. All these values are going to be present inside the official K8S documentation. Always be referring to the same if you have any questions.
 		kind: ClusterRoleBinding # This time you can see that the kind is going to be 'ClusterRoleBinding'. Reason: We are trying to bind a Cluster Role to the ServiceAccount that we have created previously
 		metadata: # Under the metadata configurations, we need to mention child definition(s) i.e., what is the name of this 'ClusterRoleBinding'. I am giving the name as 'admin-user' just like how we have given for the ServiceAccount. After defining this name value, your 'ClusterRoleBinding' will expect 2 informations i.e., What is the Cluster Role that you want to bind and to which ServiceAccount/User you want to bind.That's why under the roleRef configurations we need to give/define 'apiGroup', 'kind' and 'name'
 			name: admin-user
 		roleRef:
 			apiGroup: rbac.authorization.k8s.io # This is always going to be a constant value. You can see that this is the same as what we have defined as a value to the key 'apiVersion'
 			kind: ClusterRole # This has to be 'ClusterRole' because we are trying to bind a ClusterRole inside the K8S cluster. All role has to be of kind 'ClusterRole'
 			name: cluster-admin # The name of the cluster. This means, we are trying to assign a role 'cluster-admin' - It is a predefined role inside the K8S cluster. Whoever has this role, they will become the Admin of the K8S cluster. Like this we have mentioned which role we want to bind. As a next step, we need to tell, to whom we need to assign this role. All this we can define under the 'subjects' configuration.
 		subjects: # Under the subjects, we need to start a new array element. Inside yaml, whenever we want to start array element or list element, we need to mention the hyphen. So, under subjects, we can mention/define any number of Service Accounts/User details. But in our scenario since we only have one ServiceAccount, we are mentioning/defining only one single element under the subjects. Under each element of the subjects, we need to mention/define what is the kind, name, and what is the namespace.
 		  - kind: ServiceAccount # The kind of User that we want to assign this role is ServiceAccount, that's why we need to mention the same.
 			name: admin-user # What is the service account name. We gave as 'admin-user'. So the same we also need to mention here.
 			namespace: kubernetes-dashboard # The namespace we gave also we need to mention the same here
 *
 * Now, our file is ready, apply it also so that the role of cluster-admin will be bind to the user 'admin-user'. For this, run the kubectl apply -f dashboard-rolebinding.yaml. You will get an output i.e., 'clusterrolebinding.rbac.authorization.k8s.io/admin-user created'. My instructor got the output i.e., 'clusterrolebinding.rbac.authorization.k8s.io/admin-user unchanged' because he had already executed these configurations.
 * As a next step as guided in the github repo i.e., 'https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md' labeled, 'Getting a Bearer Token for ServiceAccount', we need to run the command i.e., 'kubectl -n kubernetes-dashboard create token admin-user' in order to get a token for the 'admin-user'. '-n' in the command indicates what is the namespace, and that's why you can see, just after this flag, we are specifying the namespace i.e., 'kubernetes-dashboard' that we are working on where we have our ServiceAccount/User. Followed by 'create token' and finally the service account name i.e., 'admin-user'. This command will generate a token output for my admin-user. This you need to copy and mention the same in the K8S Admin dashboard where you were bing prompted to input it. i.e., https://localhost:8443/#/login. Once you paste it, click on the sign in button and hurreey! you should be happily redirected to the K8S Admin dashboard. At the top LHS dropdown, by default the selected namespace is going to be 'default'. If you scroll on this dropdown and select other namespaces i.e., 'kubernetes-dashboard', you will be able to see that there are 2 deployments running with 2 different pods and 2 different replica sets. Reason: To see just this dashboard, behind the scenes some code has to be deployed and that's why we are bale to see these deployments, pods and replica sets.
 * Just on this dashboard, we can explore a lot of information. We will explore a lot even in the coming sessions. For now we are trying to explore/discuss about a particular topic. As of now we created a ServiceAccount and ClusterRoleBinding under the namespace 'kubernetes-dashboard'. That's why in the K8S Admin dashboard, I selected the same namespace from the dropdown at the top LHS of the Admin dashboard. Inside the same namespace only, the deployments related to this UI are created and that's why under the 'Workload Status' we are able to see the 'deployments', 'pods', and 'replica sets'. On the dashboard on the LHS nav, if you scroll down, under 'Cluster' there is information related to 'ServiceAccounts'. Here you should be able to see the ServiceAccount named 'admin-user' that we created earlier on. So, to understand what role we have bound for this user, on the LHS nav, under 'cluster' you can see a nav link 'Cluster Role Bindings', here we have also given the same name which is 'admin-user'. If you try to open that, here you can see that it is clearly stated/highlighted that for so-and-so subject, which is 'admin-user', under the namespace 'kubernetes-dashboard' which of kind 'ServiceAccount' we have assigned the role 'cluster-admin' - this you can see under the sections 'Subjects' and 'Resource information' respectively. If you try to click on nav link, cluster-admin, labeled 'Role Reference' under the 'Resource information', you will be taken to the page related to 'Cluster Roles'. This you can also find on the LHS nav under the 'cluster' module. So, whoever has this role, 'cluster-admin' assigned, they will have the rules defined under the 'Rules' section applicable. Here, you can see we have '*' symbol on at the 'resources' field which implies that they can invoke all kind of resources with all kind of verbs and api groups. This is what the '*' symbol on the fields 'Resources', 'verbs' and 'Api Groups' are trying to communicate to you. haha, simple english.
 * Very similarly, under the 'Non-Resource URL' field also you can see a '*' symbol. What does all this we have seen under the 'Rules' section tell you about the role 'cluster-admin' haha. If this is assigned to a user, in our case we assigned it to the user/service account 'admin-user'. This simply implies that this user/service account is a superuser, as they can perform any kind of operation inside the K8S cluster. Hope you are clear.
 * Back to our GitHub page i.e., https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md under the section 'Getting a Bearer Token for ServiceAccount', you can see that whenever you want to get a bearer token, there is a command you need to run. And btw, this token is going to be short-lived which simply means, if you don't use your dashboard, it is going to expire and everytime you want to access the dashboard, and you are asked for a token, you need to run this command. Otherwise, if you are looking for a long-lived bearer token, under the section 'Getting a long-lived Bearer Token for ServiceAccount', we have some instructions that you can follow which are: You need to create a secret by using the yaml configurations specified i.e.,
 	apiVersion: v1
 	kind: Secret  # You can see the kind is mentioned as 'Secret' here. So 'Secret' is a concept inside K8S using which we can store our sensitive information like pwds. Okay for this k-v, you can see we are trying to create a Secret with the 'name' key under metadata config being 'admin-user' under the namespace 'kubernetes-dashboard'.
 	metadata:
 		name: admin-user
 		namespace: kubernetes-dashboard
		annotations: # And whatever secret that we are using we are assigning to the service account with the name 'admin-user'. That's how you can read the k-v definition of 'kubernetes.io/service-account.name: "admin-user"' that is under the 'annotations' config.
 			kubernetes.io/service-account.name: "admin-user" # This means nothing but, we are creating a secret for the 'admin-user' service account/user that we have created previously.
 	type: kubernetes.io/service-account-token # Under this 'type' definition, you just need to mention this constant value i.e., 'kubernetes.io/service-account-token'. Reason: It because with the help of the secret we are trying to create a token for the service account/user. That's why we need to mention this 'type' value i.e., 'kubernetes.io/service-account-token'
 *
 * Now, let's explore how we can create this long-lived bearer token so that it will make our lives easy. For the same, I am going to copy the yaml configuration and paste it into a file I am  creating inside the C:\Users\pc\Desktop\K8s location i.e., touch secret.yaml. On saving the file, execute the command 'kubectl apply -f secret.yaml'. You should get an output like 'secret/admin-user created' which can be read as 'the secret to the admin-user is created successfully.
 * As a next step, back to our GitHub page i.e., 'https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md' under the section labelled 'Getting a long-lived Bearer Token for ServiceAccount' you can run the command mentioned i.e., 'kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" | base64 -d'. The output will be a long-lived token that will be generated for you behind the scenes. This token that I received as an output is going to be the constant token all the time. With this approach, I don't need to get a new token with the help of this command which is 'create token for the admin-user' i.e., 'kubectl -n kubernetes-dashboard create token admin-user'.
 * Since the long-lived token is going to be super convenient, I am going to copy and save this token somewhere as I will be using this anytime I want to login into the K8S Admin dashboard. If you copy and paster the token the way you got it in the output, I mean everything completely, and pass as an input while loging into the K8S Admin dashboard, you will run into a 401  error i.e., 'Unauthorized (401): Invalid credentials provided'. Reason, inside the terminal, whenever we try to run the command, 'kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath="{.data.token}" | base64 -d', in the ouptut we will get some suffix characters that we need to omit i.e., '/usr/bin/base64: invalid input'. For my instructor it was '%'. So make sure to omit that while copying the token value to use as input while loging into the K8S Admin dashboard. I saved my token in at the location 'C:\Users\pc\Desktop\K8s' in the file 'Token.txt'
 * S, while login into my K8S dashboard and it asks for the Token value, I will give this always, if you don't follow this long-lived token approach, you will be needed each time to execute the command 'kubectl -n kubernetes-dashboard create token admin-user' which is going to give different different tokens at different different times. With this, you should have crisp clarity on how to set up the K8S dashboard.
 * 
 * Deep Dive on K8S YAML configurations/Manifest Files.
 * -----------------------------------------------------
 * As of now, we have our local K8S cluster running successfully inside our system. As a next step, we should try to deploy all our ms's into this K8S cluster. Post that, we can also validate all the features provided by the K8S to make our lives easy during the deployment of ms's. To get started with the deployment of our ms's, first we need to give instructions to the K8S on how we want to deploy our ms's and how we want to expose them. So, all this information we need to provide it in a yaml configuration. Whatever we have been using with the help of docker compose, we cannot simply give those to the K8S because K8S cannot understand the format or the syntax of the docker compose. K8S has its own format/syntax that we need to follow while defining 'how we want to deploy our ms's' and 'how we want to expose them'. First, we will try to deploy the config server into the K8S cluster because whenever I want to set up all my ms's into a K8S cluster the very first ms or application that I want to deploy is 'ConfigServer'. For this reason only, I am going to consider config server in this session. In the same process, we will be learning on how to provide instructions to the K8S cluster in the format of yaml.
 * To get started with the deployment of config server, follow the below steps:
  1. Inside our lobby folder where v2-spring-cloud-config folder is existed, create a folder i.e., K8S.
  2. Open a terminal inside this folder location. Then touch configserver.yaml. Using this file we will be providing instructions to the K8S cluster on how to deploy my configserver. That's why I am trying to give the name configserver.yml. Inside this file, I have pasted the below yaml configurations as discussed line by line. And please note that, In real projects a developer is not responsible to write this K8S configuration files. The DevOps/Platform team members will be responsible for writing these configurations. But as a developer, you should always be aware about the basic syntax of writing K8S configurations files. That's why inside this session, I am trying to explain them. Now let's discuss obout these configurations.
 apiVersion: apps/v1 # Before the kind 'Deployment', make sure you are mentioning the 'apiVersion' as 'apps/v1'. And btw, you can always get the syntax of this files from the K8S official documentation i.e., https://kubernetes.io/docs/concepts/workloads/controllers/deployment/. You can follow thr crumb i.e., Kubernetes Documentation - Concepts/Workloads/Workload Management/Deployments. So yea, this is the K8S website where we have instructions about deployments. Inside this page, if you can scroll down, there is a syntax on how you can give the deployment instructions to your K8S cluster. The same kind of syntax we are trying to follow for our ms's as well. It might look complicated but don't worry, with multiple practice deploying ms's to a K8S cluster with the help of this K8S yaml config file(s), you are going to get used to this syntax just like how right now you are comfortable with the docker compose format. So, whenever you want to provide the deployment instructions to the K8S cluster, the first two definitions i.e., 'apiVersion' and 'kind' are always started.
 kind: Deployment # Whenever we want to deploy our ms's into the K8S cluster, we need to create a configuration of 'kind' 'Deployment'. This kind 'Deployment' is a predefined object inside K8S. If you try to give the kind as 'Deploy' instead of 'Deployment', it is not going to work, your K8S cluster is going to throw an error. That's why make sure you are always mentioning this kind as 'Deployment'
 metadata: # Next is the metadata configurations. Here we are defining/providing some metadata information about these deployment instructions. Under the metadata like you can see, I am providing definitions for 'name' and 'label' related configurations.
 	name: configserver-deployment # I am giving a value to the name as 'configserver-deployment'. This name value can be anything. We are trying to give a name to our deployment, using the same name, we can always try to search inside the K8S cluster. So, just to differentiate between multiple deployments we should always make sure we are giving a unique name for your deployment instructions. Here, since I am trying to deploy my config server, I am giving the name 'configserver-deployment' which is going to be very apt for this scenario.
 	labels: # Here I am defining a key 'app' and the value being the app name i.e., 'configserver'. These label related configurations are very important. There importance we will discuss in the next sentences. For now, you can see, under the 'metadata' configurations that we have just discussed, for this deployment I have given a name i.e., 'configserver-depolument'. I have created a label i.e., which is app as 'configserver'.
 		app: configserver
 spec: # After the metadata related configurations discussed above, we need to define the specifications related configurations i.e., spec related configurations. Since we are trying to deploy our ms into a K8S cluster we need to provide the specifications. How many replicas we want to deploy? What are the container details? What are the docker image details? So, all such details we are going to mention under this specifications related configurations.
 	replicas: 1 # As of now, I am trying to deploy only a single instance of my configserver. Instead of 1, if you mentioned 3 then, your K8S cluster behind the scenes is going to deploy your config server in 3 different pods with 3 different instances. So, based upon your traffic needs you need to define your number of replicas.
 	selector: # After the replicas definition, with the help of selector.matchLabels property, we need to mention, whatever label that we have previously defined under the metadata related configurations which is 'app: configserver'. with the help of selector.matchLabels property, we are trying to convey that, 'Whatever specifications configurations that we are trying to define here, I want them to be applicable for the deployment named 'configserver-deployment' which has the label 'app: configserver'. That's the reason we are mentioning under the selector.matchLabels property the same thing i.e., 'app: configserver'. So, behind the scenes my K8S will do/perform the matching of labels with the lebels that we have defined under the 'metadata' related configurations. That's why always make sure you are mentioning the same 'configserver' app name/value defined under the metadata related configurations here under the matchLabels as well.
 		matchLabels:
 			app: configserver
 	template: # The next specification is the template related configurations. With this we are trying to provide what is the template instructions on how we want to deploy our ms.
		metadata: # Under the template, again we have the metadata information.
 			labels: # Here again we need to mention labels
 				app: configserver # app with the value configserver.
 		spec: # After the metadata, again the specifications related configurations. But these are related to the 'template'. The ones that we discussed earlier are related to the deployment. Like this under the deployment specifications, we are going to define/create a template and under this template we are going to provide specification related to the template.
 			containers: # Now, under the specification related configurations that is under the template, we are going to provide the important information with the help of 'conatiners' related configurations'. If you see here, under containers, we have mentioned/defined the list element with the help of this hyphen. So, as of now we have only defined a single container related configurations as discussed below. So, with this single container information, my K8S cluster will create deployment and as part of the deployment a pod will be created inside one of the worker node and my pod is going to have a single container. What if your configserver needs some helper containers? In such scenarios, you need to repeat the container instruction like we have defined in the first element of the list, with a new list element under this containers. But as of now, since we don't need any helper container for our config server, we have only mentioned one list element for a single container specifications
 			- name: configserver # Under the containers, you need to mention what is the name of your container
              image: colince819/configserver:v4 # and what is the image name that your K8S cluster has to consider to deploy your ms. Here I have mentioned the one with the tag v4 that represents tha latest stable version of configserver that we have. So, this is my complete image name i.e., 'colince819/configserver:v4'. You may have a question like, but how will my K8S cluster know whether my images are present inside the dockerhub or Amazon container registry or any other container registry? During our docker sessions, like we discussed, whenever we don't mention the container registry information internally it is going to consider 'docker.io/colince819/configserver:v4 by default. If you have your images in some other registry that is not dockerhub then you need to mention those container registry details in place of the prefix 'docker.io'. But since right now we have our images inside the dockerhub then no need to mention the prefix 'docker.io' for the registry details. So, 'docker.io/colince819/configserver:v4' can be considered as the fully qualified image name of our ms present inside the respective registry, in this case dockerhub.
              ports: # After the image, under the ports we need to mention what is the container port, like at which port do we want our container to get started. Like we know the configserver has to get started at the port 8071 and that's why  I have give the same.
              - containerPort: 8071
 ---
 apiVersion: v1 # Whenever we are trying to create the 'Service' configuration, we need to make sure we are mentioning the 'apiVersion' as 'v1'
 kind: Service # And the 'kind' has to be 'Service', just like we saw for the deployment related instructions/configurations, the kind is 'Deployment'. Since right now we are giving the service related configurations, we have to give/define/mention the 'kind' as 'Service'
 metadata: # Under the metadata we need to mention a child property 'name' and give it a value which is 'configserver'. It's the service name.
 	name: configserver # With this definition, I am trying to give a name to my service which is 'configserver'. Please note that we are following the same service name like 'configserver' because whatever you are mentioning here, using the same only, my other ms's inside the K8S cluster they are going to communicate with each other. So this definition is going to act as a hostname inside your K8S cluster. So please make sure this name you are giving as 'configserver' otherwise you will need to make changes in lot many other places when we will be deploying other ms's. Now, after defining this name for this Service, next we need to define/provide the specifications.
 spec: # Whatever Specifications that we are trying to define here, we want to apply that for app with the name 'configserver' and that's why we need to use this slector.app propertyvto define/mention the app name that we want to appy the specifcations related configurations under this 'Service' related configs.
 	selector:
 		app: configserver # If you see here, the same 'app: configserver' I have mentioned under the 'Deployment' related configurations - that is under the 'metadata.labels.app:configserver'. This tells to my K8S cluster, whatvere 'Service' related configurations that I am defining here, please apply to the container(s) that I have created with the help of 'Deployment' instructions with the same label app name i.e., 'configserver' That's how my K8S cluster is going to establish a link between a 'Deployment' and a 'Service'
 	type: LoadBalancer # Under spec.type, we are assigning a value i.e., 'LoadBalancer'. Whenever we mention this, it is an indication to the K8S cluster that we want to expose our ms to outside world which means nothing but to the outside of the K8S cluster. For now, note that this definition/mention i.e., type:LoadBalancer will expose the container(s) to the outside of the cluster. Later on we will discuss what other options that we have. This spec.type property inside the 'Service' related configurations is going to be super important becuase based on this property only, the behavior of exposing our container(s) will be controlled.
 	ports: # After 'type' we need to mention 'ports' and under 'ports' we need to mention child properties i.e., 'protocol' 'port' and 'targetPort'
 		- protocol: TCP # We are assigning this a value i.e., 'TCP' since we want to communicate over web.
 	      port: 8071 # Now since with the mention i.e., spec.type:LoadBalancer we are trying to expose this configserver related container(s) to the outside world, we need to mention at which 'port' it has to expose. So, whatever value I have mentioned/assigned in the 'port' property definition is the port that is going to be exposed to the outside world.
          targetPort: 8071 # Whereas whatever I have defined/mentioned here as the targetPort, this is the port where my container is going to start internally inside the K8S network. Here, we need to make sure the value we mentioned for the property 'spec.template.spec.containers.ports.containerPort' under the 'Deployment' related configurations matches whatever we are mentioning here under the property 'spec.ports.targetPort' of 'Service' related configurations. Reason: If you try to define/mention different values K8S is going to throw an error.
 *
 * As of what we have discussed so far, the most important information that I want you to consider is, the definition i.e., app: configserver, has to be the same in all the places it is defined. Like initially under the metadata related configurations, we have given a label for our deployment with the k-v 'app: configserver'. The same definition, we need to mention under the specifications related configurations under the selector.matchLables and that's how the K8S is going to map these specifications to this 'configserver-deployment' because both have the same k-v definitions which is 'app: configserver'
 * Very similarly under the specifications, template child property configurations, under metadata.labels we need to mention the same k-v i.e.,app: configserver and since this template related configurations has the same label, those template related instructions/configurations will be considered for the same deployment which is 'configserver-deployment'. So with all this that we have discussed, deployment instructions related to configserver, inside configserver.yaml, now my configserver docker image will get deployed as a docker container inside my K8S cluster. But the story is not going to end here haha. With the deployment, we are simply deploying our ms into a K8S cluster but how about exposing that container to the outside world or restricting that container only for the internal communication. So, to control on how we want to expose our ms container to the outside world, we need to create, we need to create one more K8S object with the name/key -value i.e., kind: Service
 * In between the 2 configurations i.e., 'Deployment' related configurations and 'Service' related configurations you can notice 3 hyphens. Do you know the meaning of 3 hyphens inside yaml file? Whenever we mention these 3 hyphens inside yaml file, it indicates to the yaml that please treat this single yaml file i.e., configserver.yaml as 2 separate yamls. Whatever I have mentioned on the top of the three hyphens all those configurations, Deployment configurations, will be treated a s a single yaml file and whatever is below the 3 hyphens will be treated as a second yaml file, Service related configurations yaml file. I could decide to define these 'Service' related instructions/configurations in another separate yaml file but this is going to be very difficult for me because whenever we want to provide some instructions to the K8S cluster, we need to provide the filename. So, if I have so many files, I need to run the apply command with all these file names. That's why wherever possible, I will try to define all the K8S yaml configurations to a single ms in a single yaml file, just like we have done for configsever.yaml 'Deployment' and 'Service' related configurations. And beheind the scenes since yaml has this feature of 3 hyphens to separate configurations in a single yaml file to act as 2 different yaml file, my K8S cluster is going to exacute these 2, 'Deployment' and 'Service' related configurations/instruction, one by one by seeing this special character which is 3 hyphens.
 * For some reason if you want to define more configurations in the same file, you can mention the 3 hyphens after the end of this 'Service' related configurations. This way you can define any number of yaml configurations in a single yaml file. What is present inside this 'Service' configurations I have discussed above using comments against each definition/mention.
 * With all we have discussed above you should now be having confidence and crisp clarity. Now, we have created and in details discussed the K8S configuration file for deploying our configserver into the K8S cluster. In other words, people also call/refer to this K8S configuration file as K8S manifest files. So yea, that's the industry standard - we should always call them as K8S manifest files. So, with this manifest files only we are going to deploy our containers into the K8S cluster and expose them to the outside world. Now, as a next step, we will try to use the configrations that we have just discussed above and try to deploy our configserver ms into the local K8S cluster.
 *
 * Deploying configserver into the K8S cluster
 * -----------------------------------------------
 * In our previous discussion, we have prepared the required K8S manifest file to deploy our configserver into our local K8S cluster. In this discussionm we will use the same and do the deployment of configserver into K8S cluster. For the same in the terminal, first we are going to verify that there are no pods, deployments and even services as of now in the default namespace. For the same I am going to run few commands i.e., 'kubectl get deployments' - This will show me all the deployments inside my K8S cluster. As you can see, as of now,'No resources found in default namespace.' is the output you will receive. Very similarly I can also look for services by running the command, 'kubectl get services'. For this you will get an output like below:
 NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
 kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d
 *
 * Under services, as of now you can see there is one default service related to K8S cluster is available. But, there is no service which is related to our ms's. The next command I am going to run is, 'kubectl get replicaset'. You will get an output i.e., 'No resources found in default namespace.'. This confirms that as of now our default namespaceis empty.
 * All these, instead of running commands, you can also confirm the same inside the K8S dashboard as well. Inside your K8S dashboard, please make sure from the dropdown at the top LSH of the dashboard, you are selecting 'namespace' as 'default'. Post that on the LHS nav, if you click on the 'deployments' link, as of now you will see there is nothing to display here. The same you can do for 'Pods', Replicasets', and  for the 'services', you can verify that as of now we only have one service entry which is related to K8S. Now, as a next step, we will try to deploy our configserver with the manifest file that we have just prepared in our previous discussions. First, we need to make sure we are in the same location where our configserver.yml manifest file is available. Here I am going to run the command which is 'kubectl apply -f configserver.yaml'. Don't worry about all these K8S commands that I am trying to run inside our sessions, my instructor will also provide them in his GitHub repo where you can quickly make reference antime and refresh yourself on them. On executing the command, I got an output saying that;
 deployment.apps/configserver-deployment created
 service/configserver created

 * You can read the output as, 'So and so deployment and so and so service has been created. Now If I try to run the commands like, kubectl get deployments I will get an output like;
 NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
 configserver-deployment   1/1     1            1           2m9s

 * Like this we are able to see one deployment with the name 'configserver-deployment'. Under the 'READY' field, it is showing an output like 1/1 which is nothing but, 'To maintain only one container or one replicaset' - This means that desired state is one and the actual state is also one. Ther is no issue here which means that our conatiner is in healthy status. Very similarly, I am going to run the command, 'kubectl get services' and you will see an output like;
 NAME           TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
 configserver   LoadBalancer   10.99.48.155   localhost     8071:31618/TCP   7m21s
 kubernetes     ClusterIP      10.96.0.1      <none>        443/TCP          14d
 
 * You should be able to see the new service that got created of 'TYPE' LoadBalancer. And btw, by default, each of your conatiner services is going to get a 'cluster-ip' address which is gping to make sense only within the cluster. But when you try to expose to the outside world, the 'EXTERNAL-IP' will be indicated under this field/column. Since this is my local system, for configserver service, you can see the external-ip is being indicated as 'localhost'. If you try to deploy into a cloud environment, you will see/get a real public IP address mentioned under the 'external-ip' addrees field fro your service(s) that you are exposing outside the K8S cluster. You can also see at the 'PORT(s) column/field that as of now, from the output, my container is exposed at the piort 8071. Later on, I will explain why we are seeing the other port which is '31618/TCP' when will be trying to discuss about LoadBalancer and other service types available inside the K8S. For now note that, our configserver container/service/ms is exposed at the port 8071 in our localhost. The next command that we are running is, 'kubectl get replicaset'. You should get an output i.e,
 NAME                                 DESIRED   CURRENT   READY   AGE
 configserver-deployment-5bf77dc4fb   1         1         1       19m

 * You can see the name of the replicaset that is created for the configserver-deployment. The Desired is one and the current is one and that's why my K8S is going to be very happy. For some reason if the current is zero, then immedeiately K8S will come into picture and it will take care of creating a new container in the place of defective container. Very similarly, we can try to run a command which is , 'kubectl get pods'. This will show you an output like;
 NAME                                       READY   STATUS    RESTARTS   AGE
 configserver-deployment-5bf77dc4fb-tcff2   1/1     Running   0          24h

 * You can be able to see/visualize what is the poid name where your ms/container is deployed. 'configserver-deployment-5bf77dc4fb-tcff2' is the pod name and as of now the 'STATUS' is running. We can also validate all these we have been discussing inside the K8S dashboard as well. On the LHS nav, if you click the 'services' link, you can be able to see the new service with the name 'configserver' of type 'LoadBalancer' and it's exposed external endpoint is going to be 'localhost:8071'. On the same LHS nav, if you click the link, 'deployments', as can be seen we have a deployment with the name 'configserver-deployment'. Next, on the LHS nav, if you click on 'pods' link you should be able to see a pod named, 'configserver-deployment-5bf77dc4fb-tcff2' where my container is deployed and it has used the image 'colince819/configserver:v4' and the labels you can also see in the field 'Labels'. Under the Name field, if you try to click on the Pod i.e., 'configserver-deployment-5bf77dc4fb-tcff2'. You will see a detailed page about our pod. Here also on the top RHS nav, you will see 3 and half parallel linesicon on which if you hover you will notice it is labelled 'view logs'. This is nothing but an option to view the logs of that pod. Her you will be able to see all the spring boot logs of your configserver. So, whenever you have some issues related to your container and you want to debug what is happening behind the pod, this is the place where you need to check the logs.
 * Similarly, on the LHS nav, you can check the replica Sets. As of now we have a replicaset named, 'configserver-deployment-5bf77dc4fb', if you get into it by clicking the link under the 'Name' field, you will be able to see the details about that specific Replicaset i.e., You will see a 'Pods status' section having what is the 'running' what is the 'desired'. There is also a pods sections which tells us under which pods the container is running. There is also a services section. This way, we can always check lots of information with the help of K8S dashboard. Now, as a next step, we are going to validate if we are able to access our configserver. For the same I am going to access the url which is: http://localhost:8071/accounts/prod - This is a random url that I want to access.You can see that we are getting the accounts ms related properties both from production as well as default profiles. Very similarly, I can check and verufy for loans, In the url just replace 'accounts' with 'loans' - You will get loans specific properties. I can also check for 'cards', 'eurekaserver'- For eurekaserver, you can commit the suffix i.e., '/prod' in the url because Eurekaserver does not have any profile specific properties. By default all the properties of eurekaserver are going to be available inside the default profile. etc. This confirms that our configserver is successfully deployed into the K8S cluster.
 *
 * * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
