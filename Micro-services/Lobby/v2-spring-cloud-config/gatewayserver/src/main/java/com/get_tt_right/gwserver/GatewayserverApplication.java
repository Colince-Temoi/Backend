package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 22/06/2025
 * Distributed Tracing with OpenTelemetry, Tempo and Grafana
 * -----------------------------------------------------------
 * - Like we said before, with the help of OpenTelemetry we are going to add the  traces and spans information automatically. OpenTelemetry also known as OTel, is a vendor neutral open-source observability framework for instrumenting, generating, collecting, and exporting telemetry data such as traces, metrics and logs. How this Otel is going to generate the traces information is, at runtime, it is going to attach some byte code to your ms application. Using the same byte code, it is going to attach all the tracing information, span information or any other metadata information. Once my Otel has generated all the logs with the required tracing information, we are going to use a component inside the grafana ecosystem which is Tempo. What is its purpose? Just like Loki for logs and Prometheus for metrics, similarly using tempo we are going to index all the tracing information. This Tempo again is an Open-Source product that is highly scalable and cost-effective solution. Using this Tempo, we are going to store and analyze all the trace information. So, Tempo is only capable of storing the tracing information. Therefore, we need some UI application which is going to connect to the tempo and display the same inside a UI page. And that's where Grafana will come into picture.
 *   So, as you can see the ecosystem of grafana is very beautiful and fascinating. They always have a common UI component which is Grafana. And based upon the DataSource/Connection details, it can connect with various components like Loki, Prometheus, and now Tempo. Based upon your scenario, from a central service which is grafana, we can search for logs information or tracing information or metrics information. That's why learning the foundational concepts of Grafana, and it's various components inside Grafana ecosystem is mandatory for any ms developer and even deeper for platforms/operations team member. So, using Tempo, grafana is going to show all the distributed tracing information in an easily understandable visual information. When we see the demo, it is going to be super crisp clear for you. As of now, we are done only with the changes related to OTel, As a next step, we need to generate the docker images for all our ms's and post that we need to update the docker compose file to implement the changes related Tempo and integrating the same with Grafana. In the docker-compose.yml file, we will do updates in regard to OpenTelemetry, Tempo and Grafana Integration of Tempo changes. First, inside the common-config.yml prod profile we will define environment properties that are going to be common for all ms's. For this, do under the 'microservice-base-config' service. So, under the 'environment' parent paste the below environment properties:
 *       JAVA_TOOL_OPTIONS: "-javaagent:/app/libs/opentelemetry-javaagent-2.11.0.jar"  -> With the help of JAVA_TOOL_OPTIONS, I am trying to pass '-javaagent' parameter along with the path where my otel jar - opentelemetry-javaagent-2.11.0.jar - is present. Since we have defined 'opentelemetry-javaagent' dependency inside the pom.xml file(s) of our service(s). This jar is going to be present under the path '/app/libs/' inside all the containers. That's why I have mentioned the same here.
 *       OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4318   -> With the help of 'OTEL_EXPORTER_OTLP_ENDPOINT' I am trying to mention 'tempo' service name and what is the port, 4318 , where my Tempo is going to expose its service. So, the OpenTelemetry is going to send all the tracing related information to this 'tempo' service
 *       OTEL_METRICS_EXPORTER: none   -> Using the property, 'OTEL_METRICS_EXPORTER', we are assigning a value as 'none'. Reason - We don't want to get metrics with the help of OpenTelemetry because, we already used a better tool whenever we are trying to export metrics which is Prometheus.
 *       OTEL_LOGS_EXPORTER: none     -> Same drill as 'OTEL_METRICS_EXPORTER' property explanation above. The better tool here is Loki.
 *   Now, inside the docker-compose.yml file, under all our services, we need to create an environment property i.e., OTEL_SERVICE_NAME, to it assign the respective service name. Like this we are trying to convey what is the respective service name to the Otel library, in our case - opentelemetry-javaagent-2.11.0.jar. In all the services, the 'SPRING_APPLICATION_NAME' and the OTEL_SERVICE_NAME' values are going to be the same. It is not mandatory that the SPRING_APPLICATION_NAME and OTEL_SERVICE_NAME need to have the same values, but we are just trying to maintain som consistency here. Now we have defined all the environment related properties, related to opentelemetry - inside common-config.yml and docker-compose.yml files. As a next step, we need to add the configurations related to Tempo. Before that under the 'observability' folder create a new directory with the name 'tempo'. Inside this folder, I am going to create a new configuration file which is related to tempo i.e., tempo.yml. You can always copy it from the github and paste it. Now, lets try to understand what is present inside the tempo.yml file. Check that out in the respective file for detailed explanation.
 *    . Inside the docker-compose.yml file, just above the 'grafana' service definition, we are going to define/create a new service with the name 'tempo' - Check that out for details.
 * As a next step, we need to provide the integration between grafana and tempo. Where we need to accommodate this? haha. With the help of datasource.yml file present inside grafana folder. As of now, we had created 2 datasource's with the names prometheus and loki. Now, very similarly we are defining/creating datasource details related to tempo as well - check that out inside the datasource.yml file. After making the changes, my grafana should be able to connect with tempo and show the distributed tracing information.
 * - Up to now, you should be clear with the changes that we have done. In the next session we will try to start all our applications and visualize the distributed tracing in action.
 * While spinning up my containers via the docker-compose up -d command, I encountered an issue like::
 *   configserver-ms | Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/libs/opentelemetry-javaagent-1.27.0.jar
     configserver-ms | Error opening zip file or JAR manifest missing : /app/libs/opentelemetry-javaagent-1.27.0.jar
     configserver-ms | Error occurred during initialization of VM
     configserver-ms | agent library failed to init: instrument
 * To resolve this, I followed the below advice from Madan:
 *  "Can you please check if your config server container has the opentelemetry related jar inside the path /app/libs
     If it is present, please try starting the configserver alone using docker compose and check if you receive the same error. Your docker might need some good RAM memory to load this jar, so please make sure to close all unnecessary applications."
 *  Notes: When I spin up the configserver on its own i.e., "docker run -p 8071:8071 colince819/configserver:v3", I didn't face any issues. I went further to the directory '/app/libs/' where the 'opentelemetry-javaagent.jar' was present and I noticed it was loaded sucessfully! but with a version 2.17.0 unlike what I had configured inside my common-config.yml file i.e., 2.11.0. Now, the reason for the failure when I was running the docker-compose up -d command is. Inside the common-config.yml I had configured a different version of 'opentelemetry-javaagent.jar' i.e., 2.11.0 which differed by what I had configured in the pom.xml files of all the microservices i.e., 2.17.0 before generating the docker images for the respective microservices. To fix this, I had 2 options, 1. I could change the version of 'opentelemetry-javaagent.jar' inside the pom.xml files of all the microservices to 2.11.0 and then generate the docker images. 2. I could change the version of 'opentelemetry-javaagent.jar' inside the common-config.yml file to 2.17.0 and then just right away spin up my containers using the docker-compose up -d command. I decided to go with option 2. I will use option 1 during the next milestone while generating other docker images for the microservices, while maybe doing a demo on ms's security concepts.
 * Demo of Distributed tracing
 * -----------------------------
 * We started all our containers from the prod profile with the help of docker compose up -d command. Before running this docker compose up -d command make sure to have generated all the services images based upon the latest changes that we have done related to Otel. WAfter running this command, if you wait for more than the expected time and see some of your custom services being Errored out. The reason this could be happening is because, for example if you go and open one of the custom ms's container, for example if the accounts ms errored out, if you open its console logs, you maybe suprised that the application started successfully without any issues with a message like; " Started AccountsApplication in xx.xxx seconds (Process running for xx.xxx) ". If keen enough you will notice that it took more time because if you try to see the very first statements in the logs file - The runtime library(s) are getting loaded/picked up, in our case opentelemetry-javaagent-2.17.0.jar. This it is trying to load into the memory. Since it is trying to load this jar or 20 or 30 mbs, whatever size it is, it is going to take few seconds. Since it si taking more time, the health checks that we have defined/mentioned in our docker-compose file are not working properly becuase if you go to the docker compose file and look for the health check information for all the custom ms's  we are only retrying for 10 times with a gap of 10 seconds. In short, after 100 seconds 0r 1.40 mins we are going to declare the custom ms's as 'unhealthy'.Hence, the reason why they errored out. To resolve this issue, we can try to give more time for our containers to prove there health status. That's why you can increas the interval to 20 or 30 seconds and the retries 20 to 30 as well inside all our custom ms's that we have inside the docker compose fle.
 * For gatewayserver we don't have any health check and since it does not depend on any other service to start, as in, it will start last after every other ms has proven healthy. You can save the changes and run the docker compose down to make sure we are deleting all the containers and then finally run the docker compose up -d command. It should work now. You should keep in mind that when you run the docker compose up -d command with many containers defined inside your docker-compose file - especially if they have a chain of dependencies - then it is going to take more time to start them all, especially because of the health checks that we have defined. This being attributed maybe to low RAM in you PC and reason for us increasing the retries to 30 and the interval to 30 seconds. In the docker desktop, you can easily confirm that all containers started successfully by checking with the logs of the last ms expected to start which is the gatewayserver. And as we have discussed, if you arbitrarily go to the any of the containers logs  console, the very first statement you are expected to see is "Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/libs/opentelemetry-javaagent-1.27.0.jar". If you try to click on the Files tab on your docker desktop, you should be able to see all the files present inside your container. You will see a folder with the name "app" and inside that folder you will see a folder called "libs" and inside that folder you will see the opentelemetry-javaagent-1.27.0.jar file alongside many other jars that we added inside the pom.xml file. Right now, we are particularly interested in the opentelemetry-javaagent-1.27.0.jar file. This jar we are trying to load  with the  help of JAVA_TOOL_OPTIONS that we defined inside the docker-compose.yml file configurations and that's why in the first lines of the container console logs you are seeing the statement "Picked up JAVA_TOOL_OPTIONS: -javaagent:/app/libs/opentelemetry-javaagent-1.27.0.jar".
 * So far you should be clear. Make sure to check the gatewayserver container logs to verify that its started successfully. Yes! it started successfully at the port 8072. As a next step, in the postman we will try to test few APIs. 1st we will try to create an accounts with the help of eazybank/accounts/api/create available inside the accounts ms. Do the same for cards - eazybank/cards/api/create and same for loans - eazybank/loans/api/create. All these, invoke them from the gatewayserver folder. As a next step, go to the FetchCustomerDetails API and try to invoke it. You should get success responses so far.  As a next step, we can go to the grafana and try to see how the tracing information is going to look like
 * Now, lets see the distributed tracing in action. For the same, on the grafana page click the explorer link. Then select Loki as we want to look for our logs information here. As you can see, we have 3 datasource's in the dropdown now i.e., Loki, Tempo and Prometheus. Now, select Loki. Under the Label filters Select Label as Container and Value as Accounts ms as we want to see the logs of accounts ms. If you run that query, and scroll down to the logs, all my loggers right now have some tracing information. If for example you search the string, "Fetch Customer Details Method End" as part of the query you should be able to see something like: 2025-06-25T04:41:36.166Z DEBUG [accounts,46c0673f49d126825f08d9f0b591f4be,8c9900e1f416da3f] 1 --- [accounts] [nio-8080-exec-1] c.g.a.controller.CustomerController      : Fetch Customer Details Method End. This is just one of the log statements that we added. The pattern it is following is; first it is trying to mention what is the severity of the log like 'DEBUG', 'INFO' or 'WARN' - that's why we have mentioned the Initial 5 characters in the logging pattern. Post that inside the square brackets we have the application name which is 'accounts' followed by tracing id information which is '46c0673f49d126825f08d9f0b591f4be' and then the span id which is '8c9900e1f416da3f'. So, this trace id is going to be common for all the services where this request entered . You can copy it and then, try to search for it inside gatewayserver, cards as well as loans ms's as we know very well that the FetchCustomerDeatails API is going to aggregate all the customer details in regard to accounts, loans as well as cards from the respective ms's. You should be able to see something like below:
 *  2025-06-25T04:41:36.078Z DEBUG [cards,46c0673f49d126825f08d9f0b591f4be,e545f3c060574fdb] 1 --- [cards] [nio-9000-exec-8] c.g.cards.controller.CardsController     : FetchCardDetails method End
 *  2025-06-25T04:41:36.031Z DEBUG [cards,46c0673f49d126825f08d9f0b591f4be,e545f3c060574fdb] 1 --- [cards] [nio-9000-exec-8] c.g.cards.controller.CardsController     : FetchCardDetails method Start
 *
 *  2025-06-25T04:41:37.423Z DEBUG [loans,46c0673f49d126825f08d9f0b591f4be,6df0b9561cb5eea4] 1 --- [loans] [nio-8090-exec-4] c.g.loans.controller.LoansController     : Fetch Loan Details Method End
    2025-06-25T04:41:36.201Z DEBUG [loans,46c0673f49d126825f08d9f0b591f4be,6df0b9561cb5eea4] 1 --- [loans] [nio-8090-exec-4] c.g.loans.controller.LoansController     : Fetch Loan Details Method Start

 *  2025-06-25T04:41:36.184Z DEBUG [gatewayserver,46c0673f49d126825f08d9f0b591f4be,32687cae2d28d2d3] 1 --- [gatewayserver] [or-http-epoll-1] c.g.g.filters.ResponseTraceFilter        : Updated the correlation id to the outbound headers: 3f2fa272-248c-460b-8efe-33a25787b77c
 *  2025-06-25T04:41:34.643Z DEBUG [gatewayserver,46c0673f49d126825f08d9f0b591f4be,32687cae2d28d2d3] 1 --- [gatewayserver] [or-http-epoll-1] c.g.gwserver.filters.RequestTraceFilter  : eazyBank-correlation-id generated in RequestTraceFilter : 3f2fa272-248c-460b-8efe-33a25787b77c
 *
 * The Trace Id is the same in all the services. Span is the same in each respective service and differ in different services. This way, we can use the same trace id and search the logs of all containers to understand which method is executed and if you have some logger statements inside your exception scenarios the same statements you can also be able to easily trace them.
 * We can also see the tracing information with the help of Tempo as well. So, far we've been seeing the information discussed above inside logs. With this tracing information, we can try to understand which methods are invoked, are there any exceptions happening inside a particular method. All such information we can easily understand. But in order to see the full power of tracing, you need to go to the Tempo. So, select the Tempo datasource. Here we are going to search with the help of a Trace Id that we have identified in any of the services like Gatewayserver, Accounts, Loans or Cards ms's where we know very well the request is going to pass. Now if you run the query and scroll down in the page, you will be able to see a very beautiful UI representation of your request navigating through all the respective services. For FetchCustmerDetails request which we are tracing, you can see that, initially it entered into your ms network at the Gatewayserver and the respective method(s) it invoked. From the gatewayserver, it went to the API which is fetchCustomerdetails of accounts ms. Inside accounts ms, you can see the methods through which it passed. So, all the history you are able to see here. After fetching the accounts related information, the accounts service forwarded the request to the loans ms. Once the details got fetched from the loans ms, it came back to the accounts ms, which again forwarded the request to the cards ms. Like this, you should be able to see what are all the methods that got invoked, how much time it took at each method. If the request is taking a lot of time, you can easily try to understand at which method or at which service a lot of time is being consumed by your request.
 *  So, you can see, since accounts ms is trying to invoke loans and cards ms's. On the RHS section, you can be able to see the consolidated timing information. Whatever you see at the top,entry point of your request is the consolidated time taken by all the services and all the methods invoked. So, this gives you a complete good information about the request and you are able to understand what are the performance bottlenecks. And for some reason if you want to understand, up to which point the request is travelled inside the network. This is a good information for you as a developer to gain understanding and proceed with your debugging. So, far, hope you have seen the power of tempo here. We also have similar kind of products with names like Zipkin and Jaeger. You can check them out. If you vist the Zipkin site i.e., https://zipkin.io/, you can see that it is one such product which we can leverage to see and visualize the distributed tracing information just like how we have seen and visualized with the help of Tempo. But the problem/drawback of Zipkin is, using it we can only see the tracing information, what if we want to see the logs information? Thats why we have Tempo which has a good integration with grafana and inside grafana we also have Loki, Prometheus. So, like this Grafana is a complete Package which we can easily leverage to implement observability and monitoring inside our ms's. For the RedHat product - jaeger, with which you can leverage for tracing. But like we have seen, they don't have good integration with grafana, nothing but with your logs information and that's why we are using the best product available as of now which is Tempo as it has a good integration with grafana. So, inside a single product - grafana -you can access and visualize all the 3 pillars of observability and monitoring i.e., tracing, logs and metrics.
 *
 *  Navigating to Tempo from Loki Logs
 *  -----------------------------------
 * - With the current setup, we are able to visualize the trace and span id's inside our Loki logs, but to understand the tracing information with the help of Tempo, we need to copy the trace id and then navigate to tempo and search for the same and post that only we are able to see the complete tracing information. But don't you think it is going to be super easy for the developers if they can directly open the tempo from the log statement itself inside Loki. We can achieve this easily and for the same, we need to navigate to the LHS nav and click the connections link >> Data sources >> Loki. Under this page, if you scroll down a bit, you will notice we have a section i.e., 'Derived fields'. With the help of this section, we can try to tell to the Loki that, whenever it identifies some pattern, it can try to derive a field and using that field, we can try to link with other components inside the grafana. So, I am going to give the 'field name' as TraceID, post that we need to give the regex that is going to match our logging pattern, for that we gave \[.+,(.+),.+\] We can also try to test this pattern by taking some example log message. Click on the 'show example log message' and in the input box labelled 'debug log message' paste a sample log message. You can do this by opening grafana in another tab and then go to the 'Explore' option and search for the logs of accounts ms. Make sure to have selected 'Loki'. Then, search for a log that has tracing information and copy that entire content of tracing information from that log i.e., [loans,46c0673f49d126825f08d9f0b591f4be,6df0b9561cb5eea4]. Now you can see based on the regex pattern, we have given, from my log message(s), it is going to extract the trace id and this value will be assigned to the field name which I have created which is 'TraceID'. So, whenever it extracts a field like, 'TraceID', I want to establish a link to
 *    one of the components inside the grafana. For the same, enable the toggle 'Internal Link' and post that we need to select what is the datasource of the other component, in our case 'Tempo'. Here we need to go to Tempo from Loki and that's why we need to select this 'Tempo'. Post that, using the 'Query' labelled input, we need to pass some query param which my Tempo is going to use to search for the tracing information based upon the TraceID. For the same, since it is going to be an internal link, we don't need to mention any domain. We just need to directly mention the value ${__value.raw} So, what we are try to say her is, whatever TraceID value that we have extracted, the same we want to pass as a query value to this Tempo datasource. So, save the changes by clicking on the button, 'Save & Test'. Now you can go to the LHS nav and click on the explore option and this time we are going to search for the logs again. Label filters 'Container' value Accounts ms' and then run the query. Her if you try to click on any of the logs where we have trace information, you will be able to see a new field with the name 'TraceID' along with the trace value. Here, there is a Link option for you with the name 'Tempo' which if you try to click on it. On the RHS section, all the tracing information related to that particular log you will be able to see/visualize. Don't you think this is a very good option. Haha of course evryone will like this option. And yea, this is the beauty of Grafana. Now its the best time to invest in some large monitor for this observability and monitoring thing!! To make our lives easy, instead of defining these derived fields explicitly from the UI application like we just did everytime, what we can do is that, we can try to define all these derived fields information inside the datasource.yml file that we have inside our docker-compose/observability/grafana folder.
 *    So, under the Loki, under the 'jsonData' parent attribute, I am going to mention some configurations related to derrived fields - check that out for detailed explanation. With that, you don't need to mention the derrived fields manually whenever your grafana is trying to create. Along with the Loki datasource, it is going to consider all the derrived fields that you will have defined just like we have. So far you should be super clear on all the changes that we have done.
 *
 * Conclusion on Observability and monitoring
 * -------------------------------------------
 * We have seen the demo of distributed tracing with the help of OpenTelemetry, Tempo and Grafana. Check the slide number 151 for the high level summery flow. It is also worth noting that all the docker-compose changes that we have done inside the prod profile. We have copied them under default and qa profile as well.
 * Hope you have seen the movie of 'Taken 3'. The hero gives a warning to the Vilian saying, " I don't know where you are/ where you are hiding, if you don't leave my data right now I am going to catch you, and I am going to Kill you " haha. The same now, we can tell to our defects also, "I don't know where you are hiding inside my microservice network, but I do have a particular set of skills related to observability and monitoring. Using those skills, I am going to find you and I will fix you in no time". Haha. Now, you have all the stepping stone knowledge to go and explore more and more in regard to observability and monitoring. Next we will be discussing microservices security.
 *  * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
