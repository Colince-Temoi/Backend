package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 19/10/2025
 *Create environment variables inside K8S cluster
 -------------------------------------------------
 * As a next step, we need to deploy all the remaining ms's into the K8S cluster. But before that, we need to create certain environment variables inside our K8S cluster which we can inject into the ms's deployments. Because, if you try to see the docker compose files that we have written, as of now all our containers are dependent on many environment variables like, what is the activated profile i.e., SPRING_PROFILES_ACTIVE: prod. What is the Spring config import url i.e., SPRING_CONFIG_IMPORT: configserver:http://cofigserver:8071/. What is the eurekaserver url i.e., EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://eurekaserver:8070/eureka/, ...etc. Since all our ms's need these environment variables to get started, we need to look for an option on how to create the same kind of environment variables inside the K8'S cluster as well. For the same we are going to create an object of Configmap inside K8S. In the official docs i.e., "https://kubernetes.io/docs/concepts/configuration/configmap/". It can be noted that, "A ConfigMap is an API object used to store non-confidential data in key-value pairs". Pods and your containers they can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume. So, the same ConfigMap object we need to consider to define all our environment variables inside the K8S cluster. There is also a syntax provided on how we can create this. We are going to follow the same and define a K8S manifest file to create the Configmap that is specific to our ms's. In the LHS nav in the official documentation under the Configurations, you will notice, we also have 'Secrets' object which we can leverage whenever we want to store some confidential data. Previously, we created a Secret object whenever we were trying to deploy the K8S Admin dashboard/UI. If yoy scroll down in the doc where they have given an example manifest definition, you will see the 'kind' has to be 'Secret'. The same thing we did previously - refer to the previous commit docstring.
 * Right now, our focus is on Configmaps to create/define environment variables. For the same, I am going to create a new file inside my K8S folder i.e., touch configmaps.yaml. Inside this file I have pasted the below discussed configurations:
 apiVersion: v1 # The api version for our ConfigMap object we need to mention the value as v1
 kind: ConfigMap # The kind has to be ConfigMap
 metadata: # Under the metadata we are defining the name that we want to give for this ConfigMap configurations
 	name: get_tt_right-configmap # The name that we are trying to give here is 'get_tt_right-configmap'
 data: # After the metada, we need to mention an element which is 'data' under which we can provide any number of environment properties with k-v.
 	SPRING_PROFILES_ACTIVE: "prod" # The profile that I want to consider while deploying my ms's
	SPRING_CONFIG_IMPORT: "configserver:http://configserver:8071/" # What is the configserver url.
 	EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: "http://eurekaserver:8070/eureka/" # What is the eurekaserver url.
 	CONFIGSERVER_APPLICATION_NAME: "configserver" # What is the application name of configserver ms.
 	EUREKA_APPLICATION_NAME: "eurekaserver" # What is the application name of eurekaserver ms.
 	ACCOUNTS_APPLICATION_NAME: "accounts" # What is the application name of accounts ms.
 	LOANS_APPLICATION_NAME: "loans" # What is the application name of loans ms.
 	CARDS_APPLICATION_NAME: "cards" # What is the application name of cards ms.
 	GATEWAY_APPLICATION_NAME: "gatewayserver"  # What is the application name of gatewayserver ms.
 	KC_BOOTSTRAP_ADMIN_USERNAME: "admin" # What is the KeyCloak Admin username
 	KC_BOOTSTRAP_ADMIN_PASSWORD: "admin" # What is the admin password of KeyCloak.
 	SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI: "http://keycloak:7080/realms/master/protocol/openid-connect/certs" # What is the KeyCloak url where my resource server has to fetch the certificate

 * So, all the above environment variables I have mentioned. Coming to the values, the hostnames in urls have to be your respective service name i.e.,'configserver' in http://configserver:8071/. I have created/deployed my configserver with the service name as 'configserver'. Similarly, for the eurekaserver, I will create/deploy my eurekaserver with the service name as 'eurekaserver' and that's why in the value url you can see 'http://eurekaserver:8070/eureka/'. Coming to the KeyClaok url details/value which we need to mention under the resource server, I am going to start my KC service with the name 'keycloak'and that's why in the url value, I am going to mention the hostname as 'keycloak' and it is going to get exposed at the port 7080. That's why I am trying to mention the correct things here i.e., what is the hostname and what is the port number.
 * Here,I am not defining any properties related to Kafka or RabbitMQ or OpenTelemetry. Reason: Once we discuss the Helm charts, then only we are going to see how to easily set up the Kafka, Grafana related components inside K8S because writing all these K8S manifest files for industry provided components like Kafka, Grafana is unnecessary and is overcomplicated. That';s why when we try to learn about Helm charts, using the same, I am going to show you how easy it is to set up these components like Kafka, Grafana components. But as of now we will only deploy the custom ms's that we have developed along with the KeyCloak. KeyCloak is mandatory and we cannot skip it because as of now, our GatewayServer is secured and without KC server we cannot access any of the APIs to create the data inside our ms's. So, with this we have successfully created the configMap K8S manifest file. As a next step, we will try to use this file and try to feed the same to my K8S's cluster. For the same, I am going to run the command 'kubectl apply -f configmaps.yaml'. As soon as I execute this, I get an output saying that, 'configmap/getttright-configmap created'. Inside the K8S dashboard, I can validate if it is created or not.
 * In the K8S dashboard, make sure the namespace selected is 'default' then on the LHS nav, there is a section 'Config and Storage', under this click on the nav link 'Config Maps'. You should be able to see an entry of the config map which we just created named 'getttright-configmap'. If you click on it, you should be able to see all the environment variables that we have provided to the K8S cluster. As a next step, by using these environment variables, we can try to deploy all the remaining ms's. Before that, lets try to understand and visualize from the Admin dashboard what the difference between Config Maps and Secrets are. If you open the 'getttright-configmap' config maps that we just created you will notice that we are able to see the data directly whereas secrets will be base 64 encoded. As of now, on the LHS nav if you click on 'Secrets' we have no secret(s) mentioned/defined. But if, if you switch to the namespace which is 'kubernetes-dashboard' from the top LHS dropdown,you should be able to see a secret entry that we created earlier on named 'admin-user'. If you click on it, under the data section, you will not be able to see the data, by default, it is hidden from the dashboard. But if you want to see that, there is an eye symbol, on clicking it, you can be able to see the base 64 representation of that data - For example, I clicked on the eye symbol of the data labeled 'Token'. Okay this is not a super secure way to secure your secrets, that's why in K8S there is a,so a Joke that 'Secrets in K8S are not actually Secrets'. There is always better approaches to store the secrets inside Cloud environments. Your Platform/DevOps team members should be able to set up them inside your m's network.
 * With this, you should be crisp clear on how to create a ConfigMap inside K8S cluster.
 *
 * Preparing K8S manifest files for the remaining ms's
 * ----------------------------------------------------
 * Now, we will try to deploy all the remaining m's into the K8S cluster. For the same first we need to prepare the K8S manifest files for the remaining m's. Since we are already familiar with this process, behind the scenes I have created all the required Manifest files as shown below;
 * I have the manifest files named with a numeric prefix value i.e., 1, 2, 3, ...etc. With the help of these prefix values, I am giving an order in which you need to apply these yaml files inside your K8S cluster. So, in future, whenever you want to deploy all our ms's into a brand-new cluster, first we need to create the KC service, Post that we need to create the configmap, followed by configserver, eurekaserver, accounts, loans, cards and lastly gatewayserver. This is the orede that you need to follow in order to make our life easy. Below, we are going to discuss what we have defined/configured in all these files except configmaps and configserver yaml files as we have already in details discussed their respective contents.
 * Below, nothing new as for every service/ms that we want to deploy we have to create a 'Deployment' object followed by what is the 'Service' object.
 1_keycloak.yml
 ----------------
 apiVersion: apps/v1
 kind: Deployment
 metadata:
 	name: keycloak
 	labels:
 		app: keycloak # Giving a label app name as 'keycloack'
 spec:
 	replicas: 1 # After this metadata name labels app definitions, under the specifications I'm mentioning the replicas as one.
 	selector:
 		matchLabels:
 			app: keycloak # selector.matchLabels.app as 'keycloak'
 	template:
 		metadata:
 			labels:
 				app: keycloak
 		spec:
 			containers:
              - name: keycloak # Under this specifications you can see the container name is going to be 'keycloak'
       			image: quay.io/keycloak/keycloak:26.4.0 # Image
 				args: ["start-dev"] # Arguments that I want to consider while starting my container. Since we want to start our K8S cluster in a dev mode we need to provide this 'args' key with a value ["start-dev"]
 				env: # This is a new piece of configuration that you may find here compared to what we have discussed initially with respect to configserver. Here, we are trying to pass the environment variables to this deployment instructions. So to pass the environment variables, we need to mention an element i.e., 'env' under which we can define any number of environment variables.
 				- name: KC_BOOTSTRAP_ADMIN_USERNAME # With the help of this hyphen, after hyphen, we need to mention what is the name of the environment variable that you want to inject to your container. In this case the name of the property is 'KEYCLOAK_ADMIN'
                  valueFrom: # To the above defined environment variable, I want to provide the value that I stored inside the configmaps. This valueFrom property is like from where I want to fetch the value
                  	configMapKeyRef: # Under the valueFrom I have to mention 'configMapKeyRef'
                    	name: getttright-configmap # What is the configmap name where we stored this value.
 						key: KC_BOOTSTRAP_ADMIN_USERNAME # At last inside our 'getttright-configmap' configmap, we may have many environment variables. So, which environment variable it has to consider? That's why we need to give the key value i.e., 'KC_BOOTSTRAP_ADMIN_USERNAME'. So, it will go and look for this key value  inside the 'getttright-configmap' configmap. Against to that respective key, there is a value which is 'admin'. The same will be injected as a value to the property defined i.e., 'KC_BOOTSTRAP_ADMIN_USERNAME' that is just under 'env' i.e., - name: KC_BOOTSTRAP_ADMIN_USERNAME. In the getttright-configmap, under the data we have a k-v entry i.e.,  KC_BOOTSTRAP_ADMIN_USERNAME: "admin". If we want, we can mention the key as 'username' instead of 'KC_BOOTSTRAP_ADMIN_USERNAME'. The same key/name i.e., 'username' we then have to use inside the key property that is defined under 'configMapKeyRef' BUT PLEASE DON'T mention this 'username' here under the environment name i.e., - name: username Reason: Whatever you mention under the environment name, that will be fed as an input to your container or your Spring Boot application, so if you give some random property name(s) under the 'env.name' definition, it is not going to work. That's why make sure you are ALWAYS giving the correct environment name under the env.name and not just random names. But you have the flexibility to maintain any key name inside your configMap i.e., getttright-configmap and under this configMapKeyRef.key property in your manifest file. But to be consistent and to make our lives easy, I am maintaining both the configMapKeyRef.key property value same as the env.name property value.
 				- name: KC_BOOTSTRAP_ADMIN_PASSWORD # Same drill, we have injected one more environment variable with the name 'KC_BOOTSTRAP_ADMIN_PASSWORD'
                  valueFrom:
 					configMapKeyRef:
 						name: getttright-configmap
						key: KC_BOOTSTRAP_ADMIN_PASSWORD
 				ports: # At las I have also provided what are the port configurations
 					- name: http
 					  containerPort: 8080 # It has to start at the port 8080
 ---
 apiVersion: v1
 kind: Service # After the 'Deployment' instructions, we have the 'Service' related instructions as well.
 metadata:
 	name: keycloak
 	labels:
 		app: keycloak # The same label app name we are mentioning here under the Service. This is how the K8S is going to connect this 'Service' object with the 'Deployment' object of keycloak service/ms.
 spec:
 	selector:
 		app: keycloak
 	type: LoadBalancer # Here also we are trying to expose our KC service as a LoadBalancer which means anyone can access my KC server at the port 7080
 	ports:
 		- name: http
		  port: 7080
          targetPort: 8080 # But please make sure you are mentioning the targetPort value same as your containerPort value defined inside the 'Deployment' instructions. Under containerPort and targetPort, we need to always mention what is the port where your container is going to start internally inside your K8S cluster.

 *
 * 2_configmaps.yaml
 * -------------------
 * Already discussed in details
 *
 * 3_configserver.yaml
 * --------------------
 * Already discussed in details
 *
 * 4_eurekaserver.yml
 * ----------------------
 * Here also nothing new. Same drill as discussed for other manifest files. We have provided the 'Deployment' instructions as well as the 'Service' instructions. Under the 'Deployment' instructions, you can see I have provided the image name as colince819/eurekaserver:v4. My instructor was using S12 related images and that's why for him the same tag name he has mentioned inside the Deployment instructions of configserver as well i.e., s12. For me, I mentioned the tag name V4 for configserver related deployment instructions and for other deployment related instructions they may be different.
 * For eurekaserver we are feeding 2 environment variables i.e., What is the application name of eurekaserver i.e., SPRING_APPLICATION_NAME. And btw, if you are keen, in my configmaps, the eurekaserver application name is present under the key 'EUREKA_APPLICATION_NAME'. The respective value will be injected into my eurekaserver container as the application name. Do you know why I have not used the same key name in my configmaps as the environment variable name i.e., 'SPRING_APPLICATION_NAME'. haha. It's because we need to define this same application name environment variable for all the remaining ms's, with that reason I have mentioned different relevant key names i.e.,EUREKA_APPLICATION_NAME, ACCOUNTS_APPLICATION_NAME, etc inside the configmaps and the same I am going to refer in the respective K8S manifest files for different applications/ms's when defining the application name environment variable.
 * Here, I am also injecting what is the configserver endpoint url into an environment variable i.e., SPRING_CONFIG_IMPORT
 * After the deployment instructions, we have Service instructions also which you are already familiar with, nothing new or fancy.
 *
 * 5_accounts.yml
 * ---------------
 * Nothing new or fancy. Everything is familiar. Just note the image tag. My instructor is using s12 related containers for all his Deployments. For me I am trying to leverage my containers that do not have event streaming stuff. That's why you may see different inconsistent tag name for different images in different Deployment instructions. For Account ms's I am using tag name i.e., V9
 * Compared to the eurekaserver manifest file, for the accounts ms manifest file, there is a new environment variable that I am injecting which is what is the eureka server client service url default zone. Reason: My accounts ms has to register itself with the eurekaserver and that's why we are trying to provide this environment variable. And, after Deployment related instructions, we also have Service related instructions.
 *
 * 6_loans.yml/7_cards.yml/8_gateway.yml
 * --------------------------------------
 * Same kind of setup as in accounts mangiest file. For gatewayserver the tag name I am using is V5. Loans and cards is V9 same as accounts ms.
 * For gatewayserver we are injecting the extra environment variable i.e., what is the keycloak server url where my resource server has to fetch the certificate i.e., SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_JWK-SET-URI
 *
 * With this, it is safe to assume that you are clear with all the K8S manifest files that we have created discussed so far for the ms's that were remaining. Next, we will be applying them into K8S cluster so that all our containers will get deployed into the cluster and post that we can also validate the same.
 *
 * Deploying the remaining ms's into the K8S cluster.
 * ----------------------------------------------------
 * Deploy them in the order prefixed in by the file name i.e., kubectl apply -f 1_keycloak.yml - This will set up the KeyCloak service. The next file which I need to execute here is one related to configMaps i.e., kubectl apply -f 2_configmaps.yaml - You will get the output i.e., configmap/getttright-configmap unchanged. Reason: We had already earlier on executed this. You may have a question like, earlier on we had already creaed this/executed this why are we re-executing it?? Reason: Here we are seeing a demo, so, when trying to provide the same instructions to the K8S cluster it is smart enough to detect that there are mo new changes and it is going to give an output like, 'configmap/getttright-configmap unchanged'. It's like saying that, 'Huh! nothing changed, and I am not doing anything based upon your instructions'. This is one of the beauty of K8S.
 * Next, 'kubectl apply -f 3_configserver.yaml'. Do you know of any change in the configserver compared to the previous deployment that we have done? My instructor changed the image name from s14 to s12. If you execute this command you will get two outputs like, 'deployment.apps/configserver-deployment configured' which means 'The new deployment is configured - of course based upon the new image name. hahah' And another output like, 'service/configserver unchanged' which means like - The Service configurations related to configserver are unchanged because we didn't make any changes in the 'Service' object related to configserver in the file 3_configserver.yaml. Now, behind the scenes, my configserver might have been set up, and we can confirm the same by going to the K8S dashboard. You will see that as of now we have 2 deployments running. So, if you try to navigate to the nav link i.e., 'Pods' on the LHS nav, and the open configserver related pod, we can see the logs of my configserver by clicking on the icon '3 and half' lines present on the top, RHS nav. Here you should be able to see some log statements saying that, 'Started ConfigserverApplication in 40.56 seconds' which means that our confiserver got started successfully. Note: Always make sure that your deployment is completed successfully before you try to apply the next set of instructions in the K8S cluster. So, once we make sure that configserver is successfully set up, as a next step, I am going to apply the K8S manifest file related to eurekaserver i.e., kubectl apply -f 4_eurekaserver.yml and this is going to deploy my eurekaserver into the K8S cluster.
 * Same drill, navigate to the pod related to eurekaserver and post that check the logs after a few seconds. You should see a log statement saying that my eurekaserver got successfully started i.e., 'Started EurekaserverApplication in 40.57 seconds'. Next, deploy the instructions related to accounts ms i.e., kubectl apply -f 5_accounts.yml, then followed by kubectl apply -f 6_loans.yml and then followed by kubectl apply -f 7_cards.yml then followed by kubectl apply -f 8_gateway.yml. So, one you have applied this loans, accounts and cards related K8S manifest files, please wait for these containers to get started completely - post that only you can start the gateway server. Here, you may have a question like, inside docker-compose we are able to achieve this depends-on automatically with the help of health checks but unfortunately K8S does not have any direct/easy approaches to define the interdependency between your K8S services - that's why whenever we are trying to start a ms, we need to make sure that all the dependent service(s) of that particular ms are/is successfully started. Otherwise, nothing wrong is going to happen, your container(s) will keep trying to restart multiple times and eventually your deployment(s)will be successful. So, just to avoid those/such inconviniences you can wait for your dependee ms's to start completely and successfully. You can confirm if our custom ms's are started completely and successfully or not by going to the eureka dashboard  i.e., localhost:8070- here you can see that all the 3 custom ms's i.e., accounts, loans and cards are registered with the eurekaserver. As a last step, I can try to apply the last file which is related to gatewayserver to deploy our gatewayserver also into the K8S cluster. In real production environments, your production team members or DevOps team members will use some advanced approaches to deploy your ms's in a predefined order but as of now as a developer we don't need to understand all of them because in our sessions we are not focusing on K8S concepts - our main focus is on how to build ms's.
 * Now, while deploying my custom ms's, I encountered a challange such that they were not starting as expected as I am not using  H2 DB for them. So I set up docker DB containers for them and then defined DATASOURCE_URL environment variable for them such that they could be able to startup as expeceted but still they din't. I came to find out the reason is - "In Kubernetes, Pods cannot access containers started with docker run on your local system. DNS inside the cluster only resolves Kubernetes Services and Pods." and that;s why I was seeing in the respective ms Pod logs exception messages like " java.net.UnknownHostException: accountsdb: Temporary failure in name resolution". GPT suggested two alternative solutions i.e., Run MySQL inside the same Kubernetes cluster (Din't go this route" and "Point your JDBC URL to the host’s IP or host.docker.internal, not accountsdb." as I still insisted on using the Docker DB containers. So, In configmaps changed from 'ACCOUNTS_SPRING_DATASOURCE_URL: "jdbc:mysql://accountsdb:3307/accountsdb"' to 'ACCOUNTS_SPRING_DATASOURCE_URL: "jdbc:mysql://host.docker.internal:3307/accountsdb' I then reapplied my ConfigMap and redeployed the microservice. I then was given the belwo note: ⚠️ Note: host.docker.internal only works on Docker Desktop / Mac / Windows setups. On Linux, you’ll have to use your host machine’s IP (e.g., 172.17.0.1). I also encountered another challange where on stopping and starting the docker desktop,K8S kong service fails to start hence I am on running the command kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 I end up getting the response i.e., E1028 07:58:13.818903 8492 portforward.go:413] an error occurred forwarding 8443 -> 8443: error forwarding port 8443 to pod e40cfb7c2185f30a5ad93868321e2b38c3b517079cab71666a0ed0e0cc8b1a44, uid : exit status 1: 2025/10/28 04:58:13 socat[15108] E connect(7, AF=2 127.0.0.1:8443, 16): Connection refused error: lost connection to pod. I came to find out on running the kubectl command i.e., kubectl -n kubernetes-dashboard get pods the Kong service was in a CrashLoopBackOff status. To fix this, I tried inspecting the reason for the crash by running the two commands i.e., kubectl -n kubernetes-dashboard logs kubernetes-dashboard-kong-78fd98d579-dff6s and kubectl -n kubernetes-dashboard describe pod kubernetes-dashboard-kong-78fd98d579-dff6s. I could see something like " [emerg] bind() to unix:/kong_prefix/sockets/we failed (98: Address already in use)
 * nginx: [emerg] still could not bind()"  which means, "Kong’s Nginx worker is trying to bind to a Unix socket (/kong_prefix/sockets/we. But that socket file already exists (likely left over from a previous shutdown). Even though an init container (clear-stale-pid) tries to clean /kong_prefix/pids, it doesn’t remove the socket file under /kong_prefix/sockets. So Kong keeps trying to start, fails immediately on bind(), crashes, restarts, and loops — giving you the CrashLoopBackOff status." I cam eto realize that this is a known Docker Desktop + Kubernetes issue, where leftover files persist in the ephemeral volume when Docker Desktop restarts uncleanly. As a solution I opted to Delete the crashing Pod by running the command kubectl -n kubernetes-dashboard delete pod kubernetes-dashboard-kong-78fd98d579-dff6s, This, removes the old volume mount and then forces Kubernetes to spawn a fresh pod with a clean /kong_prefix/ directory. You can then run the command kubectl -n kubernetes-dashboard get pods -w. Once you see kubernetes-dashboard-kong-xxxxx   1/1   Running Ctrl + C  then everything will be good and now you can perform the port forwarding happilly and access your K8S Admin dashboard.
 * Now if you refresh the eurakaserver page/dashboard i.e., localhost:8070, you can see that my gatewayserver is also registered successfully. As a next step, you can go to postman and try to test the  scenrios. Before that we need to make sure that inside the KC we have set up the client details with which we can use to get the token(s) and the same we can to the gatewayserver as part of our request that we will be testing. For the same access the port i.e., 7080 where our KC service is exposed. Sign in to the Administration console with creds i.e., 'admin' and 'admin'. Post that, you can navigate to the LHS nav and click on the navlink i.e.,'Clients' >> Create Client button >> Enter the client id details >> Next button >> Enable Client authentication toggel >> Uncheck Standard Flow and Direct Access grants >> Check/enable Service Account Roles >> Next button >> Save Button.  At last navigate to the credentials tab and copy the credentials to this Client that we have just created and mention them inside my postman so that we can try to get an access token. Once we have created these client details we should also create the roles. To do that, on the LHS nav "Realm Roles" >> "Create Role" i.e., 'ACCOUNTS, CARDS AND LOANS'. As a next step, on the LHS nav, "Clients" >> Click on the Client that we have just created i.e., "get_tt_right-bank-callcenter-cc" and then navigate to the "Service accounts roles' tab and the click on assign roles dropdown >> Realm Roles then try to assign the roles that we have just created to our client which is get_tt_right-bank-callcenter-cc. With this, now we set up everything on the KC side. Now, try getting a token for the request i.e., gateway_security/Accounts_POST_ClientCredentials. Post that you can fire your request and you should get a response saying that Accounts details successfully created. Very similarly, I will try creating the cards as well as the loans details by firing the respective request. As a last step, you ca try fetching all the records by invoking the request gatewayserver_security/Accounts_GET_PermitAll. Since its is a GET operation, we don't need to pass any authentication information. Happily if you pass the correct mobile number that you considered while creating the account you should get a successful response. Also make sure while creating the loans and cards details, use the same phone number you used to create the accounts details. Hurreeey!! this conforms that all our deployments are working perfectly inside K8S cluster. This is nothing new, fancy or exciting compared to what we have done with docker containers and docker compose. Now, the magic of K8S we will see in the coming sessions, but before that it was a requirement that we are aware about all these basics we have been discussing and we set up all our ms's inside the K8S cluster.
 * In the slides I have provided all the instructions in summary on how to create a K8S manifest file for various scenarios i.e, Creating a 'ConfigMap', Deploying a container with the help of 'Deployment' object. Inside the slides, I have explained the purpose of each and every element that we have defined inside the K8S manifest files. After the 'Deployment' instruction, I have also mentioned the instructions about how to define a 'Service' or how to expose our container with the help of 'Service' object inside K8S. I have also explained all the elements of 'Service' related configurations. At last, there is also a slide explaining about how the link between your 'Deployment' and 'Service' objects are going to be established by your K8S. Like you can see, the binding between your Accounts 'Deployment' and Accounts 'Service' is happening with the help of label.app:accounts that we have created/ defined under the 'Deployment' related instructions that is under the 'metadata' related configurations. The same we have mentioned under the 'Service' related instructions that is under the specifications.selector configurations. In the screenshot attached on the slide for the 'Service' related configurations you can see that the 'type' definition is assigned a value i.e., 'ClusterIP' which means that I don't want to expose my ms's to the outside of the cluster. In the coming sessions, we are going to have a detailed explanation 'LoadBalancer', ClusterIP' and when to go with a specific Service 'type'. With all we have discussed you should have crisp clarity and confidence to write/define your own K8S manifest files based upon your requirements. If not, haha, don't worry as you don't need to be a master of writing K8S manifest files - as long as you are able to understand the manifest files provided by your DevOpd team then you should be good and with that knowledge you should be able to easily debug any kind of production issues inside your ms network. More magic about K8S we are going to explore in the coming sessions.
 *
 * Automatic self-healing inside K8S cluster.
 * -------------------------------------------
 * As of now, we deployed all our ms's into K8S cluster. As a next step, inside this session, we are going to visualize the self-healing capability of K8'S. Which means, whenever K8'S sees that a specific container is not working properly or is completely down, then immediately K8S will try to self-heal the container by re-creating a new container in place of the defective container. That demo is what we are going to visualize inside this session. For the same inside the terminal, first, we will execute a command which is kubectl get replicaset. We all know that inside the 'Deployment' instructions we have specified/defined how many replicas we want to maintain for our respective containers. You can quickly verify this in any of the k8s manifest files that we have created against the key 'replicas'. The same replica information for respective container deployments, we can see by running the command 'kubectl get replicaset'. In the output, against each of the record/replicset there is a 'DESIRED' state column and what is the 'CURRENT' state column and how many containers are 'READY' column, to serve to the client applications. So, based upon this replicaset information/output, always my k8s will try to maintain the 'DESIRED' number of containers. If you can go and execute the command which is 'kubectl get pods', you can see from the output that there is a single pod for each of my deployments. Assuming as per my requirement I need the 'DESIRED' replica information for accounts to be 2, this I can try to increase and we will visualize what is going to happen. To achieve this we just have to go to the accounts manifest file  and as a value to spec.relicas definition, mention/alter the value to 2, save the file and apply it to your k8s cluster using the command kubectl apply -f 5_accounts.yml. As soon as I run this command, k8s is smart enough to detect the changes right now I have made in the 5_accounts.yml file. Now If I run the command i.e., kubectl get replicaset. This time for accounts deployment, you should be able to see the 'DESIRED' state as 2 and 'CURRENT' as 2 and 'READY' also having the value 2 for the accounts related deployment. We can also confirm the same by running the command which is kubectl get pods and in the output you should see 2 records representing your account ms. If you check with the 'AGE' column, you will see one is created earlier than the other. The Pod record related to the accounts ms with the least AGE value means it's the latest pod created by k8s, how intelligetnt!! from k8s, to much our 'DESIRED' state for accounts ms. Now, the story is going smoothly!! haha, now as a next step, to visualize the self-healing capability of k8s, I am going to delete one of the pods of related to accounts ms manually which will imply that my 'DESIRED' state will not be matching with the 'CURRENT' state.
 * So, what will k8s do behind the scenes??  It will try to create a n ew accounts ms instance. So, to delete a pod manually, use the command , 'kubectl delete pod <pod_name>'.So, I will delete the new pod that got created by k8s related to accounts ms. Now, if you run the command 'kubectl get replicaset' to see what is happening, you will see that the 'DESIRED' nad 'CURRENT' value related to accounts ms deployment being 2 - that must be fast from K8S haha! Behidn the scenes it took action quickly to create a new pod related to accounts ms in order to match the 'DESIRED' and the 'CURRENT' states. This tell you that K8S laways putting in the work/effort to match my 'DESIRED' and 'CURRENT' states. This is one of the beauties of K8S which is 'Self-healing capability'. This is something we cannot achieve with docker and docker-compose and that's why we should always look for container orchestration products like K8S. These products will always keep an eye on the running containers, such that is any of the deployed containers has some health issues, they are always going to take an action to make your 'DESIRED' and 'CURRENT' states match with each other. We can also see what happened behind the scenes when we deleted the accounts related pod by running the command 'kubectl get event --sort-by=.metadata.creationTimestamp' - This command gets all the events that happened behind the scenes inside my K8S cluster by sorting all of them based on the createTimestamp. In the outputyou will see something like below:
 $ kubectl get events --sort-by=.metadata.creationTimestamp
 LAST SEEN   TYPE     REASON             OBJECT                                      MESSAGE
 22s         Normal   Killing            pod/accounts-deployment-69f89c5779-94zn5    Stopping container accounts
 22s         Normal   Scheduled          pod/accounts-deployment-69f89c5779-htv52    Successfully assigned default/accounts-deployment-69f89c5779-htv52 to docker-desktop
 22s         Normal   SuccessfulCreate   replicaset/accounts-deployment-69f89c5779   Created pod: accounts-deployment-69f89c5779-htv52
 15s         Normal   Pulled             pod/accounts-deployment-69f89c5779-htv52    Container image "colince819/accounts:v9" already present on machine
 15s         Normal   Created            pod/accounts-deployment-69f89c5779-htv52    Created container accounts
 14s         Normal   Started            pod/accounts-deployment-69f89c5779-htv52    Started container accounts
 *
 * In the output, you can see the record event where we tried to kill our account related pod. That's why under the 'REASON' column, you can be able to see a value 'Killing' there. As soon as I killed my pod manually behind the scenes immediately my K8S tried to create a new accounts related pod and that's why you are able to see a record in the events with 'REASON' as 'SuccessfulCreate'. And also the 'MESSAGE' column for the record events are very straight forward and easy to follow through describing what is happening for each event. This confirms that K8S is capable of self-healing. Regardless of how many containers and how many containers and how many containers and m's you have inside your K8S cluster, your cluster is going to take care of all your containers and make sure that they are always running in a good healthy status.
 *
 * Automatic Rollout and Rollback inside K8S cluster
 * ----------------------------------------------------
 * In this session, we will be visualizing one more feature of k8s which is, how to deploy your new changes into k8s cluster without any downtime and at the same time if you face any issues with your new changes you can easily rollback without any downtime or manual effort. Now I am going to set the number of replicas for my accounts ms to 1, previously we had set that to 2 and now I want to rollback to 1 so that I will not face any memory issues inside my local system. So, whenever we want to pdate the replicas information for a particular ms or for a particular deployment instructions,there are 2 approaches one of which we already saw in the previous lecture which is directly altering the replicas value inside your respective ms manifest file. The alternative approach is to run a command which is,kubectl scale deployment <deployment_name_of_your_ms> --replicas=1. With this I am trying to scale my accounts related deployment to a replica value of 1.'kubectl scale deployment accounts-deployment --replicas=1' As soon as I execute this command I should be able to see one pod related to accounts ms deployment by subsequently running the kubectl get pods command. You can also confirm the same by running the command kubectl get replicaset, you should be able to visualize that the 'DESIRED' state is 1 and the 'CURRENT' state is 1. Like this of the 2 alternative approaches that we have discussed you can follow either to scale up or down your specific deployments. But make sure you are ALWAYS updating the respective manifest file because in future whenever you try to apply that file then the replicas will become 2. So make sure if any change/alteration you would have done in your existing manifest file, you resort to make that change using a kubectl command, make sure that change is also updated in the manifest file so that everything is properly streamlined - this is what is RECOMMENDED!. Now we want to visualize a demo on how to deploy a new change into the K8S cluster without any downtime. For the same I am going to show you this using the gatewayserver ms. As of now if I try to run the command kubectl describe pod <name_of_pod>. In our case we want to describe the gatewayserver related pod. If you run this command you can see a lot of information, among them, you can see that my pod of gatewayserver is using the docker image which is colince819/gatewayserver:v5 which mean right now our container is using the tag of v5. Think like I want o deploy my gatewayserver without any security.
 * As of now my gatewayserver always expects an access token whenever we try to invoke any POST/UPDATE/DELETE APIs. For the same what I am going to do is, I am going to deploy a new image. My istructor was using the tag name s12 which to him communicates that inside the section 12 we have the changes related to OAuth2 and in section 11 we don't have any security related changes. So he simply updated the tag name from s12 to s11. For my case, I noticed that the changes/version where security is yet to be incorporated is v4. To actualize this updating, there are 2 approaches one which is by simple updating the image name inside your k8s manifest file or by issuing a kubectl command which is kubectl set image for the deployment with the name <deployment_name>, inside this <deployment_name>, I have a container with the name i.e., gatewaserver. If you see inside the gateway.yaml under containers you can see we have given a container with the name 'gatewayserver'. This same name we have to use inside the kubectl command. So for this gatewayserver container I want to set a new image which is colince819/gatewayserver:v4 and post that I am going to give a flag which is '--record'. With this flag, I am telling to my k8s cluster to record the reason on why we are deploying a new image. So the full flegded command is 'kubectl set image deployment gatewayserver-deployment gatewayserver=colince819/gatewayserver:v44 --record'. Before running this command, as you can see haha, I have intentionally given an invalid image i.e., tag name v44 - which is not vaialbel inside my dockerhub or docker(local system) - instead of v4. In the output happily you will get an output saying that deployment image updated but if you run the command kubectl get pod you will notice that k8s tried to create a new pod of gatewayserver but it has a new 'STATUS' saying that 'ErrImagePull' which means that it is not able to pull the image. The beauty that I want you to visualize here is my k8s cluster did not disturb the other running pod as can be seen from the output below:
 ctemoi@CTEMOI2-PC MINGW64 ~/Desktop/K8s
 $ kubectl set image deployment gatewayserver-deployment gatewayserver=colince819/gatewayserver:v44 --record
 Flag --record has been deprecated, --record will be removed in the future
 deployment.apps/gatewayserver-deployment image updated

 ctemoi@CTEMOI2-PC MINGW64 ~/Desktop/K8s
 $ kubectl get pods
 NAME                                        READY   STATUS         RESTARTS       AGE
 accounts-deployment-69f89c5779-s7ss6        1/1     Running        10 (59m ago)   7d
 cards-deployment-7c54d9dc69-4r2k8           1/1     Running        10 (59m ago)   7d
 configserver-deployment-55b5b5979c-4lwgd    1/1     Running        4 (60m ago)    7d
 eurekaserver-deployment-86764cc979-nwkjr    1/1     Running        12 (59m ago)   7d
 gatewayserver-deployment-5bd8ccf7c8-9pcrf   1/1     Running        11 (59m ago)   7d
 gatewayserver-deployment-5d876467-w64gr     0/1     ErrImagePull   0              2m6s
 keycloak-98d6b6f56-cdz96                    1/1     Running        4 (60m ago)    7d
 loans-deployment-674db986cc-6v95m           1/1     Running        12 (59m ago)   7d
 *
 * Reason: First my k8s wants to validate whether it is able to set/spin up the gatewayserver with the new image that we have provided. Post validating the same and once it comes to a conclusion that my gatewayserver pod is successfully set up, then only that's when it is going to take the decision to terminate the other running pods of gatewayserver. If you have 3 other different pods of gatewayserver initially running, it is not going to kill all of them at once, instead it is going to incrementally create multiple pods of your gatewayserver  with the latest image and post that only it is going to terminate and delete the previous old pods. Hope you are seeing and visulaizing the veauty here haha!. Now as a next step, this time I am going to give the correct image name in the command i.e.,  v4. As soon as I run this command I can go and exvute the command kubectl get pods and you should be able to see that there is anew container that is getting created as can be seen from theoutput below:
 $ kubectl set image deployment gatewayserver-deployment gatewayserver=colince819/gatewayserver:v4 --record
 Flag --record has been deprecated, --record will be removed in the future
 deployment.apps/gatewayserver-deployment image updated

 ctemoi@CTEMOI2-PC MINGW64 ~/Desktop/K8s
 $ kubectl get pods
 NAME                                        READY   STATUS              RESTARTS       AGE
 accounts-deployment-69f89c5779-s7ss6        1/1     Running             10 (71m ago)   7d1h
 cards-deployment-7c54d9dc69-4r2k8           1/1     Running             10 (71m ago)   7d1h
 configserver-deployment-55b5b5979c-4lwgd    1/1     Running             4 (72m ago)    7d1h
 eurekaserver-deployment-86764cc979-nwkjr    1/1     Running             12 (71m ago)   7d1h
 gatewayserver-deployment-5bd8ccf7c8-9pcrf   1/1     Running             11 (71m ago)   7d1h
 gatewayserver-deployment-5d876467-w64gr     0/1     Terminating         0              14m
 gatewayserver-deployment-68f94d98ff-ztld8   0/1     ContainerCreating   0              2s
 keycloak-98d6b6f56-cdz96                    1/1     Running             4 (72m ago)    7d1h
 loans-deployment-674db986cc-6v95m           1/1     Running             12 (71m ago)   7d1h
 *
 * If you keep trying to run the command kubectl get pods, at some point of time, the pod the is trying to get created will have a status of 'Running' with an 'AGE' value of x seconds, where as the old pod which has an age of x time. it is getting terminated as can be seen from the output below.
 $ kubectl get pods
 NAME                                        READY   STATUS        RESTARTS       AGE
 accounts-deployment-69f89c5779-s7ss6        1/1     Running       10 (76m ago)   7d1h
 cards-deployment-7c54d9dc69-4r2k8           1/1     Running       10 (76m ago)   7d1h
 configserver-deployment-55b5b5979c-4lwgd    1/1     Running       4 (77m ago)    7d1h
 eurekaserver-deployment-86764cc979-nwkjr    1/1     Running       12 (76m ago)   7d1h
 gatewayserver-deployment-5bd8ccf7c8-gkdqd   1/1     Terminating   0              34s
 gatewayserver-deployment-68f94d98ff-bwqt4   1/1     Running       0              7s
 keycloak-98d6b6f56-cdz96                    1/1     Running       4 (77m ago)    7d1h
 loans-deployment-674db986cc-6v95m           1/1     Running       12 (76m ago)   7d1h
 *
 * So, my k8s is terminating my old pod only when the new pod is set up successfully! If you had 50 different instances of your gatewayserver, k8s cluster is going to deploy your new changes in an incremental fashion so that your customers will not have any downtime. Hope you are visualizing the things! Whenever we want to deploy new changes into k8s cluster we can use the command discussed i.e., kubectl set image deployment gatewayserver-deployment gatewayserver=colince819/gatewayserver:v4 --record and note that the --record flag is deprecated and may soon b removed. If you run this command in future and get an error due to this --record flag, just simply omit it from the command and run the remaining command as is. It should work perfectly without any issues. Now you can run the command to get the events i.e., kubectl get events --sort-by=.metadata.creationTimestamp to see what are all the events that happened behind the scenes. If you try to see the list of events that happened, first you will see that my k8s cluster tried to pull a new image with the tag name i.e., v4. Post that it tried to schedule a new pod with the 'OBJECT' column name ending 'xxx'. In the 'REASON' column, you will notice that it will change from 'Pulling' and some few events down to 'Pulled' which means that the image is completed pulled into our pod. Using the same image, some events down you will see 'created' which mean the container was tried to created. Eventually it 'started'. During the same process it also killed the old pod and all the containers inside the old pod. Now, we can try to validate if we can invoke gatewayserver without any security. For the same inside my postman collection i.e., Microservices/gatewayserver_security there is a request i.e., 'Accounts_POST_NoAuth'. In this request we are not providing any authentication details. If you invoke this request to the gatewayserver, you will simply get a successful response! haha! This means that our new changes are deployed successfully into the k8s cluster. Now, lets assume that thetere are some issues with the new changes that we have deployed and want to rollback to the previous working state of my gatewayserver which is to the v5 tag from the v4 tag. Lets understand how to do the same, 1st I will run a command i.e., kubectl rollout history deployment gatewayserver-deployment. This will show you the rollout history that happened for the deployment with the name 'gatewayserver-deployment'. If try to execute this you will be able to see the revisions or the rollout history that happened for all the deployments that we tried.
 * As can be seen first we had the 1st revision where we initially deployed our gatewayserver. Post that in the second revision where we tried to give an invalid tag name. Like this the 'CHANGE-CAUSE' you will be able to see as well. So since this 2nd revision is also defective, of course who in there right senses wants to rollback to it?! And right now we are using the revision 3 because inside the revision 3 we are using the gatewayserver tag v4. Think like, I want to go back to the previous working revision which is 1. The command that I can run is kubectl rollout undo deployment gatewayserver-deployment --to-revision=1. As soon as I run this command, the rollback operation will be initiated. I can also confirm by running the kubectl get pods command and you can see some few seconds ago I have a gatewayserver related pod created successfully. The other pod got deleted behind the scenes. Now, if you try to see the image name against to this newly spin up pod name by using the command which is kubectl describe pod <pod_name> this time you should be able to see that the image name is of tag v5 which gives confidence and confirms that our rollback operation is also successful! As of now, we only have one container running and this may look very simple for you but think like you have 100 instances of gatewayserver running inside your k8s cluster, all the rollback or rolling out of the new changes is going to be automatically taken care by your k8s cluster. We as admin of k8s cluster we just have to issue a command. Now, we can go to the postman and try to test accounts creation operation without providing any authentication details by invoking the request gatewayserver-security/Accounts_POST_NoAuth. This time you should get a 401 error because we have point to a new tag which is v5. With all we have discussed you should be sufficiently clear on how bwe did the rolling of the new changes and the rolling back to the previous working version automatically with the help of k8s cluster. Very similarly, k8s is also capable of autoscaling your containers based upon your defined requirements.
 *
 * Autoscaling
 * ------------
 * Your k8s admins can create an object of 'Horizontal Pod Autoscaling' You can find more details in the official k8s documentaion i.e., https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
 * With this object, they can define the requirements that, 'I want to autoscale my pods based upon some memory or CPU utilization of the pod'. This way, based on the traffic that is coming towards your ms's, the number of containers are going to be automatically scaled up or down by the k8s cluster. We will not visualize the autoscaling feature of k8s in this session as this is a super advanced topic. You k8s admins usually take care of this inside the production cluster. But if interested to know how to do/achieve this then read the official doc - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. The reason we will not discuss this is because, this requires a lot of discussion around k8s cluster and is not our main focus - our focus is more on microservices and not k8s. In your memes folder you will see a funny image named '2025-11-06_08h00_11.png'. This shows you the relationship between your Deployment, ReplicaSet, Pod and Container in a funny manner. First we will give all our instructions/specifications with the help opf 'Deployment' object. The same 'Deployment' object behind the scenes is going to create the ReplicaSet based upon the number of 'replicas' that we have mentioned in our manifest file. So, if I try to mention 'replicas' as 2 or 5 , my ReplicaSet is going to create so many pods behind the scenes, I mean, if value is 2 it is going to create 2 pods and inside these 2 or 5 pods, the actual ms container will get deployed. Hoping yopu liked this funny image, haha!
 *
 * Introduction to k8s Service types
 * -----------------------------------
 * As of now, we deployed all our ms's with 'Service' 'type' as 'LoadBalancer'. Previously, on a high level we discussed that, 'LoadBalancer' value will expose all your ms's to the outside world, like all your applications who are staying outside of the cluster also they can access your ms's. But do you think that this is a correct approach to expose all our 'Services' to outside of the cluster?! Of course NOT! because this means that all the clients who are trying to communicate with our ms's, they should only communicate through the gatewayserver which is going to act as an edgeserver inside our ms's network. This means that we should only make our gatewayserver 'Service' 'type' as a 'LoadBalancer' value and for all the remaining ms's we should not use this value. But before we try to do that, we first have to understand what are the other options that we have to expose our ms's within k8s cluster or to outside of the k8s cluster.
 * Check slide for more details.
 *
 * Demo of k8s Service types
 * ---------------------------
 * We will see and visualize the demo of the k8s Service types that we have extensively discussed. 1st we will see/understand how we exposed our ms's. For the same we can try to run the command which is kubectl get services. In the output you should see all the services that we have created. Like for accounts ms we have given the service 'NAME' as 'accounts', 'TYPE' as 'LoadBalancer' as can be seen in the output below:
 $ kubectl get services
 NAME            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
 accounts        LoadBalancer   10.99.237.225    localhost     8080:31289/TCP   13d
 cards           LoadBalancer   10.109.131.174   localhost     9000:31383/TCP   13d
 configserver    LoadBalancer   10.111.222.102   localhost     8071:31149/TCP   13d
 eurekaserver    LoadBalancer   10.97.165.156    localhost     8070:32515/TCP   13d
 gatewayserver   LoadBalancer   10.110.178.58    localhost     8072:32549/TCP   13d
 keycloak        LoadBalancer   10.104.110.20    localhost     7080:31212/TCP   13d
 kubernetes      ClusterIP      10.96.0.1        <none>        443/TCP          13d
 loans           LoadBalancer   10.96.12.244     localhost     8090:31251/TCP   13d
 *
 * So, whenever we mention the 'LoadBalancer' as a Service type, you should be able to see an 'EXTERNAL-IP' value. Since right now we have our k8s cluster inside our local system we have the value to the 'EXTERNAL-IP' as 'localhost'. In the output, you can also see an internal service 'NAME' i.e., 'kubernetes' exposed by the k8s with a Service type of 'ClusterIP'. For this, you can notice that the value to the 'EXTERNAL-IP' is '<none>'. You acn also see the internal 'CLUSTER-IP ' i.e., '10.96.0.1' that other Services can use to communicate with this service with 'NAME' 'kubernetes'. Now back to the 'accounts' service in the output, we have defined the Service 'TYPE' as 'LoadBalancer' and behind the scenes like we discussed, whenever we define the Service 'TYPE' as 'LoadBalancer', k8s is going to create a load balancer with an 'EXTERNAL-IP' as can be seen in the output, in our case the value is 'localhost' and we can access that Service at the PORT 8080. Behind the scenes, it is going to listen at the node port 31289 which is usually randomly generated as can be seen and visualized from the above output.
 * Frm this node port, the request will go to the cluster ip in my case '10.99.237.225 ' and from this the request will go to the actual ms container. We can also verify this behavior by directly accessing one of the API's inside the accounts ms. So, inside the browser, I hit the GET endpoint http://localhost:8080/api/contact-info which is available inside the accounts ms and I got a successful response. This clearly tells you that our accounts ms is exposed to the outside of the cluster. So, regardless of how many instances of accounts ms I have deployed, I can always forward the requests to this, "localhost", public IP and to this port number,"8080" and with that behind the scenes my k8s is going to take care of load balancing completely. When we try to deploy a ms with the Service type as LoadBalancer into a cloud environment under the 'EXTERNAL-IP' column, you will be able to see a public ip address. So, inside the cloud environment whenever they are assigning a public ip address to your ms they are going to charge money because public ip addresses are not free inside any cloud environment. That's why for two reasons we should avoid exposing our ms's with the help of the Service type as 'LoadBalancer'. The first reason is, we need to secure our ms's so that no external clients they can directly communicate with the ms's, they can only and always come through the edge server. The second reason is, public ip's usually attract some cloud bill which we need to avoid if we have 100's of ms's. If you try to expose these 100's of ms's with the Service type as 'LoadBalancer', behind the scenes your cloud provider is going to create 100's of different Load balancers with public ip addresses which are going to to attract a lot of bill. With what we have discussed and visualized, you should now be having crisp clarity about the LoadBalancer Service type. This we are already using and that's why we didn't make any changes inside our k8s manifest files to the respective ms's.
 * As a next step, I am going to try make my accounts ms as ClusterIP. For the same, inside my accounts ms k8s related manifest file, I am going to change the Service type to ClusterIP.Once the changes to the file are done, apply the same by running the command kubectl apply -f 5_accounts.yml. Now, k8s will try to expose my Service of accounts ms as a ClusterIP. Now if you try to execute the command which is kubectl get services you should now be able to see that now the service record named 'accounts' in the output has the Service 'TYPE' as ClusterIP' and there is no 'EXTERNAL_IP' , the value is <none>. This means that any other application inside the cluster if they want to communicate with my accounts ms they are going to use the Service 'NAME' value i.e., 'accounts' or the 'CLUSTER-IP' value which in our case is 10.99.237.225. Along with these CLUSTER-IP and the Service NAME, they need to forward the requests at the port 8080. All this information I am picking from the output below:
 $ kubectl get services
 NAME            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
 accounts        ClusterIP      10.99.237.225    <none>        8080/TCP         14d
 cards           LoadBalancer   10.109.131.174   localhost     9000:31383/TCP   14d
 configserver    LoadBalancer   10.111.222.102   localhost     8071:31149/TCP   14d
 eurekaserver    LoadBalancer   10.97.165.156    localhost     8070:32515/TCP   14d
 gatewayserver   LoadBalancer   10.110.178.58    localhost     8072:32549/TCP   14d
 keycloak        LoadBalancer   10.104.110.20    localhost     7080:31212/TCP   14d
 kubernetes      ClusterIP      10.96.0.1        <none>        443/TCP          14d
 loans           LoadBalancer   10.96.12.244     localhost     8090:31251/TCP   14d
 *
 * Now, if I try to access the same GET endpoint we had accessed earlier i.e., http://localhost:8080/api/contact-info, from the browser in the case where we had configured our Service type as LoadBalancer, this time in the case where we have configured the Service type as ClusterIP, this time I will not get any response. Reason, my k8s is not allowing the traffic into the cluster for this accounts ms and that's why happily we are not getting any response.
 * As a next step, lets try mentioning the Service type as NodePort. Once we mention this, and btw I am not explicitly giving any nodePort value as I am fine with what will be automatically generated for me randomly for this demo. Apply the manifest file and on executing the command kubectl get services you should get an output like below:
 $ kubectl get services
 NAME            TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
 accounts        NodePort       10.99.237.225    <none>        8080:31597/TCP   14d
 cards           LoadBalancer   10.109.131.174   localhost     9000:31383/TCP   14d
 configserver    LoadBalancer   10.111.222.102   localhost     8071:31149/TCP   14d
 eurekaserver    LoadBalancer   10.97.165.156    localhost     8070:32515/TCP   14d
 gatewayserver   LoadBalancer   10.110.178.58    localhost     8072:32549/TCP   14d
 keycloak        LoadBalancer   10.104.110.20    localhost     7080:31212/TCP   14d
 kubernetes      ClusterIP      10.96.0.1        <none>        443/TCP          14d
 loans           LoadBalancer   10.96.12.244     localhost     8090:31251/TCP   14d
 *
 * This time, the accounts ms has a Service TYPE as NodePort and it is exposed at the node port value which is 31597. Now, if we want to access our accounts ms from the outside of the k8s cluster we need to send the traffic at this node port which is 31597. To validate this, inside my browser, I will try to send the traffic at the port 8080 i.e., http://localhost:8080/api/contact-info, you should be able to visualize that we are not receiving any response. Now I will replace the port number in the url with the node port value i.e., http://localhost:31597/api/contact-info. Ths time you should be able to happily get a successful response. This means, that we should only be able to access our accounts ms ONLY through the node port which is exposed my my k8s cluster. And in future if my accounts ms is deployed into different worker nodes, then definitely the ip address will change and I may face some issues while accessing this api. That's why it's always recommended to make use of LoadBalancer Service type if you want to expose a service to the outside world or you can simply go with the ClusterIP so that you will restrict all the external traffic from the outside world. So, in the account ms related k8s manifest file, I am rolling back the Service type to LoadBalancer. In the coming sections and discussions when we will be deploying all our components like kafka, grafana, etc. At that time I am going to change all these Service types to ClusterIP for all the ms's except gatewayserver. For now, I will leave the value of the Service type as LoadBalancer, so that when you are getting started with the k8s deployments based upon what we have just been discussing, things will be clear for you. With what we have been visualizing in our demo you should now be having crisp clarity.
 *
 * Problems with manually created k8s manifest files
 * ---------------------------------------------------
 * As of now, we deployed all our ms's to k8s cluster with the help of k8s manifest files. By following the same steps you can always deploy any number of ms's inside your real projects as well. That, as a developer you will be happy and your DevOps team members will be happy with the successful deployments of your ms's. But there are few problems if you follow this basic approach of deploying ms's into k8s cluster. We will discuss what those problems are and we we see a quick introduction about what is the solution that we are going to use. As of now if you go and see inside the k8s folder that is available inside our vcs i.e., Backend/Micro-services/Lobby/ we are maintaining the k8s manifest files which will help us to create the configmaps, deploy and expose our custom ms's as well as pre-defined ms/components like KeyCloak and all that. Don't you think there is a problem with this? As now think like I have 5 or 6 ms's and that's why we are able to achieve this job very quickly by creating these manifest files inside this k8s folder. Think like in real projects,you may have 100's of ms's. Preparing k8s manifest files and maintaining the same for all your ms's is going to be a nightmare. For sometime, somehow let's assume you are able to create the 100 manifest files and are able to proceed with your k8s deployments. The problem will not end here, once you have created all these manifest files, to deploy your ms's into k8s you need to apply all these manifest files one by one i.e., kubectl apply -f <file_name.yml>. Let's assume you are also fine with this problem and are super patient to apply all the files one by one. The problem is not going to end there. We have more problems haha! The problem is, you may have multiple environments inside your organisation i.e., dev, qa, prod. For all these environments you may have a k8s cluster and for different different environments you may have different different requirements. Like inside dev, you may want to have only one replica of accounts ms and other ms's whereas in qa you want have 3 replicas and inside prod you may want to deploy 5 replicas, 10 replicas based upon your incoming traffic. In such scenarios, you need to maintain your k8s manifest files according to your different different environments that you may have. Now imagine that you have 100's of manifest files multiplied by the number of your environments which means double or triple you k8s manifest files. Are you fine facing these challenges! haha. Of course no one will use this approach because this takes a lot of effort.
 * On top of that, even when we want to uninstall your ms's, in such scenarios also we need to run the commands like kubectl delete -f <file_name.yml>. This we can visualize, start deleting in the backward order you deployed the ms's. Good thing you have the manifest files numbered, so deleted from the highest number backwards. In our case starting with 8_gateway.yml...How painful it was for me to just delete/unistall the 8 manifest files/ empty my cluster / uninstall my applications from the k8s cluster. Think like if you are having 100's of ms's then you need to run this command 100 different times which is a challange for anyone. Now if you go to the k8s dashboard under the default realm/namespace , you will see all of our deployments, pods, everything got deleted, because we deleted/uninstalled/cleared all our deployments and services with the help of this kubectl delete -f <file_name.yml>. Now, back to the challenges discussion, the most important question is how are we going to mitigate/solve these problems/challenges? For the same we have a solution which is helm - https://helm.sh/. Helm is a package manager for k8s. In the next session/commits we are going to focus on helm, that is how to set up our ms's with the help of helm. You are going to like it because it is going to make our life super easy.
 * * * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
