package com.get_tt_right.gwserver;

import io.github.resilience4j.circuitbreaker.CircuitBreakerConfig;
import io.github.resilience4j.timelimiter.TimeLimiterConfig;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.circuitbreaker.resilience4j.ReactiveResilience4JCircuitBreakerFactory;
import org.springframework.cloud.circuitbreaker.resilience4j.Resilience4JConfigBuilder;
import org.springframework.cloud.client.circuitbreaker.Customizer;
import org.springframework.cloud.gateway.filter.ratelimit.KeyResolver;
import org.springframework.cloud.gateway.filter.ratelimit.RedisRateLimiter;
import org.springframework.cloud.gateway.route.RouteLocator;
import org.springframework.cloud.gateway.route.builder.RouteLocatorBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.http.HttpMethod;
import reactor.core.publisher.Mono;

import java.time.Duration;
import java.time.LocalDateTime;

/** Update as of 28/11/2025
 * Creating Helm Charts for other ms's
 * ------------------------------------
 * As of now, we created helm chart for accounts ms only. As a next step, we will be creating a helm charts for the remaining ms's as well. For the same this time we will not follow all the steps that we have followed in the previous sessions. This time I will simply copy the accounts helm chart and paste it inside the same folder and rename it to cards so that this helm chart is going to act as a helm chart for cards ms. If you open this cards folder now, inside ypu will have a chart.yaml file. Open the same then change the name k-v from accounts to cards. Post that, we don't need to make any other changes in this file as we should be good. As a next step, open the values.yaml file. We won't touch the templates folder and its contents as they are generic. Similarly for the charts folder, we don't need to touch it as we already have the compiled version of getttrightbank-common. The file Chart.lock will be generated by the helm whenever you try to compile your helm chart. For some reason if you are getting some issues during the compilation you can always remove this Chart.lock and try again. For now we don't need to compile this cards ms again because we already have the dependent charts compiled and available in compressed format inside the charts folder. Now, lets open the values.yaml file. Her we are going to change some few values, in place where we have accounts just change that to cards. replicaCount will also stay as 1. For the image details i.e., name just change accounts to cards, we should be fine with the tag value. For the containerPort assign it a value i.e., 9000, the same value also mention under port and targetPort. For Service type we are fine with 'ClusterIP'. Coming to the boolean values, we should be fine with all the assignments except kafkaEnabled - change its value to false as my cards ms is not going to connect with the kafka. With this we should be good from the cards ms perspective.
 * In the very similar fashion, I am going to create the other ms's specific helm charts as we have done above for cards ms. We know that all the stuff is the same except values.yaml file and the name k-v inside the chart.yaml file. Now, lets see what is inside the values.yaml file of each of this helm charts. For all the helm charts where we have 'cards' just change that to the respective helm chart name value i.e., configserver, loans, etc. For configserver values.yaml file, I will assign 8071 as values for containerPort, port and targetPort. Service type is going to be clusterIP. Coming to the boolean values, we don't need to provide any profile details to the configserver because it is going to load all the properties of all the profiles. Only the individual ms's i.e., accounts, cards, loans need this property because based on the activated profile they need to fetch the properties from the spring cloud configserver. So, profile_enabled and config_enabled values should be false. For config_enabled, it is false because my confiserver does not need any url details of configserver/itself. Very similarly, eureka_enabled is also going to be false. resources_enabled is also false. otel_enabled as true and kafka_enabled as false. That' it for configserver helm chart.
 * As a next step, lets open the values.yaml file of eurekaserver helm chart. Here also the story is same. For containerPort, port and targetPort I will mention the value 8070. Coming to boolean value, appname_enabled is going to be true, profile_enabled: false, config_enabled: true, eureka_enabled: false, resouceserver_enabled: false, otel_enabled: true and kafka_enabled: false. That' it for eurekaserver helm chart.
 * For the values.yaml of gatewayserver, same drill also. containerPort, port and targetPort will be 8072. Now, for boolean values, resourceserver_enabled will be true because this is the only ms which is going to act as a resource server inside our ms. At the same time since we want this to be exposed to the external client applications, we have mentioned the Service type as 'LoadBalancer'. Coming to the boolean values, appname_enabled: true, profile_enabled: true, config_enabled: true, eureka_enabled: true, otel_enabled: true, kafka_enabled: false. That' it for gatewayserver helm chart.
 * For value.yaml file of loans ms, same drill. containerPort, port and targetPort will be 8090. For the boolean value, it is going to be same as accounts and cards ms's. That' it for loans helm chart.
 * At last, the values.yaml file which is present inside the message ms helm chart. My message ms is build on top of the Spring Cloud Functions and Spring Cloud Stream. For that reason we don't need to pass the profile details, configserver details, eurekaserver details hence their respective boolean value being assigned false. otel_enabled also false, resouceserver_enabled: false but kafka_enabled as true. containerPort, port and targetPort will be 9010. That' it for message helm chart.
 * With all we have discussed you should be super clear now. This way we have created the helm charts for all the ms's and we don't have to compile them again because the dependent charts folder are already available. If you try to recompile again the recompilation will happen and you will get the same compressed file by replacing the older one. You can try the same for one of the ms's helm chart i.e., I am randomly doing this for cards helm chart and inside its terminal I am going to run the command helm dependencies build. You should be able to see a successful message saying that it deleted the outdated charts and replaced with new ones. But like we discussed,there is no need of recompiling again because we are going to use the same getttrightbank-common helm chart which we have compiled before inside the accounts ms related helm chart. You may have a question like, we are doing a lot of manual effort to creating these helm charts but we discussed that it is going to make our life easy, haha. It is worth noting that this is a one time activity ONLY! Once you set up your helm ecosystem, this is going to make your life super easy. As of now, you can see that we have created our required helm chart for our required ms's. So far, if we go with this set up, we still need to install all these helm charts individually and manually one by one. This is not our intention. That's why we are going to create one more helm chart specific to an environment and inside that specific environment helm chart we are going to define all the configMap related values along with the dependencies on these ms's helm charts that we have created so far. Don't worry if something is not clear on how all these helm charts are going to be interlinked. When we visualize the demo then it is going to make more sense to you.
 *
 * Creating helm charts for dev, qa and prod environments
 * -------------------------------------------------------
 * We will create helm charts specific to an environment so that we can deploy/install all our ms's with a single helm chart command. For the same inside the same folder where we have getttrightbank-common and getttrightbank-services, we are going to create a new folder with name 'environments'. Navigate to this new folder in your terminal as we are going to create a new helm chart here. First we are going to create a helm chart which is specific to the dev environment so, run the command which is helm create dev-env. On trying to execute this a helm chart should be successfully created. In your Windows file explorer, navigate to this newly created helm chart and first things first, inside the templates folder, delete everything. Similarly, open the values.yaml file and delete everything so that the file is empty. As a next step, open the chart.yaml inside which I am fine with the k-v i.e., apiVersion, name, description, type and even version. Here the change we need to make is on the value to the key appVersion - make it as 1.0.0 so that we are consistent with the other helm charts that we have create so far. As a next step, we need to define the details of all the dependent helm charts - you already know the format that we need to follow. Using the dependencies element we can mention any number of helm chart details. Actual have just pasted this and you can see the same inside the chart.yaml file of dev-env and even other environments specific yaml chart that we will be creating in the future. Nothing fancy as can be seen in the docstring below:
 dependencies:
 	- name: getttrightbank-common # First I am trying to mention getttrightbank-common helm chart dependency.
 	  version: 0.1.0
      repository: file://../../getttrightbank-common

 	- name: configserver # Post that I am trying to mention configserver
	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/configserver

 	- name: eurekaserver # Post that I am trying to mention eurekaserver
 	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/eurekaserver

 	- name: accounts # Post that I am trying to mention accounts
  	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/accounts

 	- name: cards # Post that I am trying to mention cards
	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/cards

	 - name: loans # Post that I am trying to mention loans
 	   version: 0.1.0
 	   repository: file://../../getttrightbank-services/loans

 	- name: gatewayserver # Post that I am trying to mention gatewayserver
 	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/gatewayserver

 	- name: message # Lastly, I am trying to mention message helm chart dependency
 	  version: 0.1.0
 	  repository: file://../../getttrightbank-services/message
 *
 * Once you have defined all these details, as a next step we can go to the templates' folder. As of now you can see it is empty. But before I can proceed with the next steps, What do you think are the template files that we need for this dev-env helm chart? Do you think we need deployment.yaml and service.yaml manifest files whenever we need to set up an environment? haha. Of course, we don't need them. Reason: all those ms specific template files are already present inside the helm charts which are specific to your respective ms's. Inside the environment related helm chart we just need to use the configmap.yaml template. That's it! Reason: Whatever we define with the help of configmap.yaml template, it is going to be common for all the ms's. That's why inside this templates folder we need to have template which is related to the configmap.yaml. For the same I pasted a configmap.yaml file there which is you open, you will see the contents like: {{- template "common.configmap" . -}} haha. Here we are not going to mention the actual template of configmap. Reason: We already defined a configmap template inside the getttrightbank-common helm chart with the name common.configmap. The same we are trying to import here and use the same template. With this we should be good to go all matters templates folder inside the dev-env helm chart. As a next step, we should populate the values.yaml file. So I pasted the below inside this vlaues.yaml file:
 global: # As you can see here, the prefix value that I have mentioned for all these values is 'global' Because whatver I ma going to define inside this values.yaml file  is going to be applicable for all my ms's. So, that's why I have this prefix but this is not mandatory or a standard from helm. This is a prefix that I want to maintain for my own understanding.
 	configMapName: getttrightbankdev-configmap # So, under the global.configMapName key, I am going to mention the name of the configmap. The same is going to be referred/used in the configMap template that we have created. If you go to the getttrightbank-common helm chart, you will see that in the templates folder we have created a configmap.yaml which if you open you will se that here , we are trying to create the configmap and under the metadata element, we need to give the name for our configmap. That's why you can see the name is going to be gotten from  name: {{ .Values.global.configMapName }} which is what we have defined here inside the values.yaml file of dev-env helm chart. So, for dev environment, I am going to have the configmap name as getttrightbankdev-configmap.
 	activeProfile: default # Here, for active profile I have mentioned default. Reason: Right now we are trying to build a chart for the dev environment. So, inside the dev environment we want to use the default profile of Springboot.
 	configServerURL: configserver:http://configserver:8071/ # Here, I am going to mention the configserver URL. This is the value where my configserver is going to be available. To note: The hostname has to be same as your service name that you have defined. So the service name that we have defined inside the service.yaml file of configserver helm chart is 'configserver'.
 	eurekaServerURL: http://eurekaserver:8070/eureka/ # Similarly, for eureka server, I am going to mention the eureka server URL. For the same reason, the service name that we have defined inside the service.yaml file of eurekaserver helm chart is 'eurekaserver'.
 	keyCloakURL: http://keycloak.default.svc.cluster.local:80/realms/master/protocol/openid-connect/certs # Now coming to the KC url, you can see that the hostname in our case is 'keycloak.default.svc.cluster.local' and it is going to be available at the port 80. This is new right! haha. In the coming sessions, we are going to set up all the 3rd party components like Keycloak, Kafka, and Grafana components with the helm charts only. In that process, they are going to be deployed with production standards. So, when we follow those production standards, that's how our service name i.e., 'keycloak.default.svc.cluster.local' is going to be built inside the k8s cluster and that's why we are trying to mention the same here.
 	openTelemetryJavaAgent: "-javaagent:/app/libs/opentelemetry-javaagent-2.11.0.jar" # After this KC url, we are trying to mention where is the path where my open telemetry related jar is going to be present.
 	otelExporterEndPoint: http://tempo.default.svc.cluster.local:4318 # At last what is the tempo url of grafana. This, tempo.default.svc.cluster.local, is the tempo url service/host name and it is going to be available at the port 4318. In earlier versions it was 4317. Since we are going to install grafana tempo with the helm chart, that is how the service name/host name is going to come.
 	otelMetricsExporter: none # The Otel metrics exporter is going to have a value of none.
 	otelLogsExporter: none # The Otel logs exporter is going to have a value of none. In earlier versions, we were not taking care of this but as per recent versions of Opentelemetry we should take care of otelLogsExporter.
 	kafkaBrokerURL: kafka-controller-0.kafka-controller-headless.default.svc.cluster.local:9092 # Coming to the kafka broker url, this is the value that we are mentioning. Here also the service name/ host name is not going to be simple because we are going to install the kafka with the help of helm charts available inside the web.
 *
 * So, with all the above values, the configmap.yaml of dev-env helm chart is going to be populated which is nothing but {{- template "common.configmap" . -}} which is the configmap.yaml of getttrightbank-common helm chart. Like this behind the scenes a configmap is going to be created inside your k8s cluster and from the same configmap all your ms's can read the values at runtime. If you have any question, check the docker-compose files that we built initially as the same properties we injected to the respective ms's with the help of docker-compose as well.
 * With this, we have set up the values.yaml file, templates folder - configmap.yaml of dev-env helm chart, and we also updated it's respective chart.yaml file. As a next step, we need to compile this dev-env helm chart so that we will be able to see all the dependent charts inside it's respective charts folder. For the same in your termil, navigate to the dev-env helm chart folder and run the command: helm dependencies build. This is going to compile all the dependent helm charts. Once the compilation is successful, if you navigate in your Windows files explorer under the charts folder of dev-env helm chart, you will see all the dependent helm charts are compiled and ready to be used. They are available in a packaged/compressed format. With this, we have successfully set up the helm chart for one environment which is the dev environment.
 * Very similarly I am going, we will create helm charts for qa and prod as well. For the same, just copy dev-env and paste it in the same environments folder and rename it to qa-env. Post that, open the chart.yaml file of qa-env and rename the name to qa-env. With this, we should be good from chart.yaml perspective. As a next step, we can update the values.yaml file of qa-env. The changes that we need to do here are: under the activeProfile key we need to mention the profile value as qa, similarly I will make the configMapName key value as getttrightbankqa-configmap. Apart from these 2 changes, we no need to change any other values but for some reason if you are having some different hostnames or service names inside your specific environment you can always update these values. With this we set up the qa-env helm chart also. Same drill, we will repeat for prod environment helm chart. With this, we have created all the required helm charts for all the environments.
 * So, now, whenever I want to set up my ms's, I can simply install/deploy one of the helm charts i.e., dev-env, qa-env, prod-env. Behind the scenes, all the ms's will be installed/deployed with the help of the helm charts along with the required configmaps. So far, you should now be having crisp clarity on what we have been discussing.
 *
 * Demo of helm template command
 * -----------------------------
 * As of now, we have done a lot of pre-work of preparing our own helm charts which are specific to getttright-bank ms's. Sometimes b4 you try to install your helm charts you may want to know how your K8S manifest files are going to look like that are going to be generated by your helm chart. To do this validation, we have a command inside the helm which is helm template. With the help of this command we can see all the k8s files that are going to be generated by your helm and the same k8s manifest files are also going to be installed/deployed inside your k8s cluster. So, it will be a wise decision to validate the k8s's manifest files before we try to install them into the k8s cluster. So, to execute this helm template command, we need to make sure that our terminal is opened inside your respective helm chart. In my case I am going to open this terminal inside the dev-env helm chart. From this location, I can try to run the command which 'helm template .'  With this dot, I am telling, my helm template is available inside this folder only so please access the same and show me all the k8s manifest files that you are going to prepare. On executing this, you should expect a big output on your terminal. If you run into any issues, the output will direct you where exactly the culprit you need to check and fix that - it could be as tiny as a spelling mistake! So be very keen. Once you fix that, I need you remember this, for example assuming the fix was in one of the files inside getttrightbank-common chart, this implies that we neeed to recompile all the charts where getttrightbank-common chart is a dependency. For the same, I will go to the getttrightbank-services folder location in my terminal. Here we have respective ms's helm charts. First I will navigate to the accounts ms helm chart and here you need to run the command helm dependencies build. Same, drill, run this command for all the remaining ms's. Like this you will have compiled all the ms's specif helm charts. As a next step, I wil navigate to the folder location which is related to the environments. Inside this I will navigate to the dev-env folder/chart and here also I am going to run the command helm dependencies build also because we want to recompile the dev environment also. Same drill also for qa-env and prod-env.
 * Now, this time if you run the command 'helm template .' in any of dev-env, qa-env or prod-env, you will get a big output on your terminal of all the k8s manifest files that are going to be generated and installed/deployed inside your k8s cluster for all our ms's. You can easily validate this. For example for message ms you can see something like below among the output:
 # Source: qa-env/charts/message/templates/deployment.yaml
 apiVersion: apps/v1
 kind: Deployment
 metadata:
 	name: message-deployment # The metadata name is going to be message-deployment
 	labels:
 		app: message
 spec:
 	replicas: 1 # And replicas is going to be 1
 	selector:
 		matchLabels:
 			app: message
 	template:
 		metadata:
 			labels:
 				app: message
 		spec:
 			containers:
 			- name: message
 			  image: "colince819/message:v9"
 			  ports:
 			  - containerPort: 9010
 				protocol: TCP
 			  env: # And below are all the environment variables that we are going to inject for message ms.
 			  - name: SPRING_APPLICATION_NAME
 				value: message
 			  - name: SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS
  				valueFrom:
 					configMapKeyRef:
 						name: getttrightbankqa-configmap
 						key: SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS

 *
 * As can be seen, this is the 'Deployment' instruction for message ms. Same drill for all the other ms's as can be verified from the terminal output. So this is what you need to cross-check one by one to make sure everything is correct as expected. All that structure/k-v pairs of k8s 'Deployment' instruction we already discussed element by element in details and so you should not be having any doubt about anything. Yes! the output on the terminal as discussed is going to be big because we have around 7 m's and for all of them as can be visualized from the terminal output helm provided us all the actual k8s manifest files that are going to be generated and installed/deployed inside our k8s cluster. So yea, those are all the actual k8s manifest files that will get executed into the k8s cluster when you try to install a specific environment helm chart i.e., dev-env, qa-env or prod-env. Same, drill you can also validate for other environments as well. Since we executed the helm template . command for prod environment you can see in the 'Deployment' instructions on the terminal output that we are getting the configmap name as 'getttrightbankprod-configmap'. If you execute the helm template . command for other environments you should be able to see a different configmap name i.e., 'getttrightbankdev-configmap' or 'getttrightbankqa-configmap' and so on.
 * So, I cleaned the terminal and navigated to the dev-env folder and ran the command 'helm template .'. And yes, you should be able to get successful response which you can validate as well, the same way we have done for qa-env above. Same drill also you can perform for prod-env. So, as a next step, to deploy all our ms's into the k8s cluster, we can simply install one of the environment helm charts. But before that we need to make sure that we set up KeyCloak, Kafka and Grafana related components inside our K8S cluster. For those we don't need to prepare any helm charts manually because, all these products are heavily used by the industry and with that reason, many open-source communities and organizations, they build the helm charts specific to these components like Kafaka, Keycloak, Grafana etc. So, let's try to explore them and try to learn how to set up those components with the help of helm. Up to this point you should now be having crisp clarity.
 *
 * Install KeyCloak into k8s cluster using helm chart
 * ----------------------------------------------------
 * One of the great advantage of helm is, it has a very good community and with that reason you should be able to easily find the helm charts to install any product inside the software industry. For example, If you want to install kafka inside your k8s cluster, you don't have to prepare the k8s manifest files manually, instead you can rely on the helm charts available inside the web. One such company/community which always maintains these helm charts with great production standards is bitnami - https://bitnami.com/. It makes it easy to get your favourite open source pieces of software up and running on any platform including your laptop, k8s or all the major clouds. This bitnami is supported by VmWare. Inside our sessions, we are going to leverage bitnami helm charts to install all the products like kafka, Grafana, Prometheus, KeyCloak, etc. inside our k8s cluster. For the same, inside google you can search " bitnami helm charts GitHub". This will show you the GitHub repo where bitnami is maintaining all the helm charts - https://github.com/bitnami/charts. Inside this repo, if you open the bitnami folder you should be able to identify the helm charts for the majority of the products that any organization needs i.e., argo-cd, cassandra, consul, drupal, elasticsearch, similarly all Grafana related helm charts are also available i.e., grafana-mimir, grafana-loki, grafana-tempo, grafana, etc. If you scroll down, you can be able to see many other products like Kafka, KeyCloak, Kibana, kube-prometheus, mongodb, logstash, mariadb, mysql, ngnix, postgresql,rabbitmq, redis, redis-cluster, etc. Like this, there are a good amount of helm charts available and so based upon your requirements you can always leverage these bitnami helm charts. So far we saw and leveraged one of the helm charts which is WordPress while visualizing about the demo of helm charts previously. In this same github location, we have many other helm charts and we are going to leverage them to set-up required components inside our k8s cluster.
 * So, to download these helm charts, what I can do is, navigate to the charts folder in the bitnami github repo. And under the Code button, you will see we have an option to download all those helm charts into local system. So, I will click on the download zip option - This should download all the help charts that are provided by the bitnami. I can then proceed to choose and edit whatever I require and leverage that to install the components into my k8s cluster. Where you have download the zip to, extract it and then you can open charts-main/bitnami folder, and you can see all the helm charts that are available and provided by the bitnami. As a first step, we are going to use the KeyCloak helm chart. Copy that folder/helm chart of keycloak and paste it to the location, helm folder where we have environments, getttrightbank-common and getttrightbank-services, where we are maintaining our helm charts that we are trying to create. Now, if you open this keycloak helm chart that we have just pasted in the helm folder you will see we have all the required files that we have been discussing like, chart.yaml, templates folder and values.yaml. If I try to run this KeyCloak  helm chart into my local k8s cluster by default it is going to deploy my KeyCloak service with a ClusterIP. But since I want to access my KeyCloak to create/set up the Client details and roles information, I am going to expose my KeyCloak Service as a LoadBalancer. So to change this behavior we need to open the values.yaml file present inside the KeyCloak helm chart. Inside this file, as can be seen we have a lot many variables/properties, so you can search for 'ClusterIP' because this is the default service.type key value that KeycLoak  helm chart is going to have mentioned. So instead of ClusterIP, change that to LoadBalancer. Once you have altered this service.type value, you can look/search for a variable with the name 'adminPassword'. As of now, you can see that my KeyCloak helm chart is going to create the admin credentials with the adminUser as 'user' and the adminPassword is by default mentioned as empty value. So, whenever my adminPassword is empty, behind the scenes, my KeyCloak helm chart is going to create a Secret which is going to have some random password. So, instead of this default behavior, what we can do is we can alter this and give our own password for the adminPassword variable instead of it being empty and following the default behavior that we have just discussed. So, here, we are going to give the adminPassword value as 'password' so that we can login into the KeyCloak admin console with the credentials 'user' and 'password'. Once we make these changes we should be good.
 * Now, let's go to the terminal and install the helm chart of KeyCloak. So, in your terminal, navigate to the keycloak folder. From this, run a helm command - helm install <And what is the name that you want to give for your keycloak service installation>. I will give this as 'keycloak' itself. So, this is going to be the name of my release that I am trying to install into my K8S cluster with the help of KeyCloak helm chart. So, once you have given some name to your release, you can mention what is the chart/folder name where your chart is present. So the folder/chart name is going to be keycloak itself. So our full-fledged command is going to be 'helm install keycloak keycloak'. So, lets try to execute this command. You will see the output like below:
 tmi@DESKTOP-CR6ONMF MINGW64 /d/Software Downloads/Spring Boot/Spring/SPRING Programming Tutorial  by Nagoor Babu Sir On 15-02-2019/VCS-Backend/Backend/Micro-services/Lobby/helm (main)
 $ helm install keycloak keycloak
 Error: INSTALLATION FAILED: An error occurred while checking for chart dependencies. You may need to run `helm dependency build` to fetch missing dependencies: found in Chart.yaml, but missing in charts/ directory: postgresql, common

 * This is saying nothing but, " My KeyCloak helm chart has some dependencies missing". So, how to fix this? We need to run the helm dependencies build command inside my keycloak helm chart. So, naviaget to the keycloak helm chart/folder in the terminal and inside this, I am going to run the command which is helm dependencies build. This will compile all the dependencies and make sure that they are available in the packaged format inside my keycloak helm chart. Now once the build is completed you can now go back to the previous folder i..e., cd ../ From this, you can now run the helm install keycloak keycloak command and you should see that the installation/deployment happened successfully as can be seen from the output below.
 tmi@DESKTOP-CR6ONMF MINGW64 /d/Software Downloads/Spring Boot/Spring/SPRING Programming Tutorial  by Nagoor Babu Sir On 15-02-2019/VCS-Backend/Backend/Micro-services/Lobby/helm (main)
 $ helm install keycloak keycloak
 coalesce.go:301: warning: destination for postgresql.tls.autoGenerated is a table. Ignoring non-table value (false)
 NAME: keycloak
 LAST DEPLOYED: Wed Dec  3 07:53:23 2025
 NAMESPACE: default
 STATUS: deployed
 REVISION: 1
 TEST SUITE: None
 NOTES:
 CHART NAME: keycloak
 CHART VERSION: 25.3.0
 APP VERSION: 26.3.3

 âš  WARNING: Since August 28th, 2025, only a limited subset of images/charts are available for free.
 Subscribe to Bitnami Secure Images to receive continued support and security updates.
 More info at https://bitnami.com and https://github.com/bitnami/containers/issues/83267

 ** Please be patient while the chart is being deployed **

 Keycloak can be accessed through the following DNS name from within your cluster:

 keycloak.default.svc.cluster.local (port 80)

 To access Keycloak from outside the cluster execute the following commands:

 1. Get the Keycloak URL by running these commands:

 NOTE: It may take a few minutes for the LoadBalancer IP to be available.
 You can watch its status by running 'kubectl get --namespace default svc -w keycloak'

 export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[?(@.name=='http')].port}" services keycloak)
 export SERVICE_IP=$(kubectl get svc --namespace default keycloak -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

 echo "http://${SERVICE_IP}:${SERVICE_PORT}/"

 2. Access Keycloak using the obtained URL.
 3. Access the Administration Console using the following credentials:

 echo Username: user
 echo Password: $(kubectl get secret --namespace default keycloak -o jsonpath="{.data.admin-password}" | base64 -d)

 WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
 - resources
 +info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

 * This output also highlights what are the steps that you can follow to access your keycloak url. In step 1, if you can run the commands given i.e.,
 export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[?(@.name=='http')].port}" services keycloak)
 export SERVICE_IP=$(kubectl get svc --namespace default keycloak -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

 echo "http://${SERVICE_IP}:${SERVICE_PORT}/"
 *
 * Copy and run all the above 3 commands at once. Now you should be able to get a url using which you should be able to use to access your keycloak. In the output I got, http://:80/, as the url. This is the url that I am going to use to access my keycloak. I can be able to access my keyclaok at the url http and host name since it is empty we can use localhost and the port is 80. Similarly, if you want to know what is the username and password , in step 3 there are also some instructiosn that will help you knwo the username and password. You can try to take the command in step 3 and execute them in your terminal at once i.e.,

 echo Username: user
 echo Password: $(kubectl get secret --namespace default keycloak -o jsonpath="{.data.admin-password}" | base64 -d)
 *
 * You will see an output where, Username: user and Password: password. If you remember isnide the values.yaml file we altered the password to password itself. So, after running the deployment/installation command i.e., helm install keycloak keycloak, you have to be a bit patient for like 1 to 2 minutes for the load balancer to get successfully created inside your local system. Easily it is going to take 1 to 3 minutes and post that, you should be able to access the keycloak at the url http://localhost:80/. Enter your KeyCloak credentials and after successful sign-in we are going to create a new client i.e., get_tt_right-bank-callcenter-cc. Click on Next >> Enable the Client authentication toggle >> Under Authentication flow enable only the 'Service Account roles' >>  Click on Next >>  Save. Now, under Credentials tab, copy the Client Secret value and paste the same inside the Postman Authorization tab. Also make sure to change the Access Token URL value in the Postman Authorization tab to from the 7080 port to 80 becuase right now, our helm chart exposed the KeyCloak Service at the port 80. Now If I try to get an access token it should successfully work! But we cannot use this because as of now we have not created any roles. To do so, back to the KeyCloak Admin console, Click Real Roles >> Create Role i.e., ACCOUNTS, CARDS, LOANS. Now Click on the Client LHS nav >> Open the client that we created i.e., get_tt_right-bank-callcenter-cc >> Service accounts roles >> Under the Assign role drop down select Realm roles >> Select all the required roles >> Click on Assign to assign the roles. Did you see how easy it is to set up KeyCloak Server with production standards by using helm charts? That's the power of helm charts, if you don't have helm charts then definately you need to prepare lot many k8s manifest files. We can also visualize the template files and with that we should be able to understand how complex it is to create and install KeyCloak inside a production k8s cluster. Under the templates folder of the keycloak helm chart provided by bitnami, you should be able to see lot many templates related to k8s objects all of which to create manually is going to be a super cumbersome process. On top of that the Keycloak helm chart provided by bitnami has dependency on other helm charts like postgresql because behind the scenes my KeyCloak is going to use Postgres database. As a next step, how can we derive the KeyCloak end-point url that we can feed to the other ms's. If you see inside the values.yaml of all the environment helm charts, we have mentioned the keycloak url as keyCloakURL: http://keycloak.default.svc.cluster.local:80/realms/master/protocol/openid-connect/certs. This is the host name - keycloak.default.svc.cluster.local. The port number is 80. So how should I know the hostname that I want to provide? For the same if you try to look at the output of the helm installation command,that we received from the terminal. You will see some important information saying that, "Keycloak can be accessed through the following DNS name from within your cluster: keycloak.default.svc.cluster.local (port 80)". So, that same name is what we have mentioned/defined inside the values.yaml file of environment helm charts. So, when we try to give these url details to gatewayserver, it should be able to easily connect to our KeyCloak because my gatewayserver is also going to be deployed in the same k8s cluster. With this we have successfuly installed KeyCloak with helm charts. Similarly we need to set up other components like kafka, grafana and prometheus components. But before we try to do that, since we are going to do lot many installations into our k8s cluster, it may not work inside your local system because you will have very less memory inside your local system. So, to some extent we can try to overcome this challange by changing some settings inside the docker desktop.
 * In the docker desktop, I can click on the settings icon >> Resources nav >> Here you can try to increase the memory that you are trying to allocate to your docker desktop. By default inside my instructors sytem previously his cpus allocation was at 4 and memory at 8GB but with those default values, he faced lot many issues because the installations started becoming very slow and the response from the microservices were also very slow. To overcome this challange, he increased the number of CPUs to 6 and memory to 12GB. He went ahead and advised that if there is a feasibility inside your local system, you can try do the same so that your installations will be fast and the throughput of your ms's will be high. For me I found out that I can be able to adjust these setting bresources by searching for WSL settings and doing the resources adjustments. But I didn't do that as I want to encounter and feel the challenges by myself first and then proceed to diagnosing the problem as instructed. Inside production this should not be a problem because inside cloud you will have some good amount of memory and CPUs. For some reason if your laptop is not cooperating - don't worry as we are going to do the same exercise in the cloud and during that time we can follow the instructors instructions and do the deployments into the cloud environment.
 *
 * Install Kafka in k8s cluster using helm chart
 * -------------------------------------------------
 * As of now, we successfully installed KeyCloak component inside our k8s cluster. You can confirm the same by going to the K8S admin dashboard >> Pods >> You should be able to see 2 pods. One related to keycloack and the other related to keycloak postgresql - That's for the bitnami keycloak helm charts that my instructor was using. But I din't manage to leverage the same as I ran into issues which I will try to troubleshoot in future to understand why and how to overcome them. As a workaround, I noticed my instructor is also not levarging the bitnami helm charts recently also - maybe due to the same challange as he resorted to manually creating his own helm charts as can be noticed from his GitHub repo. Anyway, that's what we are going to follow for now as we are focusing on learning as much concepts as we can as far as ms's are concerned. Similarly, you can click on the services nav-link in the k8s admin dashboard default realm/nameSpace. You should be able to see services related to keycloak Also under the config Maps nav link, you should also be able to see. You can confirm the same under Secrets nav-link. So, in all these locations/nav-links we have componenets related to KeyCloak - If you are using the keycloak production ready bitnami helm charts. This should confirm to you that the keycloak installation/deployment is complete. For some reason if you uninstall the helm chart of keycloak or any helm chart with the help of helm uninstall and try to install again, sometimes you may face some issues and your installation may not be successful. The reason is: As at the time my instructor was doing this tutor he said that there was a defect in helm which is related to Persistent Volume Claims - This you can be able to see in your k8s admin dashboard under Config and Storage. So, using this PVC, the pods can claim some space inside the worker nodes. So whenever you try to uninstall a specific helm chart from your k8s cluster, it is not going to delete the PVC's that are created as part of the installation and this may create some issues. So, to overcome the problem, you have to delete the PVCs manually. For I was able to see the data-happy-panda-mariadb-0 PVC - though I had uninstalled the woprdpress helm chart long time ago, I can note that there is still a PVC available inside the k8s cluster. So don't retain this. otherwise next time when you try to install wordpress, it will face issues/may not even work or may not be successful. So, to avoid this, you have to delete the PVCs manually. This is the way to overcome the problem. To delete the PVCs there are 2 options. In the k8s admin dashboard you can straight away on the specific PVC record at the far RHS you will see 3 vertical dots which if you click on you have the option to delete that PVC record. The orther option is, from the terminal we acn execute a command i.e., kubectl get pvc which wil list all the PVCs available and by running the command kubectl delete pvc <PVC_NAME> you can delete the specific PVC. With this my PVC will get deleted and next time if you try to install your wordpress related helm chart there should be no any issues. This is one of the limitations you need to know about the helm uninstall command.
 * Now, let try to set up the kafka component with the help of helm chart. For the same, my instructor copied the kafka related helm chart from the charts-main/bitnami folder that we had earlier on downloaded and pasted that in the location where we are mentioning all our the other helm charts i.e., the just created keycloak helm chart. He said that he is going to chekout all this into his GitHub repo so that we don't have to do all thes steps that he is doing i.e., copying and pasting and also changing the values inside the yaml files of these 3rd party related helm chart. Now, if you try to install this bitnami kafka helm chart it is going to have all production ready standards but my instructor says that he doesn't want to follow that inside his local system as he doen't have such a high capacity inside his local system. So what he did to overcome this challange, he went to the kafaka helm chart >> Opened the values.yaml file >> Searched for the property which is 'replicaCount' which has a value 3 assigned to it by default - this means that my helm chart is going to create 3 kafka broker nodes inside my local k8s which we don't want as that will slow down my system, in real production applications only, your kafka admin is going to control all these replica count and other values. For our local testing we need to change this value from 3 to 1. Apart from this change I also want to reduce some security related communication because inside the production, always the communication with kafka is going to be secured but inside our local system he didn't want to follow that and set-up complex security communication. So, to change this, he searched for the value which is 'SASL_PLAINTEXT'. As of now as can be seen, the protocal key has a value mentioned as 'SASL_PLAINTEXT' - this is the default security protocal my kafaka is going to levearge. To change this and to reduce the security inside my local set-up, I can simply change the value to 'PLAINTEXT'. In the commenets you can also be able to visulize what are the other allowed value i.e., Allowed values are 'PLAINTEXT', 'SASL_PLAINTEXT', 'SASL_SSL' and 'SSL'. These are all the valid values. For our local testing/set-up we are going to use 'PLAINTEXT'. This k_v i.e., protocol: SASL_PLAINTEXT is going to be present at multiple places inside your values.yaml file, so replace that with this value i.e., protocol: PLAINTEXT. You can ignore the ones commented and don't replace them. With this now, I have changed the security communication from SASL_PLAINTEXT to PLAINTEXT. Now we should be good with the values/changes that we have made. Now, lets try to set up our kafka with the help of helm. So, open your terminal in the helm folder where your kafka helm chart is present and inside this, run the helm install <Name that we want to give to the installation/relese> <name of the helm chart> i.e., helm install kafka kafka. But befire we run this command don't forget that we need to run the helm dependencies build command from the kafka helm chart folder first. After that yoiu can come out of the kafka helm chart location into the helm folder and run the command helm install kafka kafka. Otherwise, the helm install command will fail. My instructor on running this helm install kafka kafka command for the kafka bitnami helm chart at that time, it worked! The kafka installation happily happened inside my local k8s cluster. In the output there os information like, 'Each kafka broker can be accessed by producers via port 9092 on the following DNS name(s) from within your cluster: kafka-controller-0.kafka-controler-headless.default.svc.cluster.local:9092'. So yea this is the DNS name that we need to consider to provide to the accounts and message ms's because they need to connect to this kafka broker to send the information asynchronously. The same value my instructor has mentioned insde the Config Map values that we have defined inside all the environment helm charts. Hope that is clear! In the installation/deployment output you will see there are other instructions which you can follow if you want to test our k8s cluster by sending a sample message, that's why we don't need to follow all these because anyway we are going to test them using our applications. Now, my kafka bitnami helm chart installed kafka inside the k8s cluster and you can also validate the same inside the k8s admin UI/dashboard. There, if you navigate under pods nav you will be able to see a kafka related pod - But for ma case it is in red (failed) - due to possibly same reasons as we saw with keycloak bitnami helm chart.
 * Similarly, if you go to the services nav-link in the k8s admin dashboard default realm/nameSpace you should be able to see services related to kafka all of which are defined as 'ClusterIP' type services which we are fine with. This confirms that kafka is installed and configured correctly inside our k8s cluster with the help of helm chart. As a temporary workaround to my installation failure as could be noted from the k8s admin UI/dashboard, for SIT's we will use the same drill as we did for keycloak i.e., uninstall the bitnami kafka helm chart and install the one manually from the GitHub repo that your instructor pushed as a workaround to this challnge we are experiencing with bitnami helm charts. Also make sure to delete the PVC's created as a result of installation of the bitnami kafka helm chart. Hurrey! You have successfully installed Kafka inside your k8s cluster.
 *
 *  Install Prometheus in k8s cluster using helm chart
 *  ------------------------------------------------------
 *  We will try to set up Prometheus into our k8s cluster so that it can scrape all the metrics from my individual ms's. Post that, we are going to set up Grafana related components and we will make our grafana to talk with the prometheus to search some metrics and to set up the dashboards. For the same, inside my bitnami downloaded repository i.e., charts-main/bitnami copy the prometheus folder/helm chart and paste it under the helm folder where we have maintained keycloak and kafka helm charts that we discussed earlier on. You will se there is a folder/helm chart with the name kube-prometheus. That's what you need to copy and paste in the helm folder. As usual, we cannot directly install this helm chart into the k8s cluster as we need to make some changes inside the values.yaml file. Here, I am going to search for setting/configuration with the name/key additionalScrapeConfigs. Under it you will see we have details related to Scrape Configs and using these configs only my Prometheus will decide to scrape the metrics from my individual ms's. As of now/by default it is disabled  as can be seen from the child k-v mention i.e., enabled: false. Enable this to true and post that we should change the child k-v metion type: external to the value internal Reason : We don't want our Prometheus to scrape anything from outside of the k8s cluster. Everything that we have inside the k8s cluster only should be scraped by Prometheus. After making these changes, we need to provide the details of our individual ms's using which my prometheus can connect with them and read the metrics of them with the help of Actuator Promethues url. For the same, as you can see under the child configuration internal.jobList:[] as of now it is empty. Herw e will need to mention some configurations the same of which we mentioned if you can recall when we tried to set up prometheus inside our local system. So under this jobList I am going to mention all the job names as shown below using which my prometheus is going to fetch the metrics from the individual ms's.
 internal:
 	jobList: [
 	 {
 		   "job_name": "configserver", # If you see here, first I have mentioned what is the Job name of the ms. For our configserver I have just maintained the job name simply as configserver.
           "metrics_path": "/actuator/prometheus", # What is the metrics path
           "static_configs": [
             {
               "targets": ["configserver:8071"] # What is the location at which my prometheus needs to connect to scrape/get the metrics. As can be seen, configserver:8071, 'configserver' is the service name inside our k8s for configserver and it is available at the port 8071.
             }
           ]
     },
 # Similarly for the other individual ms's I have mentioned the job details as discussed in example for configserver.
     {
           "job_name": "eurekaserver",
 		   "metrics_path": "/actuator/prometheus",
 		   "static_configs": [
             {
                "targets": ["eurekaserver:8070"]
             }
           ]
     },
     {
           "job_name": "accounts",
           "metrics_path": "/actuator/prometheus",
           "static_configs": [
             {
                "targets": ["accounts:8080"]
             }
           ]
     },
     {
            "job_name": "loans",
            "metrics_path": "/actuator/prometheus",
            "static_configs": [
              {
                 "targets": ["loans:8090"]
              }
            ]
     },
     {
            "job_name": "cards",
            "metrics_path": "/actuator/prometheus",
            "static_configs": [
              {
                 "targets": ["cards:9000"]
              }
            ]
     },
     {
            "job_name": "gatewayserver",
            "metrics_path": "/actuator/prometheus",
            "static_configs": [
              {
                 "targets": ["gatewayserver:8072"]
              }
            ]
     }
   ]
 *
 * As can be see all those details have been mentioned on a Json format. You can see the list of jobs and each job in the list is separated by comma value. Like this, for whatever ms you need you can mention those job details as you need under the configuration internal.jobList:[...]. Once we have defined these details, my prometheus should be able to scrape metrics from all individual ms's. Now, as a next step, we can try to build the prometheus related helm chart. cd into kube-prometheus folder and then run the helm dependencies build command. This should compile your kube-prometheus helm chart. Post that, we can try to install this chart inside our k8s cluster. To install this chart, cd ../ - which is nothing but the helm folder and then execute the command helm install <The release name that we want to give> <folder name of the helm chart> i.e., helm install prometheus kube-prometheus. This will do the installation of prometheus inside my local system. By default, prometheus is set up with the help of 'ClusterIP' and so, we cannot really access it. But for some reason if you want to access the same, you can run the commands in the output which are related to kubectl port forward. You can see them under the label sentence, 'To access Prometheus from outside the cluster execute the following commands:'
 echo "Prometheus URL: http://127.0.0.1:9090/"
 kubectl port-forward --namespace default svc/prometheus-kube-prometheus-prometheus 9090:9090

 * Using the kubectl port forward command, we can temporarily expose a service from within the cluster so that we can access it from outside the cluster. You have to wait for sometime until the installation of prometheus is complete - this can easily take some few minutes. In my case, just like the earlier bitnami helm charts to keycloak and kafka, this kube-prometheus bitnami helm chart also didn't spin up as expected with the 'STATUS' ImagePullBackOff that is when you try to execute the command kubectl get pods. So I am going to resort to uninstalling it and installing the one prepared by my instructor manually from the GitHub repo. Hurrey! It was successfully installed inside my local k8s cluster. This new installation has 'LoadBalancer' configured as the service type which implies that we can access it from outside of the cluster from the configured port which is 9090. Now, we have successfully installed Prometheus inside our k8s cluster. Inside your browser if you open localhost:9090 you should be able to see the prometheus UI.  Here if I click on the Status dropdown and click on the 'Target health' nav link, I should be able to see all the targets related to my individual ms's. This confirms that my prometheus has been configured correctly inside my k8s cluster with the help of the kube-prometheus helm chart. As of now the individual targets, nothing but my configured individual ms's, all of them we are yet to set them up and that's why you can visualize that the State/status field/column is showing Down and in red color.
 * If you we using the bitnami kube-prometheus helm chart and it spin up successfully, in the same Target health nav link as could be seen in my instructors screen share, apart from the individual ms's, if you scroll a little bit down you will notice that there are more other targets - nothing but prometheus will also try to monitor many other metrics inside your k8s cluster. That's why you will be able to see many other components inside the Target health nav link apart from the individual custom ms's since this bitnami kube-prometheus helm chart is meant for production ready. Behind the scenes it is trying to monitor many other components inside your k8s cluster. But we are interested more about our own custom ms's, so that should not worry you. We will validate this once we set up our own custom ms's into the k8s cluster later. For now, my instructor stopped the port forwarding command by pressing ctrl + C which stops exposing my prometheus into my local network - outside world. Now, if you tried refreshing the page localhost:9090 you should now not be able to see the prometheus UI - Nothing but 'This site can't be reached' as prometheus is not reachable from outside the cluster- Nothing but it is not exposed to the local system. With this, we successfully set up prometheus inside our k8s cluster. As a next step, we will try to set up Grafana related components like Loki, Tempo and Grafana inside our k8s cluster. With this, we will have be done with all the required components for our ms's end to end testing.
 *
 * Install Grafana, Loki and Tempo in k8s cluster using helm chart
 * ----------------------------------------------------------------
 * As a next step, we are interested to set-up grafana related components like loki, tempo and grafana inside our k8s cluster. Inside our ms's network we use Loki, Tempo and Grafana. So, these 3 components we need to set them up with the help of helm chart. Before we try to set up the actual Grafana, 1st we need to make sure we set up the Loki and Tempo. For the same, I am going to copy the folders of grafana-loki and grafana-tempo into the helm folder . First, we will try to set up grafana-loki because Loki is responsible to aggregate all the logs that are generated by your individual ms's. So, to install Loki, we don't need to make any changes inside it's values.yaml file We should be fine with all the default values available inside this chart. You can straight away compile and install this grafana-loki bitnami helm chart inside your local k8s cluster. From your helm folder navigate to the grafana-loki folder and run the command 'helm dependencies build'. Here while the building is ongoing, in the output you can see that my Loki has dependencies on other chart like memcached and that's why you can be able to see it is trying to download all those dependencies. Once your compilation is successful, you can now proceed to installing the Loki helm chart with the help of helm install command i.e., helm install <name to my relaese> <chart name> i.e., helm install loki grafana-loki. Do this from the parent folder location i.e., helm. This will install many components inside my k8s cluster i.e., ingester, distributer, querier, promtail, compactor, gateway and query-frontend. Can you imagine setting up all this without helm chart?? Your k8s administrator will need a lot of help from the developers and the grafana admin to set up this and it may need months of effort to have a proper set up. Whereas with helm it is super easy.
 * I started experiencing same problems of ImagePullBackOff after during the installation of grafana-loki just as experienced before with other bitnami charts we tried to set up/ install earlier i.e., keycloak, kafka, prometheus. As a workaround to this we are resolving to not using the bitnami helm charts and will make use of the charts that your instructor pushed as a workaround to this challenge in his GitHub repo. So, uninstall the bitnami helm chart and install the one that your instructor pushed as a workaround to this challenge. Also, before doing the workaround grafana-loki installation, make sure to delete the PVC's created as a result of installation of the bitnami loki helm chart. Hurrey! You have successfully installed Loki inside your k8s cluster. And btw during the compilation of grafana-loki - the one my instructor pushed as a workaround, I ran into an issue like, 'No repository definition for https://grafana.github.io/helm-charts. Please add the missing repo via helm repo add'. I was able to resolve the issue by navigating to this github link on my browser and following some instructions which were there. Nothing but I just ran the command 'helm repo add grafana https://grafana.github.io/helm-charts' and on now compiling the grafana-loki helm chart, the issue was resolved and the compilation was successful. Next, I then easily installed the grafana-loki helm chart.
 * As a next step we can set up the tempo as well. For the same, navigate into the grafana-tempo folder/chart. Inside this chart we need to alter few values inside the values.yaml. Inside this file, search for 'otlp' - these are related to Opentelemetry  configurations. As of now you can see that the Opentelemetry communication with the help of http protocol and grpc protocol is disabled. We need to enable both of them then only the Opentelemetry Java Agent that we have inside our individual ms's it can send the tracing details to the Tempo. By default otlp as we have seen, it is disabled inside the bitnami provided grafana-tempo helm chart. That's why we need to override them by overriding the values false to true. Post this alteration, compile the grafana-tempo helm chart by executing the command helm dependencies build - this will download all the dependencies of tempo. Once the build is completed you can navigate back to the parent folder i.e., helm and from here you can run the command which is helm install <Name of the release you want to give> <Helm Chart name> i.e., helm install tempo grafana-tempo. The output of running this command will give some instructions and some other information like all the components that have been installed i.e., ingester, distributor, querier, query-frontend,compactor, vulture. But here we have a problem, our individual ms's need a tempo url to which my Opentelemetry is going to send the tracing details. In the output as can be visualized, we don't have any information about the url that we need to leverage. To overcome this challange we can try to run the command which is 'kubectl get services'. This command is going to list all the services installed inside your k8s cluster. As of now you can see so many services have been installed related to loki, prometheus, keycloak, kafka. If you can scroll down in the output, you will see some information related to tempo. There are many tempo related services that have been installed as can be visualized i.e., there is a tempo service with the name i.e, gossip-ring, ingester, metrics-generator, querier,query-frontend, vulture, compactor, distributer. So, the service tha our Opentelemetry should connect to is this distributor. That's why copy that service name and make sure you are mentioning the same inside the configMap values.yaml of your respective environment helm charts. This is under the key i.e., 'otelExporterEndPoint' as can be seen in the values.yaml file i.e., otelExporterEndPoint: http://tempo-grafana-tempo-distributor:4317 and as can be visualized also the port number has to be 4317. This is how we are establishing a link between our individual ms's Opentelemetry with the grafana-tempo. You may have a question like, how does it come that my instructor knows that we need to use only distributor but not other grafana-tempo related services as visualized from the output. Haha, based upon his experience and official documentation information, he is able to identify this information and he is trying to spoon feed me this information. So, whenever you have such chalalnges or questions always visit the official documentation and you should be able to find the answers/this information. With all this that we have discussed, my instructor successfully installed grafana-tempo. But for me I ran into the problem that I had been experiencing earlier with other bitnami charts installation i.e., keycloak, prometheus, kafka, loki where by on running the command kubectl get pod, I can see the status column for the row with grafana-tempo related pods is reading as 'ErrImagePull' and others 'ImagePullBackOff'. As a workaround, just like we did for prometheus, kafka, grafana-loki and keycloak bitnami related charts, I am going to uninstall the grafana-tempo helm chart and install the workaround one from my instructor's GitHub repo. Hurrey! You have successfully installed grafana-tempo inside your k8s cluster.
 * As a last step, we need to set up the grafana inside our k8s cluster before we try to deploy all our ms's. To install grafana, from the bitnami folder, copy the grafana related helm chart and paste it in our helm folder where we have set up the other helm charts. As usual, before we try to set up this grafana, we need to make some changes inside it's value.yaml file. Inside this values.yaml file, I need to look for the information on how to provide the datasource details of tempo, loki and prometheus. Like we discussed in the previous sections, grafana is capable of connecting to these components like prometheus, loki and tempo. We can provide these datasource details to grafana using yaml configurations or we can also manually set up the datasources from the UI of the grafana. Since we want to avoid that manual task, we need to look for the information on how o set up the datasource details inside this values.yaml.. So, search for the string, 'datasources'. You will see a configuration section about datasources and there you will notice that they have given information saying that, whenever we want to set a datasource, we need to define the datasource details under the secretDefinition element. They have even given you a commented example tha you can follow. Under this secretDefinition, we need to provide the datasource details like they have mentioned in the example/sample connection details to prometheus. So, using the same syntax, we need to set up the datasource details for grafana. So, I deleted the curly braces assigned to the datasourceDefinition key, and on the next line, I have mentiooned few configurations as shown below:
 secretDefinition:
 	apiVersion: 1 # Here have mentioned api version as 1

 	deleteDatasources: # Delete datasources if already existed in my grafana with the mentioned list of names.
 	  - name: Prometheus
 	  - name: Loki
 	  - name: Tempo

 	datasources: # Here I have mentioned 3 datasource details of prometheus, loki and tempo
 	  - name: Prometheus
 		type: prometheus
 		uid: prometheus
 		url: http://prometheus-kube-prometheus-prometheus:9090 # This is the DNS name of prometheus inside my k8s cluster
 		access: proxy
 		orgId: 1
 		basicAuth: false
 		isDefault: false
 		version: 1
 		editable: true
 		jsonData:
 		  httpMethod: GET
 	  - name: Tempo
 		type: tempo
 		uid: tempo
 		url: http://tempo-grafana-tempo-query-frontend:3200 # This is the DNS name of tempo inside my k8s cluster. Inside tempo, we have many services like we visualized previously. So the service tha we need for connecting grafana-tempo is 'tempo-grafana-tempo-query-frontend' and for this the port is going to be 3200. This you can confirm from the terminal output of services like we were visualizing previously on running the command kubectl get services.
 		access: proxy
 		orgId: 1
 		basicAuth: false
 		isDefault: false
 		version: 1
 		editable: true
 		jsonData:
 		  httpMethod: GET
 		  serviceMap:
 			datasourceUid: 'prometheus'
 	  - name: Loki
 		type: loki
 		uid: loki
 		access: proxy
 		orgId: 1
 		editable: true
 		url: http://loki-grafana-loki-gateway:80 # This is the DNS name of loki inside my k8s cluster. For loki, we need to connect to the service which is going to be loki-grafana-loki-gateway. It is exposed on port 80. This you can confirm from the terminal output of services like we were visualizing previously on running the command kubectl get services.
 		jsonData:
 		  httpHeaderName1: "X-Scope-OrgID"
 		  derivedFields: # At last, since we want to integrate loki and tempo, I have mentioned these derived fields configurations just like how we did during our local testing. You can revisit that in case you forgot.
 			- datasourceUid: tempo
 			  matcherRegex: "\\[.+,(.+),.+\\]"
 			  name: TraceID
 			  url: '$${__value.raw}'
 		secureJsonData:
 		  httpHeaderValue1: "tenant1"
 *
 * Once we defined these values, now my grafana should be able to connect with all the components related to prometheus, loki and tempo. As a next step, we can compile this grafana helm chart and install the same inside our k8s cluster. Navigate to this grafana folder/chart and run the command helm dependencies build. Once the build is completed you can now navigate to the parent folder i.e., helm and inside this folder we can run the command which is helm install <release name> <chart name> i.e., helm install grafana grafana. If I try to execute this command , the grafana is going to be installed inside my local system. By default, my grafana is going to be exposed as a clusterIP service, so whenever I have some requirements to debug any issues my k8s admin can run a command which is related to kubectl port-forward as can be seen from the terminal output as shown below. This will temporarily expose my grafana on port 8080 of my local system.
 1. Get the application URL by running these commands:
 echo "Browse to http://127.0.0.1:8080"
 kubectl port-forward svc/grafana 8080:3000 &

 * After executing this command we should be able to access the grafana at the port 8080 of my local system. Grafana internally is going to start at the port 3000 but the traffic is now going to be exposed at the port 8080 of my local system. For my instructor, this was smooth, for me I encountered an error: unable to forward port because pod is not running. Current status=pending. On visiting the k8s admin dashboard UI, I noticed that the installation of grafana was failing and on running the command kubectl get pods, I noticed on the STATUS column its because of the same issue we had been encountering with the latest bitnami helm charts i.e., 'ImagePullBackOff'. So, as a workaround, I decided to uninstall the bitnami grafana helm chart, deleted any PVC's associated with the installation and then proceeded to install the workaround grafana helm chart that my instructor had created and pushed to his github repo. Back to capturing what my instructor was saying when he was doing the port forwarding of the grafana bitnami helm chart that he had installed. He pointed out of a visible problem with the port on which the traffic was being exposed to i.e., 8080. The problem was, for some reason if you start your accounts ms as a LoadBalancer service, it is also going to expose the traffic at the port 8080 and with that reason you may face some port clashing issues. With that reason, he kill the port forwarding command he had initially executed by pressing ctrl + C and tried to run the same command again i.e., kubectl port-forward svc/grafana 3000:3000  having now my grafana being able to be exposed at the port 3000 of my local system. Like this, he just changed the port number from 8080 to 3000. This way grafana is going to be available at the port 3000 itself in my local system. Now, if you go the browser and try to access http://127.0.0.1:3000 you will be able to see the grafana welcome login page where you can enter your username and password. But I don't know the username and password, for the same if you closely observe the terminal output instructions, you will see instructions on how to get the username and password as shown below:
 2. Get the admin credentials:

 echo "User: admin"
 echo "Password: $(kubectl get secret grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 -d)"
 # Note: Do not include grafana.validateValues.database here. See https://github.com/bitnami/charts/issues/20629

 * The username as visualized is 'admin' and to get the password, we need to execute both the two echo commands at once and you should be able to see the username as well as the password as the output. This is if the installation fo the grafana bitnami helm chart was successful, otherwise for the workaround grafana helm chart, you can get these credentials from the values.yaml file. Once you log in to the grafana successfully you can validate if the datasources connections are successful or not by going to the explore LHS nav and under the dropdown, you should be able to see 3 datasources related to Loki, Prometheus and Tempo. This confirms that our grafana setup is also completed successfully. As a next step, we can now finally install all our ms's into the k8s cluster. Before that my instructor stopped the port forwarding by ctrl + C and with that you will not be able to access your grafana in future whenever you want to access it, you should again run the port forwarding command.
 * Now, try to run the command helm ls. This will show you all the releases/installations/deployments that we have done with the help of helm as shown below:
 ctemoi@CTEMOI2-PC MINGW64 ~/Desktop/K8s/helm
 $ helm ls
 NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
 grafana         default         1               2025-12-17 08:49:13.6796607 +0300 EAT   deployed        grafana-1.0.0           11.4.0
 kafka           default         1               2025-12-09 08:21:15.4633353 +0300 EAT   deployed        kafka-1.0.0             4.1.0
 keycloak        default         1               2025-12-05 11:19:56.2538402 +0300 EAT   deployed        keycloak-1.0.0          26.4.0
 loki            default         1               2025-12-15 07:48:50.9319885 +0300 EAT   deployed        grafana-loki-1.0.0      3.1.2
 prometheus      default         1               2025-12-11 08:34:09.150339 +0300 EAT    deployed        kube-prometheus-1.0.0   v3.1.0
 tempo           default         1               2025-12-15 08:46:03.5838406 +0300 EAT   deployed        grafana-tempo-1.0.0     2.6.1
 *
 * So, as of now, we have installed grafana, kafka, keycloak, loki, prometheus and tempo in our k8s cluster successfully. And as shown in the output, are the chart details that we have used to set up these components inside the k8s cluster. Hope everything we have discussed so far was crisp clear. Next, we will try to deploy our ms's into the k8s cluster.
 *
 * Install getttrightbank ms's into the k8s cluster
 * ------------------------------------------------------
 *
 * * */
@SpringBootApplication
public class GatewayserverApplication {

	public static void main(String[] args) {
		SpringApplication.run(GatewayserverApplication.class, args);
	}

	/* This method is going to create a bean of type RouteLocator and return it.
	* Inside this RouteLocator only, we are going to send all our custom routing related configurations based on our requirements.
	**/
	@Bean
	public RouteLocator eazyBankRouteConfig(RouteLocatorBuilder routeLocatorBuilder) {
		return routeLocatorBuilder.routes()
				.route(p -> p.path("/eazybank/accounts/**")
						.filters(f -> f.rewritePath("/eazybank/accounts/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time", LocalDateTime.now().toString())
								.circuitBreaker(config -> config.setName("accountsCircuitBreaker")
										.setFallbackUri("forward:/contactSupport")))
						.uri("lb://ACCOUNTS"))

				.route(p -> p.path("/eazybank/loans/**")
						.filters(f -> f.rewritePath("/eazybank/loans/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.retry(retryConfig -> retryConfig.setRetries(3)
										.setMethods(HttpMethod.GET)
										.setBackoff(Duration.ofMillis(100),Duration.ofMillis(1000),2,true))
						)
						.uri("lb://LOANS"))
				.route(p -> p.path("/eazybank/cards/**")
						.filters(f -> f.rewritePath("/eazybank/cards/(?<segment>.*)", "/${segment}")
								.addResponseHeader("X-Response-Time",LocalDateTime.now().toString())
								.requestRateLimiter(config -> config.setRateLimiter(redisRateLimiter())
										.setKeyResolver(userKeyResolver())))
						.uri("lb://CARDS")).build();
	}

	@Bean
	public Customizer<ReactiveResilience4JCircuitBreakerFactory> defaultCustomizer() {
		return factory -> factory.configureDefault(id -> new Resilience4JConfigBuilder(id)
				.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())
				.timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(4)).build()).build());
	}

	@Bean
	public RedisRateLimiter redisRateLimiter() {
		return new RedisRateLimiter(1, 1, 1);
	}

	@Bean
	KeyResolver userKeyResolver() {
		return exchange -> Mono.justOrEmpty(exchange.getRequest().getHeaders().getFirst("user"))
				.defaultIfEmpty("anonymous");
	}


}
